{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"BuildCPG Labs","text":"<p>A modern, scalable data engineering platform for managing multiple independent labs using dbt, DuckDB, and Python.</p>"},{"location":"#overview","title":"Overview","text":"<p>BuildCPG Labs enables you to:</p> <ul> <li>Run independent labs - Each lab has its own database, data, and Python environment</li> <li>Share utilities - Common code used by all labs without duplication</li> <li>Scale easily - Create new labs in minutes using templates</li> <li>Maintain quality - Built-in data inspection and automated quality checks</li> <li>Work efficiently - Per-lab virtual environments with standardized workflows</li> </ul>"},{"location":"#architecture","title":"Architecture","text":"<pre><code>graph TB\n    subgraph \"BuildCPG Labs Platform\"\n        S[shared/&lt;br/&gt;Utilities &amp; Templates]\n\n        subgraph \"Lab 1 - Sales Performance\"\n            L1[dbt Models]\n            L1D[DuckDB Database]\n            L1E[lab1_env]\n        end\n\n        subgraph \"Lab 2 - Market Sentiment\"\n            L2[dbt Models]\n            L2D[DuckDB Database]\n            L2E[lab2_env]\n        end\n\n        subgraph \"Lab 3 - Customer Segmentation\"\n            L3[dbt Models]\n            L3D[DuckDB Database]\n            L3E[lab3_env]\n        end\n    end\n\n    S -.-&gt;|Shared Utils| L1\n    S -.-&gt;|Shared Utils| L2\n    S -.-&gt;|Shared Utils| L3\n\n    style S fill:#e0e7ff\n    style L1 fill:#dbeafe\n    style L2 fill:#dcfce7\n    style L3 fill:#fef3c7</code></pre>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#multi-lab-architecture-with-per-lab-environments","title":"Multi-Lab Architecture with Per-Lab Environments","text":"<p>Each lab is completely independent:</p> <pre><code>buildcpg-labs/\n\u251c\u2500\u2500 shared/                     # Reusable utilities (ALL labs)\n\u2502   \u251c\u2500\u2500 utils/                  # DataInspector, CSVMonitor\n\u2502   \u2514\u2500\u2500 config/                 # Central configuration\n\u2502\n\u251c\u2500\u2500 lab1_sales_performance/     # Lab 1 (Independent)\n\u2502   \u251c\u2500\u2500 lab1_env/              # Own virtual environment\n\u2502   \u251c\u2500\u2500 data/                  # Own database\n\u2502   \u2514\u2500\u2500 dbt/                   # Own models\n\u2502\n\u251c\u2500\u2500 lab2_market_sentiment/      # Lab 2 (Independent)\n\u2502   \u251c\u2500\u2500 lab2_env/              # Own virtual environment\n\u2502   \u251c\u2500\u2500 data/                  # Own database\n\u2502   \u2514\u2500\u2500 dbt/                   # Own models\n\u2502\n\u2514\u2500\u2500 lab3_customer_segmentation/ # Lab 3 (Independent)\n    \u251c\u2500\u2500 lab3_env/              # Own virtual environment\n    \u251c\u2500\u2500 data/                  # Own database\n    \u2514\u2500\u2500 dbt/                   # Own models\n</code></pre> <p>Architecture Benefits: - \u2705 Complete isolation - Each lab has dedicated environment - \u2705 Independent data - Each lab has its own DuckDB database - \u2705 Dependency freedom - Labs can use different package versions - \u2705 No conflicts - Work on multiple labs simultaneously - \u2705 Shared utilities - Common code available to all labs</p>"},{"location":"#current-labs","title":"Current Labs","text":""},{"location":"#lab-1-sales-performance-analysis","title":"Lab 1: Sales Performance Analysis","text":"<p>Status: \u2705 Active Purpose: Analyze sales data with medallion architecture (Bronze \u2192 Silver \u2192 Gold) Features: - Sales data processing - Performance metrics - Trend analysis</p> <p>Documentation: See Lab 1 Overview</p>"},{"location":"#lab-2-market-sentiment-analysis","title":"Lab 2: Market Sentiment Analysis","text":"<p>Status: \u2705 Active Development Purpose: Real-time CPG brand reputation monitoring Features: - Reddit &amp; News sentiment ingestion - 5 dbt models with incremental processing - 14 automated data quality tests - Anomaly detection via z-scores - Daily sentiment aggregations</p> <p>Key Metrics: - \ud83d\udcca 800 sentiment events - \ud83e\uddea 14/14 tests passing - \ud83c\udfaf 5 CPG brands monitored - \u26a1 ~3 second build time</p> <p>Technology Stack: <pre><code>graph LR\n    A[Python 3.11] --&gt; B[dbt 1.7.0]\n    B --&gt; C[DuckDB 0.9.1]\n    C --&gt; D[Data Quality Tests]\n\n    style A fill:#3b82f6\n    style B fill:#10b981\n    style C fill:#f59e0b\n    style D fill:#8b5cf6</code></pre></p> <p>Documentation:  - Lab 2 Overview - Architecture &amp; features - Lab 2 Setup - Installation guide - Lab 2 Data Models - Complete model reference - Lab 2 Troubleshooting - Issue solutions - Lab 2 Quick Reference - Common commands</p>"},{"location":"#lab-3-customer-segmentation","title":"Lab 3: Customer Segmentation","text":"<p>Status: \ud83d\udccb Planned Purpose: Customer behavior analysis and segmentation Features: (Coming soon)</p>"},{"location":"#platform-statistics","title":"Platform Statistics","text":"Metric Value Active Labs 2 Total Data Models 10+ Data Quality Tests 20+ Shared Utilities 5 Documentation Pages 15+"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#1-clone-repository","title":"1. Clone Repository","text":"<pre><code>git clone https://github.com/narensham/buildcpg-labs.git\ncd buildcpg-labs\n</code></pre>"},{"location":"#2-work-with-lab-2-example","title":"2. Work with Lab 2 (Example)","text":"<pre><code># Navigate to lab\ncd lab2_market_sentiment\n\n# Create virtual environment\npython3 -m venv lab2_env\nsource lab2_env/bin/activate\n\n# Install dependencies\npip install -r requirements.txt\n\n# Setup dbt\ncd dbt\ndbt deps\ndbt debug\n\n# Generate sample data\ncd ..\npython pipelines/ingest_sentiment.py\n\n# Run pipeline\ncd dbt\ndbt build\n</code></pre>"},{"location":"#3-verify-success","title":"3. Verify Success","text":"<pre><code># All tests should pass\ndbt test\n\n# Expected output:\n# \u2705 PASS=14 WARN=0 ERROR=0 SKIP=0 TOTAL=14\n</code></pre>"},{"location":"#platform-structure","title":"Platform Structure","text":"<pre><code>buildcpg-labs/\n\u2502\n\u251c\u2500\u2500 shared/                          # Shared utilities\n\u2502   \u251c\u2500\u2500 utils/\n\u2502   \u2502   \u251c\u2500\u2500 data_inspector.py       # Database inspection\n\u2502   \u2502   \u251c\u2500\u2500 csv_monitor.py          # Data change detection\n\u2502   \u2502   \u2514\u2500\u2500 config_loader.py        # Configuration management\n\u2502   \u251c\u2500\u2500 config/\n\u2502   \u2502   \u251c\u2500\u2500 labs_config.yaml        # Lab registry\n\u2502   \u2502   \u2514\u2500\u2500 paths.py                # Path helpers\n\u2502   \u2514\u2500\u2500 templates/                  # Lab templates\n\u2502\n\u251c\u2500\u2500 lab1_sales_performance/         # Independent Lab 1\n\u2502   \u251c\u2500\u2500 lab1_env/                   # Virtual environment\n\u2502   \u251c\u2500\u2500 data/\n\u2502   \u2502   \u251c\u2500\u2500 raw/                    # Source data\n\u2502   \u2502   \u2514\u2500\u2500 lab1.duckdb             # Database\n\u2502   \u251c\u2500\u2500 dbt/\n\u2502   \u2502   \u251c\u2500\u2500 models/                 # Transformations\n\u2502   \u2502   \u251c\u2500\u2500 tests/                  # Quality tests\n\u2502   \u2502   \u2514\u2500\u2500 profiles.yml            # DB connection\n\u2502   \u251c\u2500\u2500 pipelines/                  # Data ingestion\n\u2502   \u2514\u2500\u2500 requirements.txt            # Dependencies\n\u2502\n\u251c\u2500\u2500 lab2_market_sentiment/          # Independent Lab 2\n\u2502   \u251c\u2500\u2500 lab2_env/                   # Virtual environment\n\u2502   \u251c\u2500\u2500 data/\n\u2502   \u2502   \u251c\u2500\u2500 raw/                    # Source data\n\u2502   \u2502   \u2514\u2500\u2500 lab2.duckdb             # Database\n\u2502   \u251c\u2500\u2500 dbt/\n\u2502   \u2502   \u251c\u2500\u2500 models/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 staging/           # Bronze layer\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 intermediate/      # Silver layer\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 mart/              # Gold layer\n\u2502   \u2502   \u251c\u2500\u2500 macros/                # Reusable SQL\n\u2502   \u2502   \u251c\u2500\u2500 tests/                 # Quality tests\n\u2502   \u2502   \u2514\u2500\u2500 schema.yml             # Contracts\n\u2502   \u251c\u2500\u2500 pipelines/\n\u2502   \u2502   \u2514\u2500\u2500 ingest_sentiment.py   # Data generation\n\u2502   \u2514\u2500\u2500 requirements.txt           # Dependencies\n\u2502\n\u251c\u2500\u2500 docs/                           # Documentation (MkDocs)\n\u2502   \u251c\u2500\u2500 architecture/\n\u2502   \u251c\u2500\u2500 getting-started/\n\u2502   \u251c\u2500\u2500 labs/\n\u2502   \u2502   \u251c\u2500\u2500 lab1-*.md\n\u2502   \u2502   \u2514\u2500\u2500 lab2-*.md              # Lab 2 documentation\n\u2502   \u2514\u2500\u2500 utilities/\n\u2502\n\u251c\u2500\u2500 .github/\n\u2502   \u2514\u2500\u2500 workflows/\n\u2502       \u2514\u2500\u2500 docs.yml               # Auto-deploy docs\n\u2502\n\u251c\u2500\u2500 mkdocs.yml                      # Documentation config\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"#technology-stack","title":"Technology Stack","text":"Layer Technology Purpose Database DuckDB 0.9.1 Embedded, Mac-compatible, no Docker Transformation dbt 1.7.0 Data modeling &amp; testing Language Python 3.11+ Scripting &amp; ingestion Testing dbt_expectations Data quality validation Documentation MkDocs Material Auto-generated docs CI/CD GitHub Actions Automated deployments Orchestration Manual (Airflow planned) Pipeline scheduling"},{"location":"#design-principles","title":"Design Principles","text":""},{"location":"#1-lab-independence","title":"1. Lab Independence","text":"<p><pre><code>graph LR\n    L1[Lab 1] -.-&gt;|Uses| S[Shared Utils]\n    L2[Lab 2] -.-&gt;|Uses| S\n    L3[Lab 3] -.-&gt;|Uses| S\n\n    L1 x--x L2\n    L2 x--x L3\n    L3 x--x L1\n\n    style S fill:#e0e7ff\n    style L1 fill:#dbeafe\n    style L2 fill:#dcfce7\n    style L3 fill:#fef3c7</code></pre> - Each lab has own database - Each lab has own environment - Labs don't interfere with each other</p>"},{"location":"#2-medallion-architecture","title":"2. Medallion Architecture","text":"<p>All labs follow Bronze \u2192 Silver \u2192 Gold pattern: - Bronze (Staging): Raw data, minimal transformation - Silver (Intermediate): Cleaned, enriched, business logic - Gold (Marts): Analytics-ready aggregates</p>"},{"location":"#3-data-quality-first","title":"3. Data Quality First","text":"<ul> <li>Automated tests on every run</li> <li>Contract enforcement where needed</li> <li>Quality flags and validation</li> <li>Comprehensive test coverage</li> </ul>"},{"location":"#4-documentation-driven","title":"4. Documentation Driven","text":"<ul> <li>Every lab fully documented</li> <li>Architecture diagrams</li> <li>Troubleshooting guides</li> <li>Quick reference cards</li> <li>Auto-deployed to GitHub Pages</li> </ul>"},{"location":"#workflow-example-lab-2","title":"Workflow Example (Lab 2)","text":"<pre><code>sequenceDiagram\n    participant D as Developer\n    participant P as Python Script\n    participant CSV as CSV Files\n    participant DBT as dbt\n    participant DB as DuckDB\n    participant T as Tests\n\n    D-&gt;&gt;P: python ingest_sentiment.py\n    P-&gt;&gt;CSV: Generate data\n    D-&gt;&gt;DBT: dbt run\n    DBT-&gt;&gt;CSV: Read raw data\n    DBT-&gt;&gt;DB: Transform (5 models)\n    D-&gt;&gt;DBT: dbt test\n    DBT-&gt;&gt;T: Run 14 tests\n    T--&gt;&gt;D: \u2705 All pass</code></pre>"},{"location":"#common-tasks","title":"Common Tasks","text":""},{"location":"#check-lab-status","title":"Check Lab Status","text":"<pre><code># From buildcpg-labs root\nls -la | grep lab\n\n# Lab specific status\ncd lab2_market_sentiment\ndbt debug\ndbt list\n</code></pre>"},{"location":"#run-specific-lab","title":"Run Specific Lab","text":"<pre><code>cd lab2_market_sentiment\nsource lab2_env/bin/activate\ncd dbt\ndbt run\ndbt test\n</code></pre>"},{"location":"#view-documentation","title":"View Documentation","text":"<pre><code># Serve locally\nmkdocs serve\n\n# Visit: http://127.0.0.1:8000\n</code></pre>"},{"location":"#create-new-lab-future","title":"Create New Lab (Future)","text":"<pre><code>./setup_new_lab.sh lab3_customer_segmentation\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":""},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Quick Start Guide</li> <li>Installation Guide</li> </ul>"},{"location":"#architecture_1","title":"Architecture","text":"<ul> <li>System Overview</li> <li>Multi-Lab Design</li> <li>Medallion Architecture</li> <li>Shared vs Lab-Specific</li> </ul>"},{"location":"#lab-documentation","title":"Lab Documentation","text":"<ul> <li>Lab 1:</li> <li>Overview</li> <li>Setup</li> <li> <p>Data Models</p> </li> <li> <p>Lab 2: \u2b50 NEW</p> </li> <li>Overview</li> <li>Setup</li> <li>Data Models</li> <li>Troubleshooting</li> <li>Quick Reference</li> </ul>"},{"location":"#utilities","title":"Utilities","text":"<ul> <li>Data Inspector</li> <li>CSV Monitor</li> </ul>"},{"location":"#support","title":"Support","text":"<ul> <li>Troubleshooting</li> <li>FAQ</li> </ul>"},{"location":"#project-status","title":"Project Status","text":""},{"location":"#phase-1-foundation-complete","title":"Phase 1: Foundation \u2705 COMPLETE","text":"<ul> <li>\u2705 Shared utilities (DataInspector, CSVMonitor)</li> <li>\u2705 Central configuration</li> <li>\u2705 Lab1 working</li> <li>\u2705 Lab2 working with full documentation</li> </ul>"},{"location":"#phase-2-enhanced-lab-2-complete","title":"Phase 2: Enhanced Lab 2 \u2705 COMPLETE","text":"<ul> <li>\u2705 Market sentiment analysis pipeline</li> <li>\u2705 5 dbt models with incremental processing</li> <li>\u2705 14 automated data quality tests</li> <li>\u2705 Comprehensive documentation with diagrams</li> <li>\u2705 Troubleshooting guide</li> <li>\u2705 Quick reference card</li> <li>\u2705 Reference architecture diagrams</li> </ul>"},{"location":"#phase-3-platform-expansion-in-progress","title":"Phase 3: Platform Expansion \ud83d\udd04 IN PROGRESS","text":"<ul> <li>\ud83d\udd04 Lab 3 (Customer Segmentation)</li> <li>\ud83d\udd04 Streamlit dashboards</li> <li>\ud83d\udd04 Real API integrations (PRAW, NewsAPI)</li> <li>\ud83d\udccb Advanced sentiment analysis (Hugging Face)</li> </ul>"},{"location":"#phase-4-production-readiness-planned","title":"Phase 4: Production Readiness \ud83d\udccb PLANNED","text":"<ul> <li>\ud83d\udccb Airflow orchestration</li> <li>\ud83d\udccb CI/CD for data pipelines</li> <li>\ud83d\udccb Monitoring &amp; alerting</li> <li>\ud83d\udccb Data quality gates</li> <li>\ud83d\udccb Performance optimization</li> </ul>"},{"location":"#whats-new-in-lab-2","title":"What's New in Lab 2","text":""},{"location":"#key-achievements","title":"Key Achievements","text":"<ol> <li>Complete Sentiment Pipeline</li> <li>Reddit + News data ingestion</li> <li>5-layer transformation (Staging \u2192 Intermediate \u2192 Marts)</li> <li> <p>Incremental processing for efficiency</p> </li> <li> <p>Robust Data Quality</p> </li> <li>14 automated tests (100% passing)</li> <li>Contract enforcement on critical models</li> <li>Quality flags and validation</li> <li> <p>Anomaly detection</p> </li> <li> <p>Comprehensive Documentation</p> </li> <li>Full architecture diagrams (Mermaid)</li> <li>Step-by-step setup guide</li> <li>Complete model reference with schemas</li> <li>Troubleshooting for all issues encountered</li> <li> <p>Quick reference card</p> </li> <li> <p>Battle-Tested Solutions</p> </li> <li>Resolved duplicate key issues</li> <li>Fixed contract enforcement problems</li> <li>Solved CTE reference errors</li> <li>Optimized incremental logic</li> </ol>"},{"location":"#lab-2-metrics","title":"Lab 2 Metrics","text":"<ul> <li>Models: 5 (2 staging, 1 intermediate, 2 marts)</li> <li>Tests: 14 (all passing)</li> <li>Data Quality: 100%</li> <li>Documentation: 5 comprehensive guides</li> <li>Build Time: ~3 seconds</li> <li>Code Coverage: All models documented</li> </ul>"},{"location":"#learning-path","title":"Learning Path","text":""},{"location":"#beginners","title":"Beginners","text":"<ol> <li>Read Quick Start</li> <li>Complete Lab 1 setup</li> <li>Understand Medallion Architecture</li> </ol>"},{"location":"#intermediate","title":"Intermediate","text":"<ol> <li>Complete Lab 2 setup</li> <li>Study Lab 2 Data Models</li> <li>Learn dbt best practices</li> </ol>"},{"location":"#advanced","title":"Advanced","text":"<ol> <li>Customize Lab 2 for real data sources</li> <li>Build Lab 3 from scratch</li> <li>Implement Airflow orchestration</li> <li>Create production dashboards</li> </ol>"},{"location":"#contributing","title":"Contributing","text":""},{"location":"#report-issues","title":"Report Issues","text":"<p>Found a bug or have a suggestion? - Create a GitHub issue - Include error messages and steps to reproduce - Reference relevant documentation</p>"},{"location":"#improve-documentation","title":"Improve Documentation","text":"<ul> <li>Fix typos or unclear sections</li> <li>Add examples or diagrams</li> <li>Share your use cases</li> </ul>"},{"location":"#support_1","title":"Support","text":"<ul> <li>\ud83d\udcd6 Read the docs</li> <li>\u2753 Check FAQ</li> <li>\ud83d\udd27 Troubleshooting guides</li> <li>\ud83d\udc1b GitHub Issues</li> </ul>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Python: 3.11+</li> <li>OS: Mac 11+, Linux, or Windows with WSL2</li> <li>Memory: 2GB minimum</li> <li>Disk: 1GB base + data per lab</li> <li>Skills: Basic terminal, SQL, Python</li> </ul>"},{"location":"#quick-wins","title":"Quick Wins","text":"<p>Get started in 10 minutes: <pre><code># Clone\ngit clone https://github.com/narensham/buildcpg-labs.git\ncd buildcpg-labs/lab2_market_sentiment\n\n# Setup\npython3 -m venv lab2_env &amp;&amp; source lab2_env/bin/activate\npip install -r requirements.txt\n\n# Run\npython pipelines/ingest_sentiment.py\ncd dbt &amp;&amp; dbt deps &amp;&amp; dbt build\n\n# Success!\n# \u2705 14/14 tests passing\n</code></pre></p> <p>Platform: Multi-Lab Data Engineering Architecture: Independent labs + shared utilities Current Labs: 2 active, 1 planned Last Updated: November 2025 Maintainer: narensham Repository: GitHub Documentation: GitHub Pages</p>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#general-questions","title":"General Questions","text":""},{"location":"faq/#what-is-buildcpg-labs","title":"What is BuildCPG Labs?","text":"<p>A data engineering platform for managing multiple independent data labs using dbt, DuckDB, and Python. Each lab is isolated but shares common utilities.</p>"},{"location":"faq/#why-multiple-labs-instead-of-one-big-project","title":"Why multiple labs instead of one big project?","text":"<p>Different labs often have: - Different data sources and formats - Different transformation logic - Different schedules and SLAs - Independent teams working on them</p> <p>Keeping them separate prevents one from breaking others.</p>"},{"location":"faq/#do-i-need-to-use-all-labs","title":"Do I need to use all labs?","text":"<p>No. You can start with lab1 and create additional labs as needed. Each lab works independently.</p>"},{"location":"faq/#can-labs-share-data","title":"Can labs share data?","text":"<p>Currently, each lab has its own database. Future versions will support lab-to-lab data sharing through view exports.</p>"},{"location":"faq/#does-this-work-on-windows","title":"Does this work on Windows?","text":"<p>Yes, with WSL2 (Windows Subsystem for Linux). Mac 11+ and Linux are natively supported. No Docker required.</p>"},{"location":"faq/#architecture-questions","title":"Architecture Questions","text":""},{"location":"faq/#why-duckdb-instead-of-other-databases","title":"Why DuckDB instead of other databases?","text":"<ul> <li>No server needed (embedded)</li> <li>Works on older Macs (Mac 11+)</li> <li>No Docker required</li> <li>Small, easy to manage</li> <li>Perfect for single-machine setups</li> <li>Can scale to multi-machine later</li> </ul>"},{"location":"faq/#whats-the-difference-between-shared-and-lab-specific-files","title":"What's the difference between shared and lab-specific files?","text":"<p>Shared: Code used by ALL labs (DataInspector, CSVMonitor) Lab-specific: Code only that lab uses (models, dashboards, data)</p>"},{"location":"faq/#how-do-labs-communicate","title":"How do labs communicate?","text":"<p>They don't (by design). Future phases will add data export/import between labs.</p>"},{"location":"faq/#can-i-use-different-python-versions-per-lab","title":"Can I use different Python versions per lab?","text":"<p>Yes. Each lab has its own venv, so different packages/versions are allowed.</p>"},{"location":"faq/#setup-questions","title":"Setup Questions","text":""},{"location":"faq/#how-long-does-setup-take","title":"How long does setup take?","text":"<ul> <li>First time: 10-15 minutes (download, install dependencies)</li> <li>Each additional lab: 2 minutes (automatic bootstrap)</li> </ul>"},{"location":"faq/#can-i-skip-phase-1","title":"Can I skip Phase 1?","text":"<p>Not recommended. Phase 1 creates the foundation that Phases 2+ depend on.</p>"},{"location":"faq/#what-if-i-only-want-phase-1","title":"What if I only want Phase 1?","text":"<p>That's fine. Phase 1 is fully functional for basic lab work. Phase 2 adds convenience but isn't required.</p>"},{"location":"faq/#can-i-customize-the-templates","title":"Can I customize the templates?","text":"<p>Yes. Edit files in <code>shared/templates/</code> and they'll be used for new labs created after that.</p>"},{"location":"faq/#usage-questions","title":"Usage Questions","text":""},{"location":"faq/#what-does-make-setup-do","title":"What does <code>make setup</code> do?","text":"<ol> <li>Creates Python virtual environment (venv)</li> <li>Installs dependencies from requirements.txt</li> <li>Runs dbt debug to verify setup</li> </ol>"},{"location":"faq/#what-does-make-run-do","title":"What does <code>make run</code> do?","text":"<p>Executes <code>dbt run</code> which transforms data through bronze \u2192 silver \u2192 gold layers.</p>"},{"location":"faq/#how-do-i-see-what-changed-in-my-database","title":"How do I see what changed in my database?","text":"<p>Run <code>make inspect</code> to see schemas, tables, row counts, and quality scores.</p>"},{"location":"faq/#can-i-use-dbt-commands-directly","title":"Can I use dbt commands directly?","text":"<p>Yes. <code>cd lab/dbt &amp;&amp; dbt run</code> works the same as <code>make run</code>. Makefile just automates activation.</p>"},{"location":"faq/#how-do-i-add-a-new-model-to-lab1","title":"How do I add a new model to lab1?","text":"<pre><code>cd lab1_sales_performance/dbt/models\n# Create .sql file\n# Run: make run\n</code></pre> <p>See dbt documentation for model details.</p>"},{"location":"faq/#shared-utilities-questions","title":"Shared Utilities Questions","text":""},{"location":"faq/#how-do-i-use-datainspector-in-my-code","title":"How do I use DataInspector in my code?","text":"<pre><code>import sys\nsys.path.insert(0, '../..')\nfrom shared.utils.data_inspector import DataInspector\n\ninspector = DataInspector('data/lab1_sales_performance.duckdb')\nschemas = inspector.get_all_schemas()\n</code></pre>"},{"location":"faq/#can-i-modify-shared-utilities","title":"Can I modify shared utilities?","text":"<p>Yes, but test thoroughly. Changes affect ALL labs.</p>"},{"location":"faq/#how-do-i-add-a-new-shared-utility","title":"How do I add a new shared utility?","text":"<ol> <li>Create file in <code>shared/utils/</code></li> <li>Import it in your lab scripts</li> <li>Test thoroughly</li> <li>Commit to git</li> </ol>"},{"location":"faq/#do-shared-utilities-get-committed-to-git","title":"Do shared utilities get committed to git?","text":"<p>Yes. They're in the git repo so everyone gets them.</p>"},{"location":"faq/#data-questions","title":"Data Questions","text":""},{"location":"faq/#where-is-my-data-stored","title":"Where is my data stored?","text":"<p>In <code>lab/data/lab_name.duckdb</code> (DuckDB database file).</p>"},{"location":"faq/#how-do-i-backup-my-data","title":"How do I backup my data?","text":"<pre><code># Simply copy the .duckdb file\ncp lab1_sales_performance/data/lab1_sales_performance.duckdb lab1_sales_performance/data/lab1_sales_performance.duckdb.backup\n</code></pre>"},{"location":"faq/#how-large-can-databases-get","title":"How large can databases get?","text":"<p>DuckDB on a single machine can handle multi-GB databases easily. For 100GB+, consider upgrading infrastructure.</p>"},{"location":"faq/#can-i-read-external-data-in-dbt","title":"Can I read external data in dbt?","text":"<p>Yes. dbt supports reading from CSVs, APIs, and other sources via Jinja2 macros.</p>"},{"location":"faq/#how-do-i-connect-to-real-databases-not-duckdb","title":"How do I connect to real databases (not DuckDB)?","text":"<p>Future versions will support PostgreSQL, MySQL, etc. Currently limited to DuckDB.</p>"},{"location":"faq/#git-questions","title":"Git Questions","text":""},{"location":"faq/#can-i-commit-my-venv","title":"Can I commit my venv?","text":"<p>No. It's huge and system-specific. Use <code>.gitignore</code> to exclude it.</p>"},{"location":"faq/#can-i-commit-my-duckdb-files","title":"Can I commit my .duckdb files?","text":"<p>For development: no (they change constantly). For tracked data: yes (if you need version history).</p>"},{"location":"faq/#how-do-i-prevent-accidental-commits","title":"How do I prevent accidental commits?","text":"<p><code>.gitignore</code> automatically excludes venv, .duckdb, and build artifacts.</p>"},{"location":"faq/#what-should-i-commit","title":"What should I commit?","text":"<ul> <li>dbt models (.sql files)</li> <li>Python scripts (.py files)</li> <li>Configuration files (.yml, .yaml)</li> <li>Documentation (.md files)</li> <li>requirements.txt</li> </ul>"},{"location":"faq/#what-shouldnt-i-commit","title":"What shouldn't I commit?","text":"<ul> <li>venv/ directories</li> <li>.duckdb files (unless intentional)</li> <li>dbt/target/ and dbt/logs/</li> <li>.DS_Store</li> <li>IDE settings</li> </ul>"},{"location":"faq/#performance-questions","title":"Performance Questions","text":""},{"location":"faq/#how-fast-does-dbt-run","title":"How fast does dbt run?","text":"<p>Depends on data size. With ~3K rows: 1-2 seconds. Scale linearly with data.</p>"},{"location":"faq/#can-i-make-dbt-faster","title":"Can I make dbt faster?","text":"<ul> <li>Incremental materialization (load only new data)</li> <li>Optimize SQL queries</li> <li>Add indexes (if using real databases)</li> </ul>"},{"location":"faq/#will-my-laptop-slow-down-with-multiple-labs","title":"Will my laptop slow down with multiple labs?","text":"<p>Each lab runs independently. Multiple venvs take ~500MB each. Not a concern for 10 labs.</p>"},{"location":"faq/#scaling-questions","title":"Scaling Questions","text":""},{"location":"faq/#can-i-scale-to-50-labs","title":"Can I scale to 50 labs?","text":"<p>Yes. Structure supports unlimited labs. You'll just need: - Disk space (~1GB per lab average) - Orchestration tool (Airflow) to manage all runs</p>"},{"location":"faq/#whats-the-limit","title":"What's the limit?","text":"<p>No hard limit. Practical limit depends on infrastructure.</p>"},{"location":"faq/#can-i-migrate-to-cloud-later","title":"Can I migrate to cloud later?","text":"<p>Yes. The architecture is platform-agnostic. Move to Snowflake/BigQuery/etc. when needed.</p>"},{"location":"faq/#phase-questions","title":"Phase Questions","text":""},{"location":"faq/#whats-the-difference-between-phases","title":"What's the difference between phases?","text":"<ul> <li>Phase 1: Shared utilities + configuration</li> <li>Phase 2: Makefile + bootstrap (automation)</li> <li>Phase 3+: Orchestration + monitoring</li> </ul>"},{"location":"faq/#can-i-use-phase-1-without-phase-2","title":"Can I use Phase 1 without Phase 2?","text":"<p>Yes. Phase 1 is standalone and functional.</p>"},{"location":"faq/#when-should-i-use-phase-2","title":"When should I use Phase 2?","text":"<p>When you want <code>make</code> commands and automated lab creation. Highly recommended.</p>"},{"location":"faq/#when-should-i-use-phase-3","title":"When should I use Phase 3?","text":"<p>When managing 3+ labs and want automated scheduling (Airflow).</p>"},{"location":"faq/#team-questions","title":"Team Questions","text":""},{"location":"faq/#can-multiple-people-work-on-same-lab","title":"Can multiple people work on same lab?","text":"<p>Yes. Use git branches for features.</p>"},{"location":"faq/#can-different-people-own-different-labs","title":"Can different people own different labs?","text":"<p>Yes. Each lab is independent.</p>"},{"location":"faq/#how-do-we-prevent-conflicts","title":"How do we prevent conflicts?","text":"<ul> <li>Use git for coordination</li> <li>Clear lab ownership</li> <li>Communication on shared utilities</li> </ul>"},{"location":"faq/#can-we-code-review-lab-changes","title":"Can we code review lab changes?","text":"<p>Yes. Pull requests work for both lab-specific and shared code.</p>"},{"location":"faq/#troubleshooting-questions","title":"Troubleshooting Questions","text":""},{"location":"faq/#my-database-is-corrupted","title":"My database is corrupted","text":"<pre><code># Delete it and recreate\nrm lab1_sales_performance/data/lab1_sales_performance.duckdb\ncd lab1_sales_performance/dbt\ndbt run  # Recreates from source\n</code></pre>"},{"location":"faq/#i-accidentally-deleted-a-model","title":"I accidentally deleted a model","text":"<pre><code>git checkout &lt;file&gt;  # Restore from git\ncd dbt\ndbt run  # Recreate tables\n</code></pre>"},{"location":"faq/#everything-is-broken","title":"Everything is broken","text":"<pre><code>git status  # See what changed\ngit diff   # See details\ngit reset --hard  # Go back to last commit\n</code></pre>"},{"location":"faq/#who-do-i-ask-for-help","title":"Who do I ask for help?","text":"<p>See Troubleshooting guide or ask maintainer.</p>"},{"location":"faq/#feature-questions","title":"Feature Questions","text":""},{"location":"faq/#can-i-add-monitoringalerting","title":"Can I add monitoring/alerting?","text":"<p>Yes, in Phase 3+. Currently manual checks only.</p>"},{"location":"faq/#can-i-use-python-notebooks","title":"Can I use Python notebooks?","text":"<p>Yes. Put them in <code>lab/notebooks/</code>.</p>"},{"location":"faq/#can-i-have-dashboards","title":"Can I have dashboards?","text":"<p>Yes. Connect BI tools (Tableau/Looker) to lab databases.</p>"},{"location":"faq/#can-i-do-ml","title":"Can I do ML?","text":"<p>Yes. Add models to <code>lab/pipelines/</code> or use Jupyter.</p>"},{"location":"faq/#still-have-questions","title":"Still Have Questions?","text":"<ul> <li>Check Getting Started</li> <li>Check Troubleshooting</li> <li>Check Architecture</li> <li>Ask in GitHub Issues</li> </ul>"},{"location":"troubleshooting/","title":"Troubleshooting Guide","text":"<p>Solutions to common issues in BuildCPG Labs.</p>"},{"location":"troubleshooting/#installation-issues","title":"Installation Issues","text":""},{"location":"troubleshooting/#python-version-error","title":"Python Version Error","text":"<p>Problem: <code>python</code> command not found or shows Python 2.x</p> <p>Solution: <pre><code># Check available versions\nwhich python3\npython3 --version\n\n# Use python3 instead of python\npython3 -m venv venv\npython3 -m pip install pyyaml\n</code></pre></p>"},{"location":"troubleshooting/#virtual-environment-not-activating","title":"Virtual Environment Not Activating","text":"<p>Problem: <code>source venv/bin/activate</code> doesn't work on Mac</p> <p>Solution: <pre><code># Try full path\nsource ./venv/bin/activate\n\n# Or use this directly\n./venv/bin/python --version\n\n# For Zsh on newer Mac:\nsource venv/bin/activate\n</code></pre></p>"},{"location":"troubleshooting/#pip-command-not-found","title":"pip: command not found","text":"<p>Problem: <code>pip install</code> doesn't work</p> <p>Solution: <pre><code># Use python -m pip instead\npython -m pip install pyyaml duckdb pandas\n\n# Or upgrade pip\npython -m pip install --upgrade pip\n</code></pre></p>"},{"location":"troubleshooting/#permission-denied-error","title":"Permission Denied Error","text":"<p>Problem: <code>permission denied: ./setup.sh</code></p> <p>Solution: <pre><code>chmod +x setup.sh\n./setup.sh\n</code></pre></p>"},{"location":"troubleshooting/#dbt-issues","title":"dbt Issues","text":""},{"location":"troubleshooting/#dbt-command-not-found","title":"dbt: command not found","text":"<p>Problem: <code>dbt run</code> or <code>dbt debug</code> doesn't work</p> <p>Solution: <pre><code># Make sure you're in lab directory\ncd lab1_sales_performance\n\n# Make sure venv is activated\nsource venv/bin/activate\n\n# Try again\ndbt --version\n\n# If still fails, reinstall\npip install dbt-duckdb\n</code></pre></p>"},{"location":"troubleshooting/#dbt-debug-shows-errors","title":"dbt debug shows errors","text":"<p>Problem: <code>dbt debug</code> fails with configuration errors</p> <p>Solution: <pre><code># Check you're in dbt directory\ncd lab1_sales_performance/dbt\n\n# Verify profiles.yml exists\nls -la profiles.yml\n\n# Check dbt_project.yml\nls -la dbt_project.yml\n\n# Run debug again\ndbt debug\n</code></pre></p>"},{"location":"troubleshooting/#profile-not-found","title":"Profile not found","text":"<p>Problem: <code>Profile 'lab1_duckdb' not found</code></p> <p>Solution: <pre><code># Check profiles.yml path\ncat ~/.dbt/profiles.yml\n\n# Or ensure it's in the lab directory\ncd lab1_sales_performance/dbt\ncat profiles.yml\n\n# Update dbt_project.yml to point to correct profile\n</code></pre></p>"},{"location":"troubleshooting/#data-inspector-issues","title":"Data Inspector Issues","text":""},{"location":"troubleshooting/#modulenotfounderror-no-module-named-shared","title":"ModuleNotFoundError: No module named 'shared'","text":"<p>Problem: Python can't find shared utilities</p> <p>Solution: <pre><code># Make sure you're in lab directory\ncd lab1_sales_performance\n\n# Run from correct location\npython scripts/inspect_data.py\n\n# Verify path in script\nhead -20 scripts/inspect_data.py\n# Should show: sys.path.insert(0, '../..')\n</code></pre></p>"},{"location":"troubleshooting/#filenotfounderror-labs_configyaml","title":"FileNotFoundError: labs_config.yaml","text":"<p>Problem: Configuration file not found</p> <p>Solution: <pre><code># Check file exists\nls -la config/labs_config.yaml\n\n# Create if missing\ntouch config/labs_config.yaml\n\n# Add content\ncat &gt; config/labs_config.yaml &lt;&lt; 'EOF'\nlabs:\n  lab1_sales_performance:\n    path: lab1_sales_performance\n    db_path: lab1_sales_performance/data/lab1_sales_performance.duckdb\n    dbt_path: lab1_sales_performance/dbt\nEOF\n</code></pre></p>"},{"location":"troubleshooting/#database-file-not-found","title":"Database file not found","text":"<p>Problem: <code>Cannot open file \".../lab1_sales_performance.duckdb\"</code></p> <p>Solution: <pre><code># Check database exists\nls -la lab1_sales_performance/data/\n\n# Create database by running dbt\ncd lab1_sales_performance/dbt\ndbt run\n\n# Then try inspection again\ncd ..\npython scripts/inspect_data.py\n</code></pre></p>"},{"location":"troubleshooting/#git-issues","title":"Git Issues","text":""},{"location":"troubleshooting/#too-many-loose-objects-in-repository","title":"Too many loose objects in repository","text":"<p>Problem: <code>warning: There are too many unreachable loose objects</code></p> <p>Solution: <pre><code># Remove gc.log if it exists\nrm .git/gc.log 2&gt;/dev/null || true\n\n# Prune repository\ngit prune\n\n# Optimize repository\ngit gc --aggressive\n</code></pre></p>"},{"location":"troubleshooting/#venv-directory-accidentally-committed","title":"venv directory accidentally committed","text":"<p>Problem: Tried to push and got huge file size</p> <p>Solution: <pre><code># Remove from git tracking (keeps files on disk)\ngit rm -r --cached venv/\ngit rm -r --cached .venv/\ngit rm -r --cached dbt/target/\n\n# Update .gitignore\necho \"venv/\" &gt;&gt; .gitignore\necho \".venv/\" &gt;&gt; .gitignore\necho \"dbt/target/\" &gt;&gt; .gitignore\n\n# Commit cleanup\ngit add .gitignore\ngit commit -m \"Remove build artifacts from git\"\n</code></pre></p>"},{"location":"troubleshooting/#makefile-issues","title":"Makefile Issues","text":""},{"location":"troubleshooting/#make-command-not-found","title":"make: command not found","text":"<p>Problem: <code>make</code> is not installed</p> <p>Solution (Mac): <pre><code># Install Xcode command line tools\nxcode-select --install\n\n# Or install make directly\nbrew install make\n</code></pre></p> <p>Solution (Linux): <pre><code>sudo apt-get update\nsudo apt-get install make build-essential\n</code></pre></p>"},{"location":"troubleshooting/#tab-indentation-error-in-makefile","title":"Tab indentation error in Makefile","text":"<p>Problem: <code>missing separator. Stop.</code> when running make</p> <p>Solution: <pre><code># Makefiles require TAB indentation, not spaces\n# Edit Makefile and ensure all recipe lines start with TAB\n\n# In vim:\n# :set noexpandtab\n\n# In nano:\n# Ctrl+I to insert tab\n\n# Visual check:\ncat -A Makefile | head -20\n# Tabs show as ^I, spaces show as regular spaces\n</code></pre></p>"},{"location":"troubleshooting/#venv-not-found-when-running-make","title":"venv not found when running make","text":"<p>Problem: <code>venv/bin/python: No such file or directory</code></p> <p>Solution: <pre><code># Run setup first\nmake setup\n\n# This creates venv and installs dependencies\n</code></pre></p>"},{"location":"troubleshooting/#common-data-issues","title":"Common Data Issues","text":""},{"location":"troubleshooting/#no-schemas-in-database","title":"No schemas in database","text":"<p>Problem: <code>inspect_data.py</code> shows only 'main' and 'information_schema'</p> <p>Solution: <pre><code># Tables haven't been created yet\n# Create them with dbt\ncd lab1_sales_performance/dbt\ndbt run\n\n# Then inspect again\ncd ..\npython scripts/inspect_data.py\n</code></pre></p>"},{"location":"troubleshooting/#quality-score-shows-0","title":"Quality score shows 0%","text":"<p>Problem: <code>get_quality_score</code> returns 0 even with good data</p> <p>Solution: <pre><code># Check table has rows\npython -c \"\nimport sys\nsys.path.insert(0, '../..')\nfrom shared.utils.data_inspector import DataInspector\ninspector = DataInspector('data/lab1_sales_performance.duckdb')\nprint(inspector.get_table_stats('raw', 'sales_data'))\n\"\n\n# If rows = 0, data wasn't loaded\n# Load it with dbt:\ncd dbt\ndbt run\n</code></pre></p>"},{"location":"troubleshooting/#csv-says-no-new-data-but-file-was-updated","title":"CSV says \"no new data\" but file was updated","text":"<p>Problem: <code>check_for_new_data.py</code> doesn't detect changes</p> <p>Solution: <pre><code># State file might be outdated\n# Remove it to reset detection\nrm lab1_sales_performance/.csv_state.json\n\n# Run again\ncd lab1_sales_performance\npython scripts/check_for_new_data.py\n</code></pre></p>"},{"location":"troubleshooting/#getting-help","title":"Getting Help","text":""},{"location":"troubleshooting/#enable-debug-logging","title":"Enable Debug Logging","text":"<pre><code># For dbt:\ndbt run --debug\n\n# For Python scripts:\n# Add this to beginning of script:\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre>"},{"location":"troubleshooting/#check-file-permissions","title":"Check File Permissions","text":"<pre><code># Verify script is executable\nls -la scripts/inspect_data.py\n\n# Make executable if needed\nchmod +x scripts/inspect_data.py\n</code></pre>"},{"location":"troubleshooting/#verify-directory-structure","title":"Verify Directory Structure","text":"<pre><code># From buildcpg-labs root:\ntree -L 2 -I '__pycache__|*.pyc|venv'\n\n# Or list key files:\nfind . -name \"dbt_project.yml\" -o -name \"labs_config.yaml\" -o -name \"Makefile\"\n</code></pre>"},{"location":"troubleshooting/#check-disk-space","title":"Check Disk Space","text":"<pre><code># Verify you have space\ndf -h\n\n# Check database size\ndu -sh lab1_sales_performance/data/\n</code></pre>"},{"location":"troubleshooting/#still-stuck","title":"Still Stuck?","text":"<ol> <li>Check FAQ for common questions</li> <li>Review Quick Start</li> <li>Check Architecture to understand the design</li> <li>Review the exact error message and search in this guide</li> <li>Check git commit history for clues: <code>git log --oneline</code> </li> </ol>"},{"location":"architecture/medallion-architecture/","title":"Medallion Architecture","text":""},{"location":"architecture/medallion-architecture/#overview","title":"Overview","text":"<p>The Medallion Architecture organizes data pipelines into progressive layers to ensure quality, reliability, and efficiency in analytics.</p>"},{"location":"architecture/medallion-architecture/#layers","title":"Layers","text":"<ol> <li> <p>Bronze (Raw)    Raw ingested data from source systems. Minimal transformation, only cleaned for parsing errors.</p> </li> <li> <p>Silver (Cleaned / Curated)    Data is cleaned, deduplicated, and enriched. Suitable for basic analytics and reporting.</p> </li> <li> <p>Gold (Business-ready / Aggregated)    Fully transformed data optimized for dashboards, ML, and advanced analytics.</p> </li> </ol>"},{"location":"architecture/medallion-architecture/#principles","title":"Principles","text":"<ul> <li>Incremental processing: Each layer adds value without overwriting prior stages.</li> <li>Traceability: Easy to trace data lineage from Gold \u2192 Silver \u2192 Bronze.</li> <li>Reusability: Models at Silver and Gold layers can be shared across multiple labs.</li> </ul>"},{"location":"architecture/medallion-architecture/#benefits","title":"Benefits","text":"<ul> <li>Reduces data quality issues downstream.</li> <li>Enables faster experimentation and analytics.</li> <li>Facilitates modular ETL development in a multi-lab environment.</li> </ul>"},{"location":"architecture/medallion-architecture/#visual-diagram","title":"Visual Diagram","text":""},{"location":"architecture/multi-lab-design/","title":"Multi-Lab Design","text":""},{"location":"architecture/multi-lab-design/#overview","title":"Overview","text":"<p>The Multi-Lab Design organizes a data engineering playground into multiple independent labs, each representing a self-contained ETL pipeline or analytical experiment. This design ensures modularity, safety, and flexibility, allowing teams or individuals to experiment without affecting other projects.</p>"},{"location":"architecture/multi-lab-design/#goals","title":"Goals","text":"<ul> <li>Isolation: Each lab operates independently with its own datasets, models, and transformations.</li> <li>Modularity: Labs can be added, removed, or updated without impacting the broader system.</li> <li>Reusability: Shared components or datasets can be referenced across labs while maintaining separation of experimental data.</li> </ul>"},{"location":"architecture/multi-lab-design/#lab-structure","title":"Lab Structure","text":""},{"location":"architecture/multi-lab-design/#typical-lab-folder","title":"Typical Lab Folder","text":"<pre><code>lab1_sales_performance/\n\u251c\u2500\u2500 raw/\n\u251c\u2500\u2500 bronze/\n\u251c\u2500\u2500 silver/\n\u251c\u2500\u2500 gold/\n\u251c\u2500\u2500 dbt/\n\u2502 \u251c\u2500\u2500 models/\n\u2502 \u251c\u2500\u2500 snapshots/\n\u2502 \u2514\u2500\u2500 seeds/\n\u2514\u2500\u2500 README.md\n</code></pre> <ul> <li>raw: Ingested source data (unchanged).  </li> <li>bronze: Cleaned, minimally transformed data.  </li> <li>silver: Enriched and validated datasets.  </li> <li>gold: Business-ready, aggregated tables for analysis or reporting.  </li> <li>dbt: Contains lab-specific transformations and metadata.  </li> </ul>"},{"location":"architecture/multi-lab-design/#shared-vs-lab-specific","title":"Shared vs Lab-Specific","text":"<ul> <li>Lab-specific datasets: Only relevant to a single lab; changes are isolated.  </li> <li>Shared datasets: Common reference tables or utilities used across multiple labs, stored in a central <code>shared/</code> directory.  </li> </ul> <p>\ud83d\udca1 Tip: Only include data in <code>shared/</code> if multiple labs depend on it. Avoid making everything shared to prevent accidental coupling.</p>"},{"location":"architecture/multi-lab-design/#benefits-of-multi-lab-design","title":"Benefits of Multi-Lab Design","text":"<ul> <li>Safe experimentation: Teams can iterate freely without breaking other pipelines.  </li> <li>Scalability: Easily add new labs for different datasets or analytical scenarios.  </li> <li>Reproducibility: Clear boundaries make it easy to reproduce experiments or roll back changes.  </li> <li>Collaboration: Multiple contributors can work in parallel without conflicts.</li> </ul>"},{"location":"architecture/multi-lab-design/#best-practices","title":"Best Practices","text":"<ol> <li>Maintain consistent folder structures across labs.  </li> <li>Clearly document each lab\u2019s purpose in its README.  </li> <li>Version shared datasets and transformations carefully.  </li> <li>Use naming conventions to avoid collisions (e.g., <code>lab1_sales_2025_bronze</code>).  </li> <li>Leverage automation (e.g., CI/CD pipelines) to validate transformations before deployment.</li> </ol>"},{"location":"architecture/multi-lab-design/#example-diagram","title":"Example Diagram","text":"<p><pre><code>+----------------+     +----------------+     +----------------+\n|   Lab 1        |     |   Lab 2        |     |   Lab 3        |\n|   raw \u2192 bronze |     | raw \u2192 bronze   |     | raw \u2192 bronze   |\n|   \u2192 silver     |     | \u2192 silver       |     | \u2192 silver       |\n|   \u2192 gold       |     | \u2192 gold         |     | \u2192 gold         |\n+----------------+     +----------------+     +----------------+\n         \\                   |                     /\n          \\                  |                    /\n           \\                 |                   /\n            \\                |                  /\n             \\               |                 /\n              +--------------------------------+\n              |           Shared Data          |\n              +--------------------------------+\n</code></pre> This diagram shows independent labs consuming shared datasets while maintaining isolated pipelines.</p>"},{"location":"architecture/multi-lab-design/#summary","title":"Summary","text":"<p>The Multi-Lab Design is ideal for learning, experimentation, and scalable analytics. By enforcing modularity and isolation, it enables reproducible workflows and safe collaboration while maintaining the flexibility to share data and reusable transformations across projects.</p>"},{"location":"architecture/overview/","title":"Architecture Overview","text":""},{"location":"architecture/overview/#system-design","title":"System Design","text":"<p>BuildCPG Labs uses a multi-lab, shared-utilities architecture designed for scalability and independence.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         ORCHESTRATION LAYER (Airflow/Prefect)           \u2502\n\u2502  Schedules pipelines, monitors health, handles alerts   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              DATA INGESTION LAYER                        \u2502\n\u2502  \u2022 CSV monitoring \u2022 API polling \u2022 Data validation       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         dbt TRANSFORMATION LAYER (Per Lab)              \u2502\n\u2502  \u2022 Bronze (Raw) \u2192 Silver (Cleaned) \u2192 Gold (Analytics)  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              QUALITY ASSURANCE LAYER                     \u2502\n\u2502  \u2022 dbt tests \u2022 Data freshness \u2022 Quality scoring         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         SHARED UTILITIES LAYER (Root Level)             \u2502\n\u2502  \u2022 DataInspector \u2022 CSVMonitor \u2022 Path Helpers            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/overview/#directory-structure-current-setup","title":"Directory Structure (Current Setup)","text":"<pre><code>buildcpg-labs/\n\u2502\n\u251c\u2500\u2500 .venv/                           # SINGLE venv (shared by all labs)\n\u2502   \u251c\u2500\u2500 bin/\n\u2502   \u251c\u2500\u2500 lib/\n\u2502   \u2514\u2500\u2500 ...\n\u2502\n\u251c\u2500\u2500 shared/                          # Shared across ALL labs\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 utils/\n\u2502   \u2502   \u251c\u2500\u2500 data_inspector.py       # Inspect databases\n\u2502   \u2502   \u251c\u2500\u2500 csv_monitor.py          # Detect new data\n\u2502   \u2502   \u2514\u2500\u2500 config_loader.py        # Load configurations\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 config/                      # Config inside shared (not at root)\n\u2502   \u2502   \u251c\u2500\u2500 labs_config.yaml        # Central lab registry\n\u2502   \u2502   \u2514\u2500\u2500 paths.py                # Path helpers\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 data_quality/\n\u2502   \u2502   \u251c\u2500\u2500 validators.py           # Quality validators\n\u2502   \u2502   \u2514\u2500\u2500 expectations.py         # Data expectations\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 templates/\n\u2502       \u251c\u2500\u2500 Makefile_template       # Template Makefile\n\u2502       \u251c\u2500\u2500 requirements_template.txt\n\u2502       \u251c\u2500\u2500 dbt_project_template.yml\n\u2502       \u2514\u2500\u2500 .gitignore_template\n\u2502\n\u251c\u2500\u2500 lab1_sales_performance/         # LAB 1 (Independent data, shares venv)\n\u2502   \u251c\u2500\u2500 dbt/\n\u2502   \u2502   \u251c\u2500\u2500 dbt_project.yml\n\u2502   \u2502   \u251c\u2500\u2500 profiles.yml            # Manually edited when switching labs\n\u2502   \u2502   \u251c\u2500\u2500 models/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 staging/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 intermediate/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 marts/\n\u2502   \u2502   \u2514\u2500\u2500 tests/\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 data/\n\u2502   \u2502   \u251c\u2500\u2500 raw/\n\u2502   \u2502   \u2514\u2500\u2500 lab1_sales_performance.duckdb\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 scripts/\n\u2502   \u2502   \u251c\u2500\u2500 inspect_data.py\n\u2502   \u2502   \u2514\u2500\u2500 check_for_new_data.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 pipelines/\n\u2502   \u2502   \u251c\u2500\u2500 data_ingestion.py\n\u2502   \u2502   \u2514\u2500\u2500 data_quality.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 requirements.txt            # Shared dependencies\n\u2502   \u2514\u2500\u2500 .gitignore\n\u2502\n\u251c\u2500\u2500 lab2_forecast_model/            # LAB 2 (Independent data, shares venv)\n\u2502   \u2514\u2500\u2500 (Same structure as lab1)\n\u2502\n\u251c\u2500\u2500 lab3_customer_segmentation/     # LAB 3 (Independent data, shares venv)\n\u2502   \u2514\u2500\u2500 (Same structure as lab1)\n\u2502\n\u251c\u2500\u2500 orchestration/\n\u2502   \u2514\u2500\u2500 airflow_dags.py             # Multi-lab orchestration\n\u2502\n\u251c\u2500\u2500 docs/                           # This documentation\n\u251c\u2500\u2500 setup_new_lab.sh                # Bootstrap new labs\n\u251c\u2500\u2500 mkdocs.yml                      # Documentation config\n\u251c\u2500\u2500 .gitignore\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"architecture/overview/#key-concepts","title":"Key Concepts","text":""},{"location":"architecture/overview/#single-virtual-environment-approach","title":"Single Virtual Environment Approach","text":"<p>Current Setup: - ONE <code>.venv/</code> at root shared by all labs - All labs use same Python packages - Switch between labs by changing dbt profiles manually</p> <p>Advantages: - Less disk space (~500MB vs ~500MB per lab) - Consistent package versions across all labs - Simpler initial setup</p> <p>Trade-offs: - Cannot have labs with conflicting dependencies - Must manually edit <code>profiles.yml</code> when switching labs - Risk of profile switching errors (writing to wrong database) - No concurrent work on different labs</p> <p>\u26a0\ufe0f See Current Setup Details for full pros/cons analysis</p>"},{"location":"architecture/overview/#labs-have-independent-data","title":"Labs Have Independent Data","text":"<ul> <li>Each lab has its own database (lab1.duckdb, lab2.duckdb)</li> <li>Each lab has its own dbt project (dbt/models/, dbt/dbt_project.yml)</li> <li>Each lab has its own raw data (data/raw/)</li> <li>Labs share Python environment but NOT data</li> </ul>"},{"location":"architecture/overview/#shared-utilities","title":"Shared Utilities","text":"<ul> <li>DataInspector - Check database quality (used by all labs)</li> <li>CSVMonitor - Detect new data in CSVs (used by all labs)</li> <li>Config Paths - Get paths for any lab (used by all labs)</li> <li>Written once in <code>shared/</code>, used by all labs</li> <li>Bug fix in shared code fixes all labs</li> </ul>"},{"location":"architecture/overview/#configuration-location","title":"Configuration Location","text":"<p>Config is inside shared/config/ (not at root level):</p> <pre><code># Import pattern for lab scripts\nimport sys\nsys.path.insert(0, '../..')\nfrom shared.config.paths import get_lab_db_path  # Note: shared.config\n\n# NOT this:\n# from config.paths import get_lab_db_path  # \u274c Wrong path\n</code></pre> <pre><code># shared/config/labs_config.yaml\nlabs:\n  lab1_sales_performance:\n    path: lab1_sales_performance\n    db_path: lab1_sales_performance/data/lab1_sales_performance.duckdb\n    dbt_path: lab1_sales_performance/dbt\n\n  lab2_forecast_model:\n    path: lab2_forecast_model\n    db_path: lab2_forecast_model/data/lab2_forecast_model.duckdb\n    dbt_path: lab2_forecast_model/dbt\n</code></pre> <p>This registry tells the system where each lab is and how to find its resources.</p>"},{"location":"architecture/overview/#data-flow","title":"Data Flow","text":""},{"location":"architecture/overview/#single-lab-example","title":"Single Lab Example","text":"<pre><code>CSV Input\n    \u2193\ndbt Load (raw schema)\n    \u2193\ndbt Transform (bronze \u2192 silver \u2192 gold)\n    \u2193\nDuckDB Tables\n    \u2193\nDataInspector (quality check)\n    \u2193\nBI Tool (Tableau/Looker)\n</code></pre>"},{"location":"architecture/overview/#multiple-labs-orchestrated","title":"Multiple Labs Orchestrated","text":"<pre><code>Airflow DAG (Daily 12 AM)\n    \u251c\u2500\u2500 Lab1: Check CSV \u2192 Load \u2192 Transform \u2192 Test \u2192 Inspect\n    \u251c\u2500\u2500 Lab2: Check CSV \u2192 Load \u2192 Transform \u2192 Test \u2192 Inspect\n    \u2514\u2500\u2500 Lab3: Check CSV \u2192 Load \u2192 Transform \u2192 Test \u2192 Inspect\n    \u2193\nAll results aggregated\n    \u2193\nAlert team if any lab fails\n</code></pre>"},{"location":"architecture/overview/#workflow-switching-between-labs","title":"Workflow: Switching Between Labs","text":"<pre><code># 1. Activate shared venv (once per session)\ncd buildcpg-labs\nsource .venv/bin/activate\n\n# 2. Work on lab1\ncd lab1_sales_performance/dbt\n# profiles.yml should point to: ../data/lab1_sales_performance.duckdb\ndbt debug  # Verify correct database\ndbt run\n\n# 3. Switch to lab2\ncd ../../lab2_forecast_model/dbt\n# Edit profiles.yml to point to: ../data/lab2_forecast_model.duckdb\nvim profiles.yml  # Update path\ndbt debug  # Verify correct database\ndbt run\n</code></pre> <p>\u26a0\ufe0f Critical: Always run <code>dbt debug</code> before <code>dbt run</code> to verify you're pointing to the correct database.</p>"},{"location":"architecture/overview/#technology-stack","title":"Technology Stack","text":"Layer Technology Database DuckDB (embedded, Mac compatible) Transformation dbt (data build tool) Scripting Python 3.11+ Environment Single venv (shared) Orchestration Airflow (optional, future) Version Control Git Documentation MkDocs"},{"location":"architecture/overview/#design-principles","title":"Design Principles","text":""},{"location":"architecture/overview/#1-data-independence","title":"1. Data Independence","text":"<p>Labs have separate databases and raw data. One lab's data corruption doesn't affect others.</p>"},{"location":"architecture/overview/#2-shared-python-environment","title":"2. Shared Python Environment","text":"<p>All labs use same venv for consistency and space efficiency (with trade-offs).</p>"},{"location":"architecture/overview/#3-reusability","title":"3. Reusability","text":"<p>Code written for shared utilities is used by all labs without duplication.</p>"},{"location":"architecture/overview/#4-scalability","title":"4. Scalability","text":"<p>Adding lab 10 takes same effort as adding lab 2 (but dependency conflicts may limit this).</p>"},{"location":"architecture/overview/#5-clarity","title":"5. Clarity","text":"<p>Each lab's purpose is clear. Shared code's purpose is clear.</p>"},{"location":"architecture/overview/#6-manual-coordination","title":"6. Manual Coordination","text":"<p>Profile switching requires discipline and verification steps.</p>"},{"location":"architecture/overview/#comparison-current-vs-alternative-architectures","title":"Comparison: Current vs Alternative Architectures","text":""},{"location":"architecture/overview/#current-setup-single-venv","title":"Current Setup (Single venv)","text":"<p>\u2705 Space efficient (one venv) \u2705 Consistent packages \u2705 Simple setup \u274c Dependency conflicts possible \u274c Manual profile switching \u274c No concurrent work  </p>"},{"location":"architecture/overview/#alternative-per-lab-venvs","title":"Alternative: Per-Lab venvs","text":"<p>\u274c More disk space \u274c More complex setup \u2705 Complete isolation \u2705 Different dependencies per lab \u2705 Automatic profile management \u2705 Concurrent work safe  </p> <p>When to migrate: See Current Setup Analysis</p>"},{"location":"architecture/overview/#migration-path","title":"Migration Path","text":""},{"location":"architecture/overview/#current-state-phase-1-single-venv","title":"Current State: Phase 1 (Single venv)","text":"<ul> <li>Shared utilities \u2705</li> <li>Central configuration \u2705</li> <li>Lab1 working \u2705</li> <li>Single venv \u2705</li> </ul>"},{"location":"architecture/overview/#future-phase-2-optional-migration-to-per-lab-venvs","title":"Future: Phase 2 (Optional migration to per-lab venvs)","text":"<p>When you hit: - 3+ labs - Dependency conflicts - Multiple team members - Production requirements</p> <p>Then consider: Per-lab venvs + automated profile management</p>"},{"location":"architecture/overview/#see-also","title":"See Also","text":"<ul> <li>Current Setup Details - Full analysis of single venv approach</li> <li>Shared vs Lab-Specific - What goes where</li> <li>Phase 1 Guide - Implementation details</li> </ul>"},{"location":"architecture/shared-vs-lab-specific/","title":"Shared vs Lab-Specific Data","text":""},{"location":"architecture/shared-vs-lab-specific/#overview","title":"Overview","text":"<p>In a multi-lab environment, some data is shared across labs, while other datasets remain lab-specific. Proper organization is crucial for maintainability and scalability.</p>"},{"location":"architecture/shared-vs-lab-specific/#lab-specific-data","title":"Lab-Specific Data","text":"<ul> <li>Stored within the lab\u2019s dedicated folder.</li> <li>Includes lab experiments, transformations, and temporary datasets.</li> <li>Changes affect only the corresponding lab.</li> </ul>"},{"location":"architecture/shared-vs-lab-specific/#shared-data","title":"Shared Data","text":"<ul> <li>Placed in a centralized <code>shared</code> directory.</li> <li>Includes:</li> <li>Standardized reference datasets (e.g., product catalogs, master lists)</li> <li>Common transformations or utility tables</li> <li>Any updates must consider impact on all labs consuming the shared data.</li> </ul>"},{"location":"architecture/shared-vs-lab-specific/#guidelines","title":"Guidelines","text":"<ol> <li>Only include truly reusable datasets in <code>shared</code>.</li> <li>Maintain versioning for shared datasets to prevent breaking changes.</li> <li>Document dependencies clearly in lab README files.</li> </ol>"},{"location":"architecture/shared-vs-lab-specific/#benefits","title":"Benefits","text":"<ul> <li>Prevents accidental data contamination.</li> <li>Encourages modularity and reuse.</li> <li>Simplifies debugging and troubleshooting across labs.</li> </ul>"},{"location":"getting-started/installation/","title":"Installation Guide","text":"<p>Complete setup instructions for BuildCPG Labs.</p>"},{"location":"getting-started/installation/#system-requirements","title":"System Requirements","text":"<ul> <li>Python: 3.11 or higher</li> <li>OS: Mac 11+ or Linux (Windows with WSL2)</li> <li>Memory: 2GB minimum</li> <li>Disk: 1GB for base installation + data</li> </ul>"},{"location":"getting-started/installation/#step-1-install-prerequisites","title":"Step 1: Install Prerequisites","text":""},{"location":"getting-started/installation/#install-homebrew-mac-only","title":"Install Homebrew (Mac Only)","text":"<pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre>"},{"location":"getting-started/installation/#install-python-311","title":"Install Python 3.11+","text":"<p>Using Homebrew (Mac): <pre><code>brew install python@3.11\n</code></pre></p> <p>Using System Package Manager (Linux): <pre><code>sudo apt-get update\nsudo apt-get install python3.11 python3.11-venv\n</code></pre></p>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>python3 --version\n# Should show: Python 3.11.x or higher\n\npip --version\n# Should show: pip 24.0 or higher\n</code></pre>"},{"location":"getting-started/installation/#step-2-clone-repository","title":"Step 2: Clone Repository","text":""},{"location":"getting-started/installation/#using-https","title":"Using HTTPS","text":"<pre><code>git clone https://github.com/narensham/buildcpg-labs.git\ncd buildcpg-labs\n</code></pre>"},{"location":"getting-started/installation/#or-using-ssh","title":"Or Using SSH","text":"<pre><code>git clone git@github.com:narensham/buildcpg-labs.git\ncd buildcpg-labs\n</code></pre>"},{"location":"getting-started/installation/#step-3-install-global-dependencies","title":"Step 3: Install Global Dependencies","text":"<p>These packages are needed at the root level:</p> <pre><code>pip install --upgrade pip\npip install pyyaml duckdb pandas mkdocs mkdocs-material\n</code></pre>"},{"location":"getting-started/installation/#step-4-verify-setup","title":"Step 4: Verify Setup","text":""},{"location":"getting-started/installation/#check-python","title":"Check Python","text":"<pre><code>python --version\n# Should show: Python 3.11.x or higher\n</code></pre>"},{"location":"getting-started/installation/#check-git","title":"Check Git","text":"<pre><code>git --version\n# Should show: git version 2.x or higher\n</code></pre>"},{"location":"getting-started/installation/#check-directory-structure","title":"Check Directory Structure","text":"<pre><code>ls -la\n# Should show: shared/, config/, lab1_sales_performance/, mkdocs.yml, etc.\n</code></pre>"},{"location":"getting-started/installation/#test-config","title":"Test Config","text":"<pre><code>python config/paths.py\n# Expected output:\n# \u2705 Lab1 config loaded: ...\n# \u2705 Lab1 DB path: /path/to/lab1_sales_performance/data/lab1_sales_performance.duckdb\n</code></pre>"},{"location":"getting-started/installation/#step-5-set-up-lab-1","title":"Step 5: Set Up Lab 1","text":""},{"location":"getting-started/installation/#create-lab-environment","title":"Create Lab Environment","text":"<pre><code>cd lab1_sales_performance\npython3 -m venv venv\nsource venv/bin/activate\n\n# If using Mac Zsh and venv doesn't activate:\nsource venv/bin/activate\n</code></pre>"},{"location":"getting-started/installation/#install-lab-dependencies","title":"Install Lab Dependencies","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"getting-started/installation/#verify-dbt","title":"Verify dbt","text":"<pre><code>dbt --version\n# Should show: dbt version 1.x.x\n</code></pre>"},{"location":"getting-started/installation/#check-dbt-configuration","title":"Check dbt Configuration","text":"<pre><code>cd dbt\ndbt debug\n# Should show: \u2705 All checks passed!\n</code></pre>"},{"location":"getting-started/installation/#step-6-verify-everything-works","title":"Step 6: Verify Everything Works","text":""},{"location":"getting-started/installation/#test-data-inspector","title":"Test Data Inspector","text":"<pre><code>cd /path/to/buildcpg-labs/lab1_sales_performance\npython scripts/inspect_data.py\n# Should show: Schemas, tables, row counts\n</code></pre>"},{"location":"getting-started/installation/#test-csv-monitor","title":"Test CSV Monitor","text":"<pre><code>python scripts/check_for_new_data.py\n# Should show: CSV file info and last modification\n</code></pre>"},{"location":"getting-started/installation/#test-config-access","title":"Test Config Access","text":"<pre><code>cd /path/to/buildcpg-labs\npython config/paths.py\n# Should show: \u2705 Config loaded successfully\n</code></pre>"},{"location":"getting-started/installation/#alternative-automated-setup-script","title":"Alternative: Automated Setup Script","text":"<p>If you want to automate steps 1-5, create this script:</p> <p>File: <code>setup.sh</code> <pre><code>#!/bin/bash\nset -e\n\necho \"Setting up BuildCPG Labs...\"\n\n# Install global dependencies\necho \"Installing global dependencies...\"\npip install --upgrade pip\npip install pyyaml duckdb pandas mkdocs mkdocs-material\n\n# Setup lab1\necho \"Setting up lab1_sales_performance...\"\ncd lab1_sales_performance\npython3 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n\n# Verify\necho \"Verifying setup...\"\ncd dbt\ndbt debug\n\necho \"\u2705 Setup complete!\"\necho \"\"\necho \"Next steps:\"\necho \"  cd ../.. (go to root)\"\necho \"  cd lab1_sales_performance\"\necho \"  python scripts/inspect_data.py\"\n</code></pre></p> <p>Run it: <pre><code>chmod +x setup.sh\n./setup.sh\n</code></pre></p>"},{"location":"getting-started/installation/#troubleshooting-installation","title":"Troubleshooting Installation","text":""},{"location":"getting-started/installation/#python-version-error","title":"Python version error","text":"<pre><code># If python3 not found\nwhich python3\npython3 --version\n\n# Try python (might be Python 2)\npython --version\n</code></pre>"},{"location":"getting-started/installation/#pip-command-not-found","title":"pip command not found","text":"<pre><code># Use python -m pip instead\npython -m pip install pyyaml\n</code></pre>"},{"location":"getting-started/installation/#venv-activation-fails-on-mac","title":"venv activation fails on Mac","text":"<pre><code># Try direct path\nsource ./venv/bin/activate\n\n# Or use Python's venv module directly\npython -m venv venv\nsource venv/bin/activate\n</code></pre>"},{"location":"getting-started/installation/#dbt-debug-fails","title":"dbt debug fails","text":"<pre><code># Make sure you're in the dbt directory\ncd lab1_sales_performance/dbt\ndbt debug\n\n# If still fails, check dbt installation\npip show dbt-core dbt-duckdb\n</code></pre>"},{"location":"getting-started/installation/#permission-denied-on-setupsh","title":"Permission denied on setup.sh","text":"<pre><code>chmod +x setup.sh\n./setup.sh\n</code></pre>"},{"location":"getting-started/installation/#post-installation","title":"Post-Installation","text":"<p>After successful installation:</p> <ol> <li>Read Quick Start</li> <li>Read Architecture Overview</li> <li>Try your</li> </ol>"},{"location":"getting-started/quick-start/","title":"Quick Start Guide","text":"<p>Get up and running with BuildCPG Labs in 10 minutes.</p>"},{"location":"getting-started/quick-start/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11+</li> <li>Mac 11+ or Linux</li> <li>Git</li> <li>Basic terminal knowledge</li> </ul>"},{"location":"getting-started/quick-start/#installation","title":"Installation","text":""},{"location":"getting-started/quick-start/#1-clone-repository","title":"1. Clone Repository","text":"<pre><code>git clone https://github.com/narensham/buildcpg-labs.git\ncd buildcpg-labs\n</code></pre>"},{"location":"getting-started/quick-start/#2-install-dependencies","title":"2. Install Dependencies","text":"<pre><code>pip install pyyaml duckdb pandas\n</code></pre>"},{"location":"getting-started/quick-start/#3-verify-setup","title":"3. Verify Setup","text":"<pre><code>python config/paths.py\n# Output: \u2705 Lab1 config loaded: ...\n</code></pre>"},{"location":"getting-started/quick-start/#your-first-lab-run","title":"Your First Lab Run","text":""},{"location":"getting-started/quick-start/#setup-lab1-one-time","title":"Setup Lab1 (One Time)","text":"<pre><code>cd lab1_sales_performance\nmake setup\n# Creates venv, installs dependencies, runs dbt debug\n</code></pre>"},{"location":"getting-started/quick-start/#run-the-pipeline","title":"Run the Pipeline","text":"<pre><code>make run\n# Executes: dbt run\n</code></pre>"},{"location":"getting-started/quick-start/#inspect-your-data","title":"Inspect Your Data","text":"<pre><code>make inspect\n# Shows: schemas, tables, row counts, quality scores\n</code></pre>"},{"location":"getting-started/quick-start/#run-tests","title":"Run Tests","text":"<pre><code>make test\n# Executes: dbt test\n</code></pre>"},{"location":"getting-started/quick-start/#create-a-new-lab","title":"Create a New Lab","text":""},{"location":"getting-started/quick-start/#bootstrap-lab2","title":"Bootstrap Lab2","text":"<pre><code>cd ..\n./setup_new_lab.sh lab2_forecast_model\n</code></pre>"},{"location":"getting-started/quick-start/#use-lab2-immediately","title":"Use Lab2 Immediately","text":"<pre><code>cd lab2_forecast_model\nmake run\n</code></pre>"},{"location":"getting-started/quick-start/#common-commands","title":"Common Commands","text":""},{"location":"getting-started/quick-start/#working-on-lab1","title":"Working on Lab1","text":"<pre><code>cd lab1_sales_performance\n\nmake setup      # Initialize lab (one time)\nmake run        # Run dbt pipeline\nmake test       # Run dbt tests\nmake inspect    # Check data quality\nmake clean      # Clean build artifacts\nmake docs       # Generate dbt documentation\n</code></pre>"},{"location":"getting-started/quick-start/#across-all-labs","title":"Across All Labs","text":"<pre><code># Check all labs\nls -la | grep lab\n\n# Status of each lab\ncat config/labs_config.yaml\n\n# Check paths\npython config/paths.py\n</code></pre>"},{"location":"getting-started/quick-start/#what-happened","title":"What Happened?","text":"<p>When you ran <code>make setup</code> in lab1:</p> <ol> <li>Created venv - Isolated Python environment</li> <li>Installed dependencies - From requirements.txt</li> <li>Ran dbt debug - Verified dbt setup</li> </ol> <p>When you ran <code>make run</code>:</p> <ol> <li>Activated venv - Automatically (no need for <code>source venv/bin/activate</code>)</li> <li>Changed to dbt directory - <code>cd dbt</code></li> <li>Ran dbt run - Executed transformation pipeline</li> <li>Created/updated tables - In DuckDB database</li> </ol> <p>When you ran <code>make inspect</code>:</p> <ol> <li>Connected to database - lab1_sales_performance.duckdb</li> <li>Listed all schemas - raw, raw_bronze, raw_gold, raw_silver</li> <li>Showed table stats - Row counts, column counts</li> <li>Calculated quality - Checked for nulls, duplicates</li> </ol>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<ul> <li>Read Lab Structure to understand directory layout</li> <li>Read Shared vs Lab-Specific to understand how code is organized</li> <li>Read Creating New Lab for detailed lab creation guide</li> <li>Read Troubleshooting if you hit any issues</li> </ul>"},{"location":"getting-started/quick-start/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/quick-start/#command-not-found-make","title":"\"command not found: make\"","text":"<p>You need to install <code>make</code>. On Mac: <pre><code>xcode-select --install\n</code></pre></p>"},{"location":"getting-started/quick-start/#venv-not-found","title":"\"venv not found\"","text":"<p>Run <code>make setup</code> first to create the virtual environment.</p>"},{"location":"getting-started/quick-start/#dbt-command-not-found","title":"\"dbt: command not found\"","text":"<p>Make sure you ran <code>make setup</code> to install dbt in the venv.</p> <p>For more help, see Troubleshooting.</p>"},{"location":"labs/lab2-api-integration/","title":"Lab 2: API Integration Guide","text":"<p>Detailed guide for integrating with Reddit and News APIs for sentiment analysis.</p>"},{"location":"labs/lab2-api-integration/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Reddit API (PRAW)</li> <li>News API</li> <li>Brand Detection</li> <li>Sentiment Analysis</li> <li>Data Schema</li> <li>Best Practices</li> </ol>"},{"location":"labs/lab2-api-integration/#reddit-api-praw","title":"Reddit API (PRAW)","text":""},{"location":"labs/lab2-api-integration/#overview","title":"Overview","text":"<p>We use PRAW (Python Reddit API Wrapper) to access Reddit's API.</p> <p>Why Reddit? - \u2705 User-generated discussions about brands - \u2705 Authentic sentiment (not corporate messaging) - \u2705 High engagement signals (upvotes, comments) - \u2705 Free API access - \u2705 Real-time data</p>"},{"location":"labs/lab2-api-integration/#authentication","title":"Authentication","text":"<pre><code>import praw\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nreddit = praw.Reddit(\n    client_id=os.getenv('REDDIT_CLIENT_ID'),\n    client_secret=os.getenv('REDDIT_CLIENT_SECRET'),\n    user_agent=os.getenv('REDDIT_USER_AGENT')\n)\n</code></pre>"},{"location":"labs/lab2-api-integration/#rate-limits","title":"Rate Limits","text":"<ul> <li>60 requests per minute</li> <li>PRAW handles rate limiting automatically</li> <li>Implements exponential backoff on errors</li> </ul>"},{"location":"labs/lab2-api-integration/#search-strategy","title":"Search Strategy","text":""},{"location":"labs/lab2-api-integration/#basic-search","title":"Basic Search","text":"<pre><code># Search across all subreddits\nsubreddit = reddit.subreddit('all')\n\n# Search for brand mentions\nfor post in subreddit.search(\"Coca-Cola\", limit=20, time_filter='month'):\n    print(post.title)\n</code></pre>"},{"location":"labs/lab2-api-integration/#advanced-search-with-filters","title":"Advanced Search with Filters","text":"<pre><code># Multiple search terms\nquery = 'Coca-Cola OR Coke OR \"Coca Cola\"'\n\n# Time filters: 'hour', 'day', 'week', 'month', 'year', 'all'\nposts = subreddit.search(query, limit=50, time_filter='month', sort='relevance')\n\n# Iterate through results\nfor post in posts:\n    # Access post attributes\n    post_id = post.id\n    title = post.title\n    body = post.selftext\n    upvotes = post.score\n    comments = post.num_comments\n    created = post.created_utc\n    subreddit_name = post.subreddit.display_name\n    author = str(post.author) if post.author else 'deleted'\n    permalink = f\"https://reddit.com{post.permalink}\"\n</code></pre>"},{"location":"labs/lab2-api-integration/#subreddit-specific-search","title":"Subreddit-Specific Search","text":"<pre><code># Target specific subreddits\ntarget_subreddits = ['food', 'snacks', 'cooking', 'AskReddit', 'unpopularopinion']\n\nall_posts = []\nfor sub_name in target_subreddits:\n    subreddit = reddit.subreddit(sub_name)\n    posts = subreddit.search(\"Coca-Cola\", limit=10, time_filter='month')\n    all_posts.extend(posts)\n</code></pre>"},{"location":"labs/lab2-api-integration/#handling-deleted-content","title":"Handling Deleted Content","text":"<pre><code>def safe_author(post):\n    \"\"\"Handle deleted authors gracefully.\"\"\"\n    try:\n        return str(post.author) if post.author else 'deleted'\n    except AttributeError:\n        return 'deleted'\n\ndef safe_text(post):\n    \"\"\"Handle deleted text gracefully.\"\"\"\n    return post.selftext if post.selftext and post.selftext != '[removed]' else post.title\n</code></pre>"},{"location":"labs/lab2-api-integration/#error-handling","title":"Error Handling","text":"<pre><code>from prawcore.exceptions import (\n    ResponseException,\n    RequestException,\n    TooManyRequests,\n    Forbidden\n)\nimport time\n\ndef fetch_posts_with_retry(query, limit=20, max_retries=3):\n    \"\"\"Fetch posts with automatic retry on errors.\"\"\"\n    for attempt in range(max_retries):\n        try:\n            posts = reddit.subreddit('all').search(query, limit=limit)\n            return list(posts)\n        except TooManyRequests:\n            wait_time = 60 * (attempt + 1)  # Exponential backoff\n            logger.warning(f\"Rate limited. Waiting {wait_time}s...\")\n            time.sleep(wait_time)\n        except (ResponseException, RequestException) as e:\n            logger.error(f\"Reddit API error: {e}\")\n            if attempt == max_retries - 1:\n                raise\n            time.sleep(5)\n    return []\n</code></pre>"},{"location":"labs/lab2-api-integration/#full-example","title":"Full Example","text":"<pre><code>def ingest_reddit_data(brands_list: list, limit_per_brand=20):\n    \"\"\"\n    Fetch Reddit posts mentioning CPG brands.\n\n    Args:\n        brands_list: List of brand dictionaries with name, aliases, etc.\n        limit_per_brand: Max posts to fetch per brand\n\n    Returns:\n        pandas.DataFrame with Reddit posts\n    \"\"\"\n    reddit = praw.Reddit(\n        client_id=os.getenv('REDDIT_CLIENT_ID'),\n        client_secret=os.getenv('REDDIT_CLIENT_SECRET'),\n        user_agent=os.getenv('REDDIT_USER_AGENT')\n    )\n\n    all_posts = []\n\n    for brand_info in brands_list:\n        brand_name = brand_info['brand_name']\n        logger.info(f\"Searching for: {brand_name}\")\n\n        try:\n            subreddit = reddit.subreddit('all')\n\n            for post in subreddit.search(brand_name, limit=limit_per_brand, time_filter='month'):\n                # Combine title and body for sentiment\n                full_text = f\"{post.title} {post.selftext}\"\n\n                # Detect all brands mentioned\n                mentioned_brands = detect_brand_mentions(full_text, brands_list)\n\n                # Create record for each mentioned brand\n                for mentioned_brand in mentioned_brands:\n                    post_data = {\n                        'post_id': post.id,\n                        'author': str(post.author) if post.author else 'deleted',\n                        'brand': mentioned_brand['brand_name'],\n                        'parent_company': mentioned_brand['parent_company'],\n                        'brand_category': mentioned_brand['category'],\n                        'title': post.title,\n                        'body': post.selftext if post.selftext else post.title,\n                        'upvotes': post.score,\n                        'comments_count': post.num_comments,\n                        'created_at': datetime.fromtimestamp(post.created_utc).isoformat(),\n                        'sentiment_score': analyze_sentiment(full_text),\n                        'source': 'reddit',\n                        'subreddit': post.subreddit.display_name,\n                        'ingested_at': datetime.utcnow().isoformat(),\n                        'url': f\"https://reddit.com{post.permalink}\"\n                    }\n                    all_posts.append(post_data)\n\n        except Exception as e:\n            logger.warning(f\"Error fetching {brand_name}: {e}\")\n            continue\n\n    df = pd.DataFrame(all_posts)\n    df = df.drop_duplicates(subset=['post_id', 'brand'], keep='first')\n\n    return df\n</code></pre>"},{"location":"labs/lab2-api-integration/#news-api","title":"News API","text":""},{"location":"labs/lab2-api-integration/#overview_1","title":"Overview","text":"<p>We use NewsAPI to access news articles from 80,000+ sources.</p> <p>Why News API? - \u2705 Professional journalism - \u2705 80,000+ sources worldwide - \u2705 Historical data (30 days on free tier) - \u2705 Simple REST API - \u2705 Good search capabilities</p>"},{"location":"labs/lab2-api-integration/#authentication_1","title":"Authentication","text":"<pre><code>from newsapi import NewsApiClient\n\nnewsapi = NewsApiClient(api_key=os.getenv('NEWS_API_KEY'))\n</code></pre>"},{"location":"labs/lab2-api-integration/#rate-limits-tiers","title":"Rate Limits &amp; Tiers","text":"<p>Free Tier: - 100 requests per day - 100 results per request (max) - Historical: Last 30 days only - Delayed: 15-minute delay on breaking news</p> <p>Paid Tiers: - 250 to 100,000 requests per day - Historical: Full archive access - Real-time: No delay</p>"},{"location":"labs/lab2-api-integration/#search-endpoints","title":"Search Endpoints","text":""},{"location":"labs/lab2-api-integration/#everything-endpoint","title":"Everything Endpoint","text":"<pre><code>from datetime import datetime, timedelta\n\n# Define date range\nfrom_date = (datetime.utcnow() - timedelta(days=7)).strftime('%Y-%m-%d')\nto_date = datetime.utcnow().strftime('%Y-%m-%d')\n\n# Search for articles\nresponse = newsapi.get_everything(\n    q='Coca-Cola',                    # Search query\n    from_param=from_date,             # Start date\n    to=to_date,                       # End date\n    language='en',                    # Language filter\n    sort_by='relevancy',              # Sort by: relevancy, popularity, publishedAt\n    page_size=100,                    # Max results (max 100)\n    page=1                            # Pagination\n)\n\n# Response structure\n{\n    'status': 'ok',\n    'totalResults': 453,\n    'articles': [\n        {\n            'source': {'id': None, 'name': 'Forbes'},\n            'author': 'John Doe',\n            'title': 'Coca-Cola Announces New Product',\n            'description': 'Brief description...',\n            'url': 'https://...',\n            'urlToImage': 'https://...',\n            'publishedAt': '2025-11-01T10:30:00Z',\n            'content': 'Full content (truncated)...'\n        },\n        # ... more articles\n    ]\n}\n</code></pre>"},{"location":"labs/lab2-api-integration/#top-headlines-endpoint","title":"Top Headlines Endpoint","text":"<pre><code># Get top headlines (breaking news)\nresponse = newsapi.get_top_headlines(\n    q='Coca-Cola',\n    category='business',              # business, entertainment, general, health, science, sports, technology\n    language='en',\n    country='us',                     # ISO country code\n    page_size=100\n)\n</code></pre>"},{"location":"labs/lab2-api-integration/#query-syntax","title":"Query Syntax","text":"<p>NewsAPI supports advanced query syntax:</p> <pre><code># OR operator\nq = 'Coca-Cola OR Coke OR \"Coca Cola\"'\n\n# AND operator\nq = 'Coca-Cola AND (earnings OR revenue)'\n\n# NOT operator\nq = 'Coca-Cola NOT Trump'\n\n# Phrase matching\nq = '\"Coca-Cola Company\"'\n\n# Combination\nq = '(Coca-Cola OR Pepsi) AND (earnings OR revenue) NOT cryptocurrency'\n</code></pre>"},{"location":"labs/lab2-api-integration/#pagination","title":"Pagination","text":"<pre><code>def fetch_all_articles(query, from_date, max_articles=500):\n    \"\"\"Fetch all articles with pagination.\"\"\"\n    all_articles = []\n    page = 1\n    page_size = 100\n\n    while len(all_articles) &lt; max_articles:\n        response = newsapi.get_everything(\n            q=query,\n            from_param=from_date,\n            language='en',\n            sort_by='relevancy',\n            page_size=page_size,\n            page=page\n        )\n\n        articles = response.get('articles', [])\n        if not articles:\n            break  # No more articles\n\n        all_articles.extend(articles)\n        page += 1\n\n    return all_articles[:max_articles]\n</code></pre>"},{"location":"labs/lab2-api-integration/#error-handling_1","title":"Error Handling","text":"<pre><code>from newsapi.newsapi_exception import NewsAPIException\n\ndef fetch_news_with_retry(query, from_date, max_retries=3):\n    \"\"\"Fetch news with retry logic.\"\"\"\n    for attempt in range(max_retries):\n        try:\n            response = newsapi.get_everything(\n                q=query,\n                from_param=from_date,\n                language='en',\n                sort_by='relevancy',\n                page_size=100\n            )\n            return response.get('articles', [])\n\n        except NewsAPIException as e:\n            if e.get_code() == 426:  # Rate limit exceeded\n                logger.error(\"Rate limit exceeded. Upgrade to paid plan or reduce frequency.\")\n                return []\n            elif e.get_code() == 429:  # Too many requests\n                wait_time = 60 * (attempt + 1)\n                logger.warning(f\"Too many requests. Waiting {wait_time}s...\")\n                time.sleep(wait_time)\n            else:\n                logger.error(f\"NewsAPI error: {e}\")\n                if attempt == max_retries - 1:\n                    raise\n        except Exception as e:\n            logger.error(f\"Unexpected error: {e}\")\n            if attempt == max_retries - 1:\n                raise\n\n    return []\n</code></pre>"},{"location":"labs/lab2-api-integration/#full-example_1","title":"Full Example","text":"<pre><code>def ingest_news_data(brands_list: list, days_back=7, articles_per_brand=20):\n    \"\"\"\n    Fetch news articles mentioning CPG brands.\n\n    Args:\n        brands_list: List of brand dictionaries\n        days_back: How many days of history to fetch\n        articles_per_brand: Max articles per brand\n\n    Returns:\n        pandas.DataFrame with news articles\n    \"\"\"\n    newsapi = NewsApiClient(api_key=os.getenv('NEWS_API_KEY'))\n\n    all_articles = []\n    from_date = (datetime.utcnow() - timedelta(days=days_back)).strftime('%Y-%m-%d')\n\n    for brand_info in brands_list:\n        brand_name = brand_info['brand_name']\n        logger.info(f\"Searching for: {brand_name}\")\n\n        try:\n            response = newsapi.get_everything(\n                q=brand_name,\n                from_param=from_date,\n                language='en',\n                sort_by='relevancy',\n                page_size=articles_per_brand\n            )\n\n            for article in response.get('articles', []):\n                # Combine title and description for sentiment\n                full_text = f\"{article.get('title', '')} {article.get('description', '')}\"\n\n                # Detect all brands mentioned\n                mentioned_brands = detect_brand_mentions(full_text, brands_list)\n\n                for mentioned_brand in mentioned_brands:\n                    article_data = {\n                        'article_id': article.get('url', '').split('/')[-1][:50],\n                        'publication': article.get('source', {}).get('name', 'Unknown'),\n                        'brand': mentioned_brand['brand_name'],\n                        'parent_company': mentioned_brand['parent_company'],\n                        'brand_category': mentioned_brand['category'],\n                        'headline': article.get('title', ''),\n                        'body': article.get('description', '') or article.get('content', ''),\n                        'url': article.get('url', ''),\n                        'published_at': article.get('publishedAt', datetime.utcnow().isoformat()),\n                        'sentiment_score': analyze_sentiment(full_text),\n                        'source': 'news',\n                        'ingested_at': datetime.utcnow().isoformat()\n                    }\n                    all_articles.append(article_data)\n\n        except Exception as e:\n            logger.warning(f\"Error fetching {brand_name}: {e}\")\n            continue\n\n    df = pd.DataFrame(all_articles)\n    df = df.drop_duplicates(subset=['url', 'brand'], keep='first')\n\n    return df\n</code></pre>"},{"location":"labs/lab2-api-integration/#brand-detection","title":"Brand Detection","text":""},{"location":"labs/lab2-api-integration/#strategy","title":"Strategy","text":"<p>We detect brand mentions using: 1. Exact brand name matching 2. Brand aliases (e.g., \"Coke\" for \"Coca-Cola\") 3. Case-insensitive comparison</p>"},{"location":"labs/lab2-api-integration/#implementation","title":"Implementation","text":"<pre><code>def detect_brand_mentions(text: str, brands_list: list) -&gt; list:\n    \"\"\"\n    Detect which brands are mentioned in text.\n\n    Args:\n        text: Content to search\n        brands_list: List of brand dictionaries with names and aliases\n\n    Returns:\n        List of brand_info dicts for mentioned brands\n    \"\"\"\n    text_lower = text.lower()\n    mentioned_brands = []\n\n    for brand_info in brands_list:\n        brand_name = brand_info['brand_name']\n        aliases = brand_info.get('brand_aliases', [])\n\n        # Check main brand name\n        if brand_name.lower() in text_lower:\n            mentioned_brands.append(brand_info)\n            continue\n\n        # Check aliases\n        for alias in aliases:\n            if alias.lower() in text_lower:\n                mentioned_brands.append(brand_info)\n                break\n\n    return mentioned_brands\n</code></pre>"},{"location":"labs/lab2-api-integration/#advanced-nlp-based-detection","title":"Advanced: NLP-Based Detection","text":"<p>For production, consider using NLP for better accuracy:</p> <pre><code>import spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef detect_brands_nlp(text: str, brands_list: list) -&gt; list:\n    \"\"\"Use NLP for more accurate brand detection.\"\"\"\n    doc = nlp(text)\n\n    # Extract organization entities\n    org_entities = [ent.text.lower() for ent in doc.ents if ent.label_ == \"ORG\"]\n\n    mentioned_brands = []\n    for brand_info in brands_list:\n        brand_name = brand_info['brand_name'].lower()\n\n        # Check if brand name or alias is in entities\n        if any(brand_name in entity or entity in brand_name for entity in org_entities):\n            mentioned_brands.append(brand_info)\n\n    return mentioned_brands\n</code></pre>"},{"location":"labs/lab2-api-integration/#sentiment-analysis","title":"Sentiment Analysis","text":""},{"location":"labs/lab2-api-integration/#vader-sentiment","title":"VADER Sentiment","text":"<p>We use VADER (Valence Aware Dictionary and sEntiment Reasoner), optimized for social media text.</p>"},{"location":"labs/lab2-api-integration/#why-vader","title":"Why VADER?","text":"<ul> <li>\u2705 Designed for social media text</li> <li>\u2705 Handles emojis, slang, capitalization</li> <li>\u2705 Fast (no ML model loading)</li> <li>\u2705 Compound score from -1 to 1</li> <li>\u2705 Pre-trained, no tuning needed</li> </ul>"},{"location":"labs/lab2-api-integration/#implementation_1","title":"Implementation","text":"<pre><code>from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\nsentiment_analyzer = SentimentIntensityAnalyzer()\n\ndef analyze_sentiment(text: str) -&gt; float:\n    \"\"\"\n    Analyze sentiment using VADER.\n\n    Args:\n        text: Content to analyze\n\n    Returns:\n        Compound score from -1 (very negative) to 1 (very positive)\n    \"\"\"\n    if not text:\n        return 0.0\n\n    scores = sentiment_analyzer.polarity_scores(text)\n\n    # VADER returns: {'neg': 0.1, 'neu': 0.6, 'pos': 0.3, 'compound': 0.5}\n    return scores['compound']\n</code></pre>"},{"location":"labs/lab2-api-integration/#example-scores","title":"Example Scores","text":"<pre><code>texts = [\n    \"I absolutely love Coca-Cola! Best drink ever! \ud83d\ude0d\",\n    \"Coca-Cola is okay, nothing special.\",\n    \"I hate the new Coke formula. It's disgusting! \ud83e\udd2e\"\n]\n\nfor text in texts:\n    score = analyze_sentiment(text)\n    print(f\"Text: {text}\")\n    print(f\"Score: {score:.3f}\\n\")\n\n# Output:\n# Text: I absolutely love Coca-Cola! Best drink ever! \ud83d\ude0d\n# Score: 0.861\n\n# Text: Coca-Cola is okay, nothing special.\n# Score: 0.296\n\n# Text: I hate the new Coke formula. It's disgusting! \ud83e\udd2e\n# Score: -0.836\n</code></pre>"},{"location":"labs/lab2-api-integration/#advanced-transformer-based-sentiment","title":"Advanced: Transformer-Based Sentiment","text":"<p>For higher accuracy, use Hugging Face transformers:</p> <pre><code>from transformers import pipeline\n\n# Load pre-trained sentiment model\nclassifier = pipeline(\"sentiment-analysis\", \n                     model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n\ndef analyze_sentiment_transformer(text: str) -&gt; float:\n    \"\"\"Use transformer for sentiment analysis.\"\"\"\n    if not text:\n        return 0.0\n\n    # Truncate to 512 tokens (BERT limit)\n    result = classifier(text[:512])[0]\n\n    # Convert to -1 to 1 scale\n    label = result['label']  # 'POSITIVE' or 'NEGATIVE'\n    score = result['score']  # 0.0 to 1.0 confidence\n\n    return score if label == 'POSITIVE' else -score\n</code></pre>"},{"location":"labs/lab2-api-integration/#data-schema","title":"Data Schema","text":""},{"location":"labs/lab2-api-integration/#reddit-data-schema","title":"Reddit Data Schema","text":"<pre><code>{\n    'post_id': str,              # Unique Reddit post ID\n    'author': str,               # Username (or 'deleted')\n    'brand': str,                # Brand name\n    'parent_company': str,       # Parent company from taxonomy\n    'brand_category': str,       # Product category\n    'title': str,                # Post title\n    'body': str,                 # Post body text\n    'upvotes': int,              # Score (upvotes - downvotes)\n    'comments_count': int,       # Number of comments\n    'created_at': str,           # ISO timestamp\n    'sentiment_score': float,    # -1 to 1\n    'source': 'reddit',\n    'subreddit': str,            # Subreddit name\n    'ingested_at': str,          # ISO timestamp\n    'url': str                   # Permalink\n}\n</code></pre>"},{"location":"labs/lab2-api-integration/#news-data-schema","title":"News Data Schema","text":"<pre><code>{\n    'article_id': str,           # URL slug or hash\n    'publication': str,          # Source name (e.g., \"Forbes\")\n    'brand': str,                # Brand name\n    'parent_company': str,       # Parent company from taxonomy\n    'brand_category': str,       # Product category\n    'headline': str,             # Article title\n    'body': str,                 # Article description/content\n    'url': str,                  # Full article URL\n    'published_at': str,         # ISO timestamp\n    'sentiment_score': float,    # -1 to 1\n    'source': 'news',\n    'ingested_at': str          # ISO timestamp\n}\n</code></pre>"},{"location":"labs/lab2-api-integration/#best-practices","title":"Best Practices","text":""},{"location":"labs/lab2-api-integration/#1-respect-rate-limits","title":"1. Respect Rate Limits","text":"<pre><code># Track API calls\napi_calls = {\n    'reddit': 0,\n    'news': 0,\n    'last_reset': datetime.utcnow()\n}\n\ndef check_rate_limit(api_name):\n    \"\"\"Check if we're within rate limits.\"\"\"\n    now = datetime.utcnow()\n\n    # Reset counters every hour\n    if (now - api_calls['last_reset']).seconds &gt; 3600:\n        api_calls['reddit'] = 0\n        api_calls['news'] = 0\n        api_calls['last_reset'] = now\n\n    # Check limits\n    if api_name == 'reddit' and api_calls['reddit'] &gt;= 3600:  # 60 per min * 60 min\n        raise Exception(\"Reddit rate limit reached\")\n    elif api_name == 'news' and api_calls['news'] &gt;= 100:  # Daily limit\n        raise Exception(\"News API rate limit reached\")\n\n    api_calls[api_name] += 1\n</code></pre>"},{"location":"labs/lab2-api-integration/#2-implement-exponential-backoff","title":"2. Implement Exponential Backoff","text":"<pre><code>import time\nimport random\n\ndef exponential_backoff(attempt, base_wait=1, max_wait=60):\n    \"\"\"Calculate wait time with exponential backoff.\"\"\"\n    wait_time = min(base_wait * (2 ** attempt) + random.uniform(0, 1), max_wait)\n    return wait_time\n\n# Usage\nfor attempt in range(5):\n    try:\n        # API call\n        break\n    except Exception as e:\n        if attempt == 4:\n            raise\n        wait = exponential_backoff(attempt)\n        logger.info(f\"Retry in {wait:.1f}s...\")\n        time.sleep(wait)\n</code></pre>"},{"location":"labs/lab2-api-integration/#3-cache-api-responses","title":"3. Cache API Responses","text":"<pre><code>import hashlib\nimport json\nfrom pathlib import Path\n\nCACHE_DIR = Path(\"data/cache\")\nCACHE_DIR.mkdir(exist_ok=True)\n\ndef cache_key(func_name, **kwargs):\n    \"\"\"Generate cache key from function name and arguments.\"\"\"\n    key_str = f\"{func_name}:{json.dumps(kwargs, sort_keys=True)}\"\n    return hashlib.md5(key_str.encode()).hexdigest()\n\ndef fetch_with_cache(func, cache_ttl=3600, **kwargs):\n    \"\"\"Fetch data with caching.\"\"\"\n    key = cache_key(func.__name__, **kwargs)\n    cache_file = CACHE_DIR / f\"{key}.json\"\n\n    # Check cache\n    if cache_file.exists():\n        age = time.time() - cache_file.stat().st_mtime\n        if age &lt; cache_ttl:\n            logger.info(f\"Using cached result (age: {age:.0f}s)\")\n            with open(cache_file) as f:\n                return json.load(f)\n\n    # Fetch fresh data\n    result = func(**kwargs)\n\n    # Save to cache\n    with open(cache_file, 'w') as f:\n        json.dump(result, f)\n\n    return result\n</code></pre>"},{"location":"labs/lab2-api-integration/#4-log-all-api-interactions","title":"4. Log All API Interactions","text":"<pre><code>import logging\n\n# Setup logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler('ingestion.log'),\n        logging.StreamHandler()\n    ]\n)\nlogger = logging.getLogger(__name__)\n\n# Log API calls\ndef fetch_reddit_posts(brand):\n    logger.info(f\"Fetching Reddit posts for: {brand}\")\n    try:\n        posts = # ... API call ...\n        logger.info(f\"\u2705 Found {len(posts)} posts for {brand}\")\n        return posts\n    except Exception as e:\n        logger.error(f\"\u274c Error fetching {brand}: {e}\")\n        raise\n</code></pre>"},{"location":"labs/lab2-api-integration/#5-validate-data-before-saving","title":"5. Validate Data Before Saving","text":"<pre><code>def validate_post_data(post_data):\n    \"\"\"Validate required fields are present.\"\"\"\n    required_fields = ['post_id', 'brand', 'sentiment_score', 'created_at']\n\n    for field in required_fields:\n        if field not in post_data:\n            raise ValueError(f\"Missing required field: {field}\")\n\n    # Validate sentiment score\n    score = post_data['sentiment_score']\n    if not -1 &lt;= score &lt;= 1:\n        raise ValueError(f\"Invalid sentiment score: {score}\")\n\n    return True\n</code></pre>"},{"location":"labs/lab2-api-integration/#testing-api-integration","title":"Testing API Integration","text":""},{"location":"labs/lab2-api-integration/#unit-tests","title":"Unit Tests","text":"<pre><code>import unittest\nfrom unittest.mock import Mock, patch\n\nclass TestAPIIntegration(unittest.TestCase):\n\n    @patch('praw.Reddit')\n    def test_reddit_fetch(self, mock_reddit):\n        \"\"\"Test Reddit data fetching.\"\"\"\n        # Mock Reddit API\n        mock_post = Mock()\n        mock_post.id = 'test123'\n        mock_post.title = 'Test post about Coca-Cola'\n        mock_post.selftext = 'Great product!'\n        mock_post.score = 100\n\n        mock_reddit.return_value.subreddit.return_value.search.return_value = [mock_post]\n\n        # Test function\n        result = ingest_reddit_data(['Coca-Cola'])\n\n        self.assertEqual(len(result), 1)\n        self.assertEqual(result[0]['post_id'], 'test123')\n\n    def test_brand_detection(self):\n        \"\"\"Test brand mention detection.\"\"\"\n        text = \"I love Coke and Sprite!\"\n        brands = [\n            {'brand_name': 'Coca-Cola', 'brand_aliases': ['Coke']},\n            {'brand_name': 'Sprite', 'brand_aliases': []}\n        ]\n\n        mentioned = detect_brand_mentions(text, brands)\n\n        self.assertEqual(len(mentioned), 2)\n        self.assertEqual(mentioned[0]['brand_name'], 'Coca-Cola')\n        self.assertEqual(mentioned[1]['brand_name'], 'Sprite')\n\n    def test_sentiment_analysis(self):\n        \"\"\"Test sentiment scoring.\"\"\"\n        positive = \"I absolutely love this product!\"\n        negative = \"Terrible quality, waste of money\"\n        neutral = \"It's okay, nothing special\"\n\n        self.assertGreater(analyze_sentiment(positive), 0.5)\n        self.assertLess(analyze_sentiment(negative), -0.5)\n        self.assertAlmostEqual(analyze_sentiment(neutral), 0.0, delta=0.3)\n\nif __name__ == '__main__':\n    unittest.main()\n</code></pre>"},{"location":"labs/lab2-api-integration/#monitoring-alerting","title":"Monitoring &amp; Alerting","text":""},{"location":"labs/lab2-api-integration/#track-api-health","title":"Track API Health","text":"<pre><code>import json\nfrom datetime import datetime\n\ndef log_api_health():\n    \"\"\"Track API health metrics.\"\"\"\n    health = {\n        'timestamp': datetime.utcnow().isoformat(),\n        'reddit': {\n            'status': 'ok',\n            'calls_today': api_calls['reddit'],\n            'errors': 0\n        },\n        'news': {\n            'status': 'ok',\n            'calls_today': api_calls['news'],\n            'errors': 0\n        }\n    }\n\n    with open('data/api_health.json', 'w') as f:\n        json.dump(health, f)\n</code></pre> <p>Last Updated: November 2025 Maintainer: narensham</p>"},{"location":"labs/lab2-architecture/","title":"Lab 2: Market Sentiment Architecture","text":""},{"location":"labs/lab2-architecture/#overview","title":"Overview","text":"<p>The Market Sentiment Analysis Architecture enables near-real-time ingestion, transformation, and aggregation of public sentiment toward Consumer Packaged Goods (CPG) brands.</p> <p>It\u2019s designed around the Medallion Architecture \u2014 Raw \u2192 Bronze \u2192 Silver \u2192 Gold \u2014 and optimized for local-first analytics using DuckDB and dbt.</p>"},{"location":"labs/lab2-architecture/#high-level-data-flow","title":"High-Level Data Flow","text":"<p>```mermaid     flowchart LR     %% Layer titles     subgraph INGESTION[\"\ud83d\udd39 Ingestion Layer\"]         A1[Reddit API]:::source \u2192 B[Python Ingestion(pipelines/ingest_sentiment.py)]:::process         A2[News API]:::source \u2192 B     end</p> <pre><code>subgraph RAW[\"\ud83d\uddc2\ufe0f Raw Data\"]\n    B --&gt; C[Raw CSV Files&lt;br&gt;(data/raw/)]:::storage\nend\n\nsubgraph BRONZE[\"\ud83e\udd49 Bronze Layer\"]\n    C --&gt; D[dbt Staging Models&lt;br&gt;(stg_reddit__*, stg_news__*)]:::bronze\nend\n\nsubgraph SILVER[\"\ud83e\udd48 Silver Layer\"]\n    D --&gt; E[dbt Intermediate Models&lt;br&gt;(int_sentiment_unified)]:::silver\nend\n\nsubgraph GOLD[\"\ud83e\udd47 Gold Layer\"]\n    E --&gt; F[dbt Marts Models&lt;br&gt;(mart_daily_sentiment, fct_sentiment_events)]:::gold\nend\n\nsubgraph ANALYTICS[\"\ud83d\udcca Analytics Layer\"]\n    F --&gt; G[Streamlit / DuckDB Dashboards]:::viz\nend\n\n%% Styling for classes\nclassDef source fill:#e3f2fd,stroke:#1565c0,stroke-width:2px,color:#0d47a1,font-weight:bold;\nclassDef process fill:#fff3e0,stroke:#ef6c00,stroke-width:2px,color:#4e342e,font-weight:bold;\nclassDef storage fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px,color:#1b5e20,font-weight:bold;\nclassDef bronze fill:#fbe9e7,stroke:#bf360c,stroke-width:2px,color:#3e2723,font-weight:bold;\nclassDef silver fill:#eceff1,stroke:#607d8b,stroke-width:2px,color:#263238,font-weight:bold;\nclassDef gold fill:#fff8e1,stroke:#f9a825,stroke-width:2px,color:#5f370e,font-weight:bold;\nclassDef viz fill:#fce4ec,stroke:#ad1457,stroke-width:2px,color:#4a148c,font-weight:bold;\n</code></pre>"},{"location":"labs/lab2-architecture/#architecture-summary","title":"Architecture Summary","text":"Layer Description Example Models Raw External Reddit &amp; News data sources <code>reddit_real.csv</code>, <code>news_real.csv</code> Bronze (Staging) Cleans &amp; standardizes raw data <code>stg_reddit__posts</code>, <code>stg_news__articles</code> Silver (Intermediate) Combines and enriches sources <code>int_sentiment_unified</code> Gold (Marts) Business-ready analytics tables <code>fct_sentiment_events</code>, <code>mart_daily_sentiment</code>"},{"location":"labs/lab2-architecture/#technology-stack","title":"Technology Stack","text":"Component Tool Purpose Database DuckDB Lightweight analytical engine Transformation dbt ELT orchestration and testing Ingestion Python 3.11+ Reddit + News API collection Sentiment Analysis VADER / TextBlob Text polarity scoring Orchestration Manual (Airflow planned) Future automation Visualization Streamlit Interactive dashboard layer"},{"location":"labs/lab2-architecture/#key-features","title":"Key Features","text":"<ul> <li>Unified Sentiment Model: Combines Reddit and News sentiment in one schema.</li> <li>Surrogate Keys: Ensures event-level uniqueness.</li> <li>Categorized Sentiment: Groups scores into positive, neutral, or negative.</li> <li>Incremental Loads: Processes only new data after each run.</li> <li>Data Contracts: Enforces consistent schema and data types.</li> <li>Comprehensive Testing: Uniqueness, nulls, ranges, and accepted values validated.</li> </ul>"},{"location":"labs/lab2-architecture/#data-quality-tests","title":"Data Quality Tests","text":"Type Target Columns Description Unique <code>sentiment_event_id</code>, <code>(sentiment_date, brand)</code> Ensures grain-level uniqueness Not Null <code>brand_key</code>, <code>sentiment_score</code>, <code>published_date</code> Core attributes must exist Range <code>sentiment_score</code> Must be between -1 and 1 Accepted Values <code>quality_flag</code>, <code>anomaly_flag</code> Restricts allowed values <p>All core tests are passing as of the latest dbt run \u2705</p>"},{"location":"labs/lab2-architecture/#example-model-flow","title":"Example Model Flow","text":"<ol> <li> <p>Ingestion: <code>ingest_sentiment.py</code> pulls new posts/articles and stores them under <code>/data/raw/</code></p> </li> <li> <p>Transformation:    dbt staging \u2192 intermediate \u2192 marts flow using DuckDB</p> </li> <li> <p>Analytics:    Aggregate daily sentiment and engagement data for dashboards</p> </li> </ol>"},{"location":"labs/lab2-architecture/#current-status","title":"Current Status","text":"Area Progress Notes Data Ingestion \u2705 Completed Synthetic Reddit + News data dbt Models \u2705 Stable Full medallion flow implemented Data Quality \u2705 All tests passing Custom tests working Dashboard \ud83d\udd04 Planned Streamlit interface next Airflow Integration \ud83d\udd04 Planned Orchestration phase"},{"location":"labs/lab2-architecture/#related-documentation","title":"Related Documentation","text":"<ul> <li>Lab 2 Setup Guide</li> <li>Lab 2 Data Models</li> <li>Lab 2 Troubleshooting</li> <li>Architecture Overview</li> </ul> <p>Author: narensham Last Updated: November 2025 Status: Active Development</p>"},{"location":"labs/lab2-data-models/","title":"Lab 2: Data Models Reference","text":"<p>Complete reference for all data models in the Market Sentiment Analysis lab with schemas, lineage, and examples.</p>"},{"location":"labs/lab2-data-models/#model-hierarchy","title":"Model Hierarchy","text":"<pre><code>graph TB\n    subgraph \"Data Sources\"\n        A[reddit_real.csv]\n        B[news_real.csv]\n    end\n\n    subgraph \"Staging Layer\"\n        C[stg_reddit__posts]\n        D[stg_news__articles]\n    end\n\n    subgraph \"Intermediate Layer\"\n        E[int_sentiment_unified]\n    end\n\n    subgraph \"Mart Layer\"\n        F[fct_sentiment_events]\n        G[mart_daily_sentiment]\n    end\n\n    A --&gt; C\n    B --&gt; D\n    C --&gt; E\n    D --&gt; E\n    E --&gt; F\n    F --&gt; G\n\n    style A fill:#fee2e2,stroke:#dc2626\n    style B fill:#fee2e2,stroke:#dc2626\n    style C fill:#dbeafe,stroke:#2563eb\n    style D fill:#dbeafe,stroke:#2563eb\n    style E fill:#fef3c7,stroke:#f59e0b\n    style F fill:#dcfce7,stroke:#16a34a\n    style G fill:#dcfce7,stroke:#16a34a</code></pre>"},{"location":"labs/lab2-data-models/#architecture-overview","title":"Architecture Overview","text":"<pre><code>flowchart LR\n    subgraph Sources\n        R[Reddit API&lt;br/&gt;~500 records]\n        N[News API&lt;br/&gt;~300 records]\n    end\n\n    subgraph Bronze[\"Bronze Layer (Staging)\"]\n        SR[stg_reddit__posts&lt;br/&gt;VIEW]\n        SN[stg_news__articles&lt;br/&gt;VIEW]\n    end\n\n    subgraph Silver[\"Silver Layer (Intermediate)\"]\n        IU[int_sentiment_unified&lt;br/&gt;TABLE&lt;br/&gt;800 rows]\n    end\n\n    subgraph Gold[\"Gold Layer (Marts)\"]\n        FCT[fct_sentiment_events&lt;br/&gt;INCREMENTAL&lt;br/&gt;Fact Table]\n        MART[mart_daily_sentiment&lt;br/&gt;INCREMENTAL&lt;br/&gt;Aggregate]\n    end\n\n    R --&gt;|CSV| SR\n    N --&gt;|CSV| SN\n    SR --&gt; IU\n    SN --&gt; IU\n    IU --&gt; FCT\n    FCT --&gt; MART\n\n    style Sources fill:#fef3c7\n    style Bronze fill:#dbeafe\n    style Silver fill:#fef9c3\n    style Gold fill:#dcfce7</code></pre>"},{"location":"labs/lab2-data-models/#staging-layer","title":"Staging Layer","text":""},{"location":"labs/lab2-data-models/#stg_reddit__posts","title":"stg_reddit__posts","text":"<p>Purpose: Clean and standardize Reddit post data from raw CSV files.</p> <p>Materialization: View (real-time freshness)</p> <p>Source: <code>data/raw/reddit_real.csv</code></p> <p>Transformations: - Column renaming to snake_case - Type casting for proper data types - Timestamp parsing and validation - Basic null handling</p> <pre><code>graph LR\n    A[reddit_real.csv] --&gt; B[Column Rename]\n    B --&gt; C[Type Casting]\n    C --&gt; D[Timestamp Parse]\n    D --&gt; E[stg_reddit__posts]\n\n    style A fill:#fee2e2\n    style E fill:#dbeafe</code></pre> <p>Schema:</p> Column Type Nullable Description Example post_id VARCHAR No Unique Reddit post identifier <code>reddit_00042</code> author VARCHAR Yes Reddit username <code>user_5234</code> brand VARCHAR No Brand mentioned <code>Coca-Cola</code> title VARCHAR Yes Post title <code>Post about brand 42</code> body VARCHAR Yes Post body text <code>Discussion about CPG...</code> upvotes BIGINT Yes Number of upvotes <code>3452</code> comments_count BIGINT Yes Number of comments <code>127</code> created_at TIMESTAMP No Post creation time <code>2024-11-15 14:23:11</code> sentiment_score DOUBLE No Sentiment (-1 to 1) <code>0.435</code> source VARCHAR No Always 'reddit' <code>reddit</code> ingested_at TIMESTAMP No Ingestion timestamp <code>2025-01-15 10:30:00</code> <p>Row Count: ~500 records</p> <p>Sample SQL: <pre><code>-- View Reddit sentiment by brand\nSELECT \n    brand,\n    COUNT(*) as post_count,\n    AVG(sentiment_score) as avg_sentiment,\n    AVG(upvotes) as avg_upvotes,\n    AVG(comments_count) as avg_comments\nFROM {{ ref('stg_reddit__posts') }}\nGROUP BY brand\nORDER BY avg_sentiment DESC;\n</code></pre></p> <p>Quality Checks: - No NULL post_ids - Valid timestamps (not future dates) - Sentiment scores in valid range</p>"},{"location":"labs/lab2-data-models/#stg_news__articles","title":"stg_news__articles","text":"<p>Purpose: Clean and standardize news article data from raw CSV files.</p> <p>Materialization: View (real-time freshness)</p> <p>Source: <code>data/raw/news_real.csv</code></p> <p>Transformations: - Column renaming to snake_case - Type casting for proper data types - URL validation - Timestamp parsing</p> <pre><code>graph LR\n    A[news_real.csv] --&gt; B[Column Rename]\n    B --&gt; C[Type Casting]\n    C --&gt; D[URL Validate]\n    D --&gt; E[Timestamp Parse]\n    E --&gt; F[stg_news__articles]\n\n    style A fill:#fee2e2\n    style F fill:#dbeafe</code></pre> <p>Schema:</p> Column Type Nullable Description Example article_id VARCHAR No Unique article identifier <code>news_00123</code> publication VARCHAR Yes News source <code>Forbes</code> brand VARCHAR No Brand mentioned <code>PepsiCo</code> headline VARCHAR Yes Article headline <code>News: PepsiCo announces...</code> body VARCHAR Yes Article body <code>Article content...</code> url VARCHAR Yes Article URL <code>https://example.com/article-123</code> published_at TIMESTAMP No Publication time <code>2024-12-01 09:15:00</code> sentiment_score DOUBLE No Sentiment (-1 to 1) <code>-0.234</code> source VARCHAR No Always 'news' <code>news</code> ingested_at TIMESTAMP No Ingestion timestamp <code>2025-01-15 10:30:00</code> <p>Row Count: ~300 records</p> <p>Sample SQL: <pre><code>-- Top publications by sentiment\nSELECT \n    publication,\n    COUNT(*) as article_count,\n    AVG(sentiment_score) as avg_sentiment,\n    MIN(sentiment_score) as min_sentiment,\n    MAX(sentiment_score) as max_sentiment\nFROM {{ ref('stg_news__articles') }}\nGROUP BY publication\nORDER BY article_count DESC;\n</code></pre></p> <p>Quality Checks: - No NULL article_ids - Valid URLs format - Valid timestamps</p>"},{"location":"labs/lab2-data-models/#intermediate-layer","title":"Intermediate Layer","text":""},{"location":"labs/lab2-data-models/#int_sentiment_unified","title":"int_sentiment_unified","text":"<p>Purpose: Unified sentiment data with business logic, enrichment, and quality validation.</p> <p>Materialization: Table (full refresh)</p> <p>Sources:  - {{ ref('stg_reddit__posts') }} - {{ ref('stg_news__articles') }}</p> <p>Key Features: - \ud83d\udd11 Surrogate key generation for unique identification - \ud83d\udcca Sentiment categorization (positive/negative/neutral) - \ud83d\udcc8 Engagement metrics and percentiles - \ud83d\udcc9 7-day moving averages - \u2705 Quality validation and flags - \ud83d\udd04 Window functions for trends</p> <pre><code>graph TB\n    A[stg_reddit__posts] --&gt; B[Union]\n    C[stg_news__articles] --&gt; B\n    B --&gt; D[Generate Surrogate Keys]\n    D --&gt; E[Sentiment Categorization]\n    E --&gt; F[Engagement Metrics]\n    F --&gt; G[Moving Averages]\n    G --&gt; H[Quality Validation]\n    H --&gt; I[int_sentiment_unified]\n\n    style I fill:#fef3c7</code></pre> <p>Schema:</p> Column Type Description Business Logic Keys &amp; Identifiers sentiment_event_id VARCHAR Unique event identifier <code>md5(content_id + published_at + source)</code> brand_key VARCHAR Brand surrogate key <code>md5(brand)</code> source_key INTEGER Source identifier <code>1=reddit, 2=news</code> content_id VARCHAR Source-specific ID From source system content_hash VARCHAR Content hash For deduplication Core Data creator VARCHAR Author/publication Username or publication name brand VARCHAR Brand name <code>Coca-Cola</code>, <code>PepsiCo</code>, etc headline VARCHAR Title/headline Post title or article headline body_text VARCHAR Full content Complete text content Sentiment Metrics sentiment_score DOUBLE Sentiment score <code>-1</code> to <code>1</code> range sentiment_category VARCHAR Category <code>positive/negative/neutral</code> Engagement Metrics engagement_count BIGINT Engagement metric Upvotes/shares/likes engagement_percentile DOUBLE Percentile rank <code>0.0</code> to <code>1.0</code> post_rank_by_brand BIGINT Rank within brand By engagement DESC Temporal Data published_at TIMESTAMP Publication time When content was published source VARCHAR Source platform <code>reddit</code> or <code>news</code> ingested_at TIMESTAMP Ingestion time When data was loaded Trend Analysis moving_avg_sentiment_7d DOUBLE 7-day MA Rolling average sentiment sentiment_change DOUBLE Change from previous Sentiment delta Metadata _dbt_load_date TIMESTAMP dbt load timestamp When model ran _dbt_run_timestamp VARCHAR dbt run identifier Run batch ID <p>Row Count: ~800 records (Reddit + News combined)</p> <p>Business Logic Implementation:</p>"},{"location":"labs/lab2-data-models/#1-sentiment-categorization","title":"1. Sentiment Categorization","text":"<p><pre><code>CASE \n    WHEN sentiment_score &gt;= {{ var('sentiment_threshold_positive') }} THEN 'positive'\n    WHEN sentiment_score &lt;= {{ var('sentiment_threshold_negative') }} THEN 'negative'\n    ELSE 'neutral'\nEND as sentiment_category\n</code></pre> Thresholds: <code>positive &gt;= 0.3</code>, <code>negative &lt;= -0.3</code></p>"},{"location":"labs/lab2-data-models/#2-engagement-percentile","title":"2. Engagement Percentile","text":"<pre><code>ROUND(\n    (engagement_count) / \n    NULLIF(MAX(engagement_count) OVER (), 0),\n    3\n) as engagement_percentile\n</code></pre>"},{"location":"labs/lab2-data-models/#3-brand-ranking","title":"3. Brand Ranking","text":"<pre><code>ROW_NUMBER() OVER (\n    PARTITION BY brand \n    ORDER BY engagement_count DESC\n) as post_rank_by_brand\n</code></pre>"},{"location":"labs/lab2-data-models/#4-moving-average-7-day","title":"4. Moving Average (7-day)","text":"<pre><code>AVG(sentiment_score) OVER (\n    PARTITION BY brand \n    ORDER BY published_at \n    ROWS BETWEEN 6 PRECEDING AND CURRENT ROW\n) as moving_avg_sentiment_7d\n</code></pre>"},{"location":"labs/lab2-data-models/#5-sentiment-change","title":"5. Sentiment Change","text":"<pre><code>sentiment_score - LAG(sentiment_score) OVER (\n    PARTITION BY brand \n    ORDER BY published_at\n) as sentiment_change\n</code></pre> <p>Data Flow Diagram:</p> <pre><code>sequenceDiagram\n    participant S as Staging\n    participant U as Union\n    participant K as Keys\n    participant B as Business Logic\n    participant Q as Quality\n    participant I as int_sentiment_unified\n\n    S-&gt;&gt;U: Reddit + News data\n    U-&gt;&gt;K: Generate surrogate keys\n    K-&gt;&gt;B: Add sentiment categories\n    B-&gt;&gt;B: Calculate engagement metrics\n    B-&gt;&gt;B: Compute moving averages\n    B-&gt;&gt;B: Calculate trends\n    B-&gt;&gt;Q: Validate quality\n    Q-&gt;&gt;I: Store validated data</code></pre> <p>Quality Filters: - \u2705 Sentiment score between -1 and 1 - \u2705 No future dates - \u2705 No NULL required fields - \u2705 Valid brand names</p> <p>Sample Queries:</p> <pre><code>-- Top performing content by brand\nSELECT \n    brand,\n    headline,\n    sentiment_score,\n    engagement_count,\n    engagement_percentile,\n    post_rank_by_brand\nFROM {{ ref('int_sentiment_unified') }}\nWHERE post_rank_by_brand &lt;= 10\nORDER BY brand, post_rank_by_brand;\n\n-- Sentiment trends over time\nSELECT \n    brand,\n    DATE_TRUNC('day', published_at) as date,\n    AVG(sentiment_score) as daily_sentiment,\n    AVG(moving_avg_sentiment_7d) as trend_7d,\n    COUNT(*) as content_count\nFROM {{ ref('int_sentiment_unified') }}\nGROUP BY brand, DATE_TRUNC('day', published_at)\nORDER BY brand, date;\n\n-- Sentiment distribution\nSELECT \n    sentiment_category,\n    COUNT(*) as count,\n    ROUND(AVG(sentiment_score), 3) as avg_score,\n    ROUND(AVG(engagement_count), 0) as avg_engagement\nFROM {{ ref('int_sentiment_unified') }}\nGROUP BY sentiment_category\nORDER BY avg_score DESC;\n</code></pre>"},{"location":"labs/lab2-data-models/#mart-layer","title":"Mart Layer","text":""},{"location":"labs/lab2-data-models/#fct_sentiment_events","title":"fct_sentiment_events","text":"<p>Purpose: Fact table for all sentiment events (granular, event-level data).</p> <p>Materialization: Incremental (delete+insert strategy)</p> <p>Grain: One row per sentiment event (Reddit post or news article)</p> <p>Unique Key: <code>sentiment_event_id</code></p> <p>Source: {{ ref('int_sentiment_unified') }}</p> <pre><code>graph TB\n    subgraph \"Incremental Logic\"\n        A[int_sentiment_unified] --&gt; B{is_incremental?}\n        B --&gt;|Yes| C[Filter: published_at &gt; MAX]\n        B --&gt;|No| D[Load all data]\n        C --&gt; E[Filter: quality_flag = VALID]\n        D --&gt; E\n        E --&gt; F[fct_sentiment_events]\n    end\n\n    style F fill:#dcfce7,stroke:#16a34a</code></pre> <p>Schema:</p> Column Type Contract Description Primary Key sentiment_event_id VARCHAR \u2705 NOT NULL, UNIQUE Unique event identifier Foreign Keys brand_key VARCHAR \u2705 NOT NULL Brand dimension key source_key INTEGER Source identifier (1=reddit, 2=news) Descriptive Attributes content_id VARCHAR Source-specific content ID creator VARCHAR Author or publication name brand VARCHAR Brand name headline VARCHAR Post title or article headline body_text VARCHAR Full content text source VARCHAR Platform (reddit/news) Metrics engagement_count BIGINT Upvotes, shares, likes sentiment_score DOUBLE \u2705 NOT NULL, [-1,1] Sentiment score sentiment_category VARCHAR positive/negative/neutral Temporal Dimensions published_at TIMESTAMP Original publish time published_date DATE \u2705 NOT NULL Publish date (for partitioning) published_year BIGINT Year (for aggregation) published_month BIGINT Month 1-12 published_day_of_week BIGINT Day 0-6 (0=Monday) published_hour BIGINT Hour 0-23 ingested_at TIMESTAMP Data ingestion time Quality &amp; Metadata quality_flag VARCHAR \u2705 Accepted values VALID/INVALID_SENTIMENT/NULL_HEADLINE _dbt_updated_at TIMESTAMP Last update time _dbt_run_timestamp VARCHAR dbt run identifier <p>Row Count: ~800 records (only VALID quality_flag)</p> <p>Incremental Logic:</p> <pre><code>WITH base_data AS (\n    SELECT *,\n        CASE \n            WHEN NOT {{ validate_sentiment('sentiment_score') }} THEN 'INVALID_SENTIMENT'\n            WHEN headline IS NULL THEN 'NULL_HEADLINE'\n            ELSE 'VALID'\n        END as quality_flag\n    FROM {{ ref('int_sentiment_unified') }}\n\n    {% if is_incremental() %}\n    WHERE published_at &gt; (SELECT MAX(published_at) FROM {{ this }})\n    {% endif %}\n)\n\nSELECT * FROM base_data\nWHERE quality_flag = 'VALID'\n</code></pre> <p>Data Quality Tests:</p> <pre><code>graph LR\n    A[fct_sentiment_events] --&gt; B[Uniqueness Test]\n    A --&gt; C[Not Null Tests]\n    A --&gt; D[Range Tests]\n    A --&gt; E[Accepted Values]\n    A --&gt; F[Custom Logic]\n\n    B --&gt; B1[sentiment_event_id UNIQUE]\n    C --&gt; C1[sentiment_event_id NOT NULL]\n    C --&gt; C2[brand_key NOT NULL]\n    C --&gt; C3[sentiment_score NOT NULL]\n    C --&gt; C4[published_date NOT NULL]\n    D --&gt; D1[sentiment_score BETWEEN -1 AND 1]\n    D --&gt; D2[No future dates]\n    E --&gt; E1[quality_flag values]\n    F --&gt; F1[Business validations]\n\n    style A fill:#dcfce7</code></pre> <p>Sample Queries:</p> <pre><code>-- Daily sentiment by brand\nSELECT \n    published_date,\n    brand,\n    COUNT(*) as event_count,\n    ROUND(AVG(sentiment_score), 3) as avg_sentiment,\n    SUM(engagement_count) as total_engagement\nFROM {{ ref('fct_sentiment_events') }}\nGROUP BY published_date, brand\nORDER BY published_date DESC, brand;\n\n-- Sentiment distribution\nSELECT \n    sentiment_category,\n    COUNT(*) as count,\n    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as pct,\n    ROUND(AVG(sentiment_score), 3) as avg_score,\n    ROUND(AVG(engagement_count), 0) as avg_engagement\nFROM {{ ref('fct_sentiment_events') }}\nGROUP BY sentiment_category\nORDER BY count DESC;\n\n-- Top brands by positive sentiment\nSELECT \n    brand,\n    COUNT(*) as total_mentions,\n    SUM(CASE WHEN sentiment_category = 'positive' THEN 1 ELSE 0 END) as positive_mentions,\n    ROUND(AVG(sentiment_score), 3) as avg_sentiment,\n    ROUND(AVG(engagement_count), 0) as avg_engagement\nFROM {{ ref('fct_sentiment_events') }}\nGROUP BY brand\nORDER BY avg_sentiment DESC;\n\n-- Temporal patterns (hourly)\nSELECT \n    published_hour,\n    COUNT(*) as event_count,\n    ROUND(AVG(sentiment_score), 3) as avg_sentiment\nFROM {{ ref('fct_sentiment_events') }}\nGROUP BY published_hour\nORDER BY published_hour;\n</code></pre>"},{"location":"labs/lab2-data-models/#mart_daily_sentiment","title":"mart_daily_sentiment","text":"<p>Purpose: Daily aggregated sentiment metrics by brand with anomaly detection.</p> <p>Materialization: Incremental (delete+insert strategy)</p> <p>Grain: One row per (date, brand) combination</p> <p>Unique Key: ['sentiment_date', 'brand']</p> <p>Source: {{ ref('fct_sentiment_events') }}</p> <pre><code>graph TB\n    A[fct_sentiment_events] --&gt; B[Group by date, brand]\n    B --&gt; C[Calculate Aggregates]\n    C --&gt; D[Compute Z-Scores]\n    D --&gt; E[Flag Anomalies]\n    E --&gt; F[mart_daily_sentiment]\n\n    subgraph \"Aggregations\"\n        C --&gt; C1[Avg/Min/Max Sentiment]\n        C --&gt; C2[Content Counts]\n        C --&gt; C3[Engagement Metrics]\n        C --&gt; C4[Source Diversity]\n    end\n\n    subgraph \"Anomaly Detection\"\n        D --&gt; D1[Z-Score &gt; 2]\n        D1 --&gt; E\n    end\n\n    style F fill:#dcfce7,stroke:#16a34a</code></pre> <p>Schema:</p> Column Type Contract Description Calculation Dimensions sentiment_date DATE \u2705 NOT NULL Aggregation date <code>CAST(published_date AS DATE)</code> brand VARCHAR Brand name Dimension Sentiment Metrics avg_sentiment DOUBLE \u2705 [-1,1] Average sentiment <code>AVG(sentiment_score)</code> min_sentiment DOUBLE Minimum sentiment <code>MIN(sentiment_score)</code> max_sentiment DOUBLE Maximum sentiment <code>MAX(sentiment_score)</code> stddev_sentiment DOUBLE Standard deviation <code>STDDEV(sentiment_score)</code> Content Counts content_count BIGINT Total pieces of content <code>COUNT(*)</code> positive_count BIGINT Positive content count <code>SUM(CASE category = 'positive')</code> negative_count BIGINT Negative content count <code>SUM(CASE category = 'negative')</code> neutral_count BIGINT Neutral content count <code>SUM(CASE category = 'neutral')</code> positive_ratio DOUBLE Ratio of positive <code>positive_count / content_count</code> Engagement total_engagement HUGEINT Sum of all engagement <code>SUM(engagement_count)</code> avg_engagement DOUBLE Average engagement <code>AVG(engagement_count)</code> Diversity source_count BIGINT Unique sources <code>COUNT(DISTINCT source)</code> Anomaly Detection z_score_sentiment DOUBLE Statistical z-score <code>(avg - brand_avg) / brand_stddev</code> anomaly_flag VARCHAR \u2705 Accepted values NORMAL/ANOMALY <code>ABS(z_score) &gt; 2</code> Metadata mart_load_date TIMESTAMP Mart load time <code>get_current_timestamp()</code> <p>Aggregation Logic:</p> <pre><code>SELECT\n    CAST(published_date AS DATE) as sentiment_date,\n    brand,\n\n    -- Sentiment metrics\n    AVG(sentiment_score) as avg_sentiment,\n    MIN(sentiment_score) as min_sentiment,\n    MAX(sentiment_score) as max_sentiment,\n    STDDEV(sentiment_score) as stddev_sentiment,\n\n    -- Content counts\n    COUNT(*) as content_count,\n    SUM(CASE WHEN sentiment_category = 'positive' THEN 1 ELSE 0 END) as positive_count,\n    SUM(CASE WHEN sentiment_category = 'negative' THEN 1 ELSE 0 END) as negative_count,\n    SUM(CASE WHEN sentiment_category = 'neutral' THEN 1 ELSE 0 END) as neutral_count,\n    positive_count::DOUBLE / NULLIF(content_count, 0) as positive_ratio,\n\n    -- Engagement\n    SUM(engagement_count) as total_engagement,\n    AVG(engagement_count) as avg_engagement,\n\n    -- Diversity\n    COUNT(DISTINCT source) as source_count,\n\n    -- Anomaly detection\n    (avg_sentiment - AVG(avg_sentiment) OVER (PARTITION BY brand)) /\n    NULLIF(STDDEV(avg_sentiment) OVER (PARTITION BY brand), 0) as z_score_sentiment,\n\n    CASE\n        WHEN ABS(z_score_sentiment) &gt; 2 THEN 'ANOMALY'\n        ELSE 'NORMAL'\n    END as anomaly_flag,\n\n    get_current_timestamp() as mart_load_date\n\nFROM {{ ref('fct_sentiment_events') }}\nGROUP BY sentiment_date, brand\n</code></pre> <p>Anomaly Detection Visualization:</p> <pre><code>graph LR\n    A[Daily Sentiment] --&gt; B[Calculate Brand Mean]\n    A --&gt; C[Calculate Brand StdDev]\n    B --&gt; D[Compute Z-Score]\n    C --&gt; D\n    D --&gt; E{|Z-Score| &gt; 2?}\n    E --&gt;|Yes| F[ANOMALY]\n    E --&gt;|No| G[NORMAL]\n\n    style F fill:#fee2e2,stroke:#dc2626\n    style G fill:#dcfce7,stroke:#16a34a</code></pre> <p>Incremental Logic:</p> <pre><code>{% if is_incremental() %}\n    WHERE sentiment_date &gt; (SELECT MAX(sentiment_date) FROM {{ this }})\n{% endif %}\n</code></pre> <p>Sample Queries:</p> <pre><code>-- Recent anomalies\nSELECT \n    sentiment_date,\n    brand,\n    ROUND(avg_sentiment, 3) as avg_sentiment,\n    ROUND(z_score_sentiment, 2) as z_score,\n    anomaly_flag,\n    content_count\nFROM {{ ref('mart_daily_sentiment') }}\nWHERE anomaly_flag = 'ANOMALY'\n    AND sentiment_date &gt;= CURRENT_DATE - INTERVAL '7 days'\nORDER BY sentiment_date DESC, ABS(z_score_sentiment) DESC;\n\n-- Brand comparison\nSELECT \n    brand,\n    ROUND(AVG(avg_sentiment), 3) as overall_sentiment,\n    ROUND(AVG(positive_ratio), 3) as positive_ratio,\n    SUM(content_count) as total_mentions,\n    ROUND(AVG(avg_engagement), 0) as avg_engagement,\n    SUM(CASE WHEN anomaly_flag = 'ANOMALY' THEN 1 ELSE 0 END) as anomaly_days\nFROM {{ ref('mart_daily_sentiment') }}\nGROUP BY brand\nORDER BY overall_sentiment DESC;\n\n-- Trend analysis (7-day)\nSELECT \n    sentiment_date,\n    brand,\n    ROUND(avg_sentiment, 3) as sentiment,\n    ROUND(LAG(avg_sentiment, 7) OVER (PARTITION BY brand ORDER BY sentiment_date), 3) as sentiment_7d_ago,\n    ROUND(avg_sentiment - LAG(avg_sentiment, 7) OVER (PARTITION BY brand ORDER BY sentiment_date), 3) as change_7d\nFROM {{ ref('mart_daily_sentiment') }}\nWHERE sentiment_date &gt;= CURRENT_DATE - INTERVAL '30 days'\nORDER BY brand, sentiment_date;\n\n-- Weekly summary\nSELECT \n    DATE_TRUNC('week', sentiment_date) as week_start,\n    brand,\n    ROUND(AVG(avg_sentiment), 3) as weekly_sentiment,\n    SUM(content_count) as weekly_mentions,\n    ROUND(AVG(positive_ratio), 3) as avg_positive_ratio\nFROM {{ ref('mart_daily_sentiment') }}\nGROUP BY DATE_TRUNC('week', sentiment_date), brand\nORDER BY week_start DESC, brand;\n</code></pre>"},{"location":"labs/lab2-data-models/#macros","title":"Macros","text":""},{"location":"labs/lab2-data-models/#generate_surrogate_key","title":"generate_surrogate_key","text":"<p>Purpose: Generate MD5 hash-based surrogate keys for unique identification.</p> <p>Location: <code>dbt/macros/generate_surrogate_key.sql</code></p> <p>Usage: <pre><code>{{ generate_surrogate_key(['column1', 'column2', 'column3']) }}\n</code></pre></p> <p>Implementation: <pre><code>{% macro generate_surrogate_key(columns) %}\n    md5(concat(\n        {% for col in columns %}\n            coalesce(cast({{ col }} as varchar), '')\n            {%- if not loop.last %},{% endif %}\n        {% endfor %}\n    ))\n{% endmacro %}\n</code></pre></p> <p>Example: <pre><code>-- Generate unique event ID\n{{ generate_surrogate_key(['content_id', 'published_at', 'source']) }} as sentiment_event_id\n</code></pre></p>"},{"location":"labs/lab2-data-models/#validate_sentiment","title":"validate_sentiment","text":"<p>Purpose: Validate sentiment scores are within acceptable range.</p> <p>Location: <code>dbt/macros/validate_sentiment.sql</code></p> <p>Usage: <pre><code>WHERE {{ validate_sentiment('sentiment_score') }}\n</code></pre></p> <p>Implementation: <pre><code>{% macro validate_sentiment(column_name) %}\n    ({{ column_name }} &gt;= -1 AND {{ column_name }} &lt;= 1)\n{% endmacro %}\n</code></pre></p>"},{"location":"labs/lab2-data-models/#get_current_timestamp","title":"get_current_timestamp","text":"<p>Purpose: Get current timestamp consistently across models.</p> <p>Location: <code>dbt/macros/get_current_timestamp.sql</code></p> <p>Usage: <pre><code>get_current_timestamp() as load_timestamp\n</code></pre></p>"},{"location":"labs/lab2-data-models/#data-lineage-complete-view","title":"Data Lineage Complete View","text":"<pre><code>graph TB\n    subgraph \"Raw Data\"\n        R[reddit_real.csv&lt;br/&gt;500 rows]\n        N[news_real.csv&lt;br/&gt;300 rows]\n    end\n\n    subgraph \"Bronze - Staging\"\n        SR[stg_reddit__posts&lt;br/&gt;VIEW&lt;br/&gt;500 rows]\n        SN[stg_news__articles&lt;br/&gt;VIEW&lt;br/&gt;300 rows]\n    end\n\n    subgraph \"Silver - Intermediate\"\n        IU[int_sentiment_unified&lt;br/&gt;TABLE&lt;br/&gt;800 rows&lt;br/&gt;+ Surrogate Keys&lt;br/&gt;+ Business Logic&lt;br/&gt;+ Quality Flags]\n    end\n\n    subgraph \"Gold - Marts\"\n        FCT[fct_sentiment_events&lt;br/&gt;INCREMENTAL&lt;br/&gt;~800 rows&lt;br/&gt;Grain: One per event&lt;br/&gt;Key: sentiment_event_id]\n        MART[mart_daily_sentiment&lt;br/&gt;INCREMENTAL&lt;br/&gt;~varies&lt;br/&gt;Grain: One per date-brand&lt;br/&gt;Key: [date, brand]]\n    end\n\n    subgraph \"Tests\"\n        T1[14 Data Quality Tests&lt;br/&gt;\u2705 All Passing]\n    end\n\n    R --&gt;|Clean &amp; Type| SR\n    N --&gt;|Clean &amp; Type| SN\n    SR --&gt;|UNION ALL| IU\n    SN --&gt;|UNION ALL| IU\n    IU --&gt;|Filter VALID| FCT\n    FCT --&gt;|Aggregate Daily| MART\n    FCT -.-&gt;|Validate| T1\n    MART -.-&gt;|Validate| T1\n\n    style R fill:#fee2e2\n    style N fill:#fee2e2\n    style SR fill:#dbeafe\n    style SN fill:#dbeafe\n    style IU fill:#fef3c7\n    style FCT fill:#dcfce7\n    style MART fill:#dcfce7\n    style T1 fill:#e0e7ff</code></pre>"},{"location":"labs/lab2-data-models/#model-execution-order","title":"Model Execution Order","text":"<p>dbt automatically determines execution order based on dependencies:</p> <pre><code>graph LR\n    A[1. Staging Models&lt;br/&gt;Parallel] --&gt; B[2. Intermediate&lt;br/&gt;Serial]\n    B --&gt; C[3. Mart Models&lt;br/&gt;Parallel]\n    C --&gt; D[4. Tests&lt;br/&gt;Parallel]\n\n    style A fill:#dbeafe\n    style B fill:#fef3c7\n    style C fill:#dcfce7\n    style D fill:#e0e7ff</code></pre> <p>Execution Time: ~2-3 seconds for full build with sample data</p>"},{"location":"labs/lab2-data-models/#model-statistics","title":"Model Statistics","text":"Model Type Rows Columns Tests Build Time stg_reddit__posts View 500 11 0 &lt;0.1s stg_news__articles View 300 10 0 &lt;0.1s int_sentiment_unified Table 800 23 0 0.4s fct_sentiment_events Incremental ~800 20 8 0.3s mart_daily_sentiment Incremental varies 14 6 0.2s"},{"location":"labs/lab2-data-models/#query-performance-tips","title":"Query Performance Tips","text":""},{"location":"labs/lab2-data-models/#optimize-staging-queries","title":"Optimize Staging Queries","text":"<pre><code>-- Use WHERE clauses early\nSELECT *\nFROM {{ ref('stg_reddit__posts') }}\nWHERE published_at &gt;= CURRENT_DATE - INTERVAL '30 days'  -- Filter early\n    AND brand IN ('Coca-Cola', 'PepsiCo');  -- Reduce data volume\n</code></pre>"},{"location":"labs/lab2-data-models/#leverage-incremental-models","title":"Leverage Incremental Models","text":"<pre><code>-- Query only recent data\nSELECT *\nFROM {{ ref('fct_sentiment_events') }}\nWHERE published_date &gt;= CURRENT_DATE - INTERVAL '7 days';\n</code></pre>"},{"location":"labs/lab2-data-models/#use-aggregates-for-dashboards","title":"Use Aggregates for Dashboards","text":"<pre><code>-- Use pre-aggregated mart instead of raw events\nSELECT *\nFROM {{ ref('mart_daily_sentiment') }}\nWHERE sentiment_date &gt;= CURRENT_DATE - INTERVAL '30 days';\n-- Much faster than aggregating fct_sentiment_events\n</code></pre>"},{"location":"labs/lab2-data-models/#data-quality-summary","title":"Data Quality Summary","text":"<pre><code>pie title Test Coverage by Category\n    \"Uniqueness\" : 2\n    \"Not Null\" : 5\n    \"Range Validation\" : 2\n    \"Accepted Values\" : 2\n    \"Custom Logic\" : 3</code></pre> <p>Test Results: \u2705 14/14 Passing</p>"},{"location":"labs/lab2-data-models/#model-relationships","title":"Model Relationships","text":"<pre><code>erDiagram\n    FCT_SENTIMENT_EVENTS ||--o{ MART_DAILY_SENTIMENT : aggregates\n    FCT_SENTIMENT_EVENTS {\n        varchar sentiment_event_id PK\n        varchar brand_key FK\n        integer source_key FK\n        double sentiment_score\n        date published_date\n    }\n    MART_DAILY_SENTIMENT {\n        date sentiment_date PK\n        varchar brand PK\n        double avg_sentiment\n        bigint content_count\n        varchar anomaly_flag\n    }</code></pre>"},{"location":"labs/lab2-data-models/#sentiment-distribution","title":"Sentiment Distribution","text":"<pre><code>graph TD\n    A[All Events&lt;br/&gt;800 total] --&gt; B{Sentiment Score}\n    B --&gt;|&gt;= 0.3| C[Positive&lt;br/&gt;~33%]\n    B --&gt;|Between| D[Neutral&lt;br/&gt;~33%]\n    B --&gt;|&lt;= -0.3| E[Negative&lt;br/&gt;~33%]\n\n    style C fill:#dcfce7,stroke:#16a34a\n    style D fill:#fef3c7,stroke:#f59e0b\n    style E fill:#fee2e2,stroke:#dc2626</code></pre>"},{"location":"labs/lab2-data-models/#related-documentation","title":"Related Documentation","text":"<ul> <li>Lab 2 Overview - Architecture and features</li> <li>Lab 2 Setup - Installation guide</li> <li>Lab 2 Troubleshooting - Common issues</li> <li>Lab 2 Quick Reference - Common commands</li> <li>dbt Documentation - Official dbt docs</li> </ul> <p>Last Updated: November 2025 Maintained By: narensham Total Models: 5 Total Tests: 14 Status: \u2705 Production Ready</p>"},{"location":"labs/lab2-overview/","title":"Lab 2: Market Sentiment Analysis","text":""},{"location":"labs/lab2-overview/#overview","title":"Overview","text":"<p>Lab 2 implements an automated, production-ready market sentiment analysis pipeline for Consumer Packaged Goods (CPG) brands. It ingests real data from Reddit and News APIs, performs sentiment analysis, tracks competitive intelligence, and provides interactive dashboards for brand monitoring.</p> <p>Key Highlights: - \u2705 Real API integration (Reddit PRAW + NewsAPI) - \u2705 GitHub Actions automation (weekly runs) - \u2705 Shared brand taxonomy across labs - \u2705 Advanced competitive analytics - \u2705 Trending topics detection - \u2705 Interactive Streamlit dashboards - \u2705 Production-grade data quality tests</p>"},{"location":"labs/lab2-overview/#purpose","title":"Purpose","text":"<p>Monitor and analyze public sentiment towards CPG brands to:</p> <ul> <li>\ud83d\udcca Track brand reputation trends over time</li> <li>\u26a0\ufe0f Identify sentiment anomalies requiring immediate attention</li> <li>\ud83c\udfc6 Benchmark competitive positioning across brands</li> <li>\ud83d\udd25 Detect trending topics and emerging themes</li> <li>\ud83d\udcc8 Measure share of voice and engagement metrics</li> <li>\ud83c\udfaf Inform brand strategy with data-driven insights</li> </ul>"},{"location":"labs/lab2-overview/#architecture","title":"Architecture","text":""},{"location":"labs/lab2-overview/#data-flow","title":"Data Flow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              INGESTION LAYER                             \u2502\n\u2502  Reddit API (PRAW) + News API \u2192 Python Scripts          \u2502\n\u2502  \u2193                                                       \u2502\n\u2502  Raw CSV Files (data/raw/)                              \u2502\n\u2502    - reddit_brands.csv                                  \u2502\n\u2502    - news_brands.csv                                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502            TRANSFORMATION LAYER (dbt)                    \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502\n\u2502  \u2502  STAGING                            \u2502                \u2502\n\u2502  \u2502  - stg_reddit__posts                \u2502                \u2502\n\u2502  \u2502  - stg_news__articles               \u2502                \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2502\n\u2502                 \u2193                                        \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502\n\u2502  \u2502  INTERMEDIATE                       \u2502                \u2502\n\u2502  \u2502  - int_sentiment_unified            \u2502                \u2502\n\u2502  \u2502    (surrogate keys, enrichment)     \u2502                \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2502\n\u2502                 \u2193                                        \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502\n\u2502  \u2502  DIMENSIONS                         \u2502                \u2502\n\u2502  \u2502  - dim_brands                       \u2502                \u2502\n\u2502  \u2502  - dim_sources                      \u2502                \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2502\n\u2502                 \u2193                                        \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502\n\u2502  \u2502  MARTS (Analytics Layer)            \u2502                \u2502\n\u2502  \u2502  - fct_sentiment_events             \u2502                \u2502\n\u2502  \u2502  - mart_daily_sentiment             \u2502                \u2502\n\u2502  \u2502  - mart_brand_competitive_analysis  \u2502                \u2502\n\u2502  \u2502  - mart_trending_topics             \u2502                \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           ANALYTICS &amp; VISUALIZATION                      \u2502\n\u2502  - Streamlit Dashboard (Main)                           \u2502\n\u2502  - Competitive Intelligence Page                        \u2502\n\u2502  - DuckDB Database (OLAP queries)                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"labs/lab2-overview/#automation","title":"Automation","text":"<p>GitHub Actions Workflow (<code>weekly_pipeline.yml</code>): - Schedule: Every Sunday at 2 AM UTC - Manual Trigger: Available via GitHub Actions UI - Steps:   1. Data ingestion from APIs   2. dbt transformations   3. Data quality tests   4. Artifact upload (DuckDB database)</p>"},{"location":"labs/lab2-overview/#technology-stack","title":"Technology Stack","text":"Component Technology Version Database DuckDB 0.9.1 Transformation dbt-core + dbt-duckdb 1.7.0 Data Quality dbt tests + dbt_expectations 0.10.4 Orchestration Prefect + GitHub Actions 2.x Ingestion Python (PRAW + NewsAPI) 3.11+ Sentiment Analysis VADER Sentiment 3.3.2 Dashboards Streamlit + Plotly 1.28.1 CI/CD GitHub Actions N/A"},{"location":"labs/lab2-overview/#shared-brand-taxonomy","title":"Shared Brand Taxonomy","text":"<p>New in v2: Lab 2 now uses a shared brand taxonomy located in <code>buildcpg-labs/shared/config/brand_taxonomy.yaml</code>.</p>"},{"location":"labs/lab2-overview/#benefits","title":"Benefits:","text":"<ul> <li>\u2705 Single source of truth for brand data across all labs</li> <li>\u2705 Consistent parent company mapping</li> <li>\u2705 Standardized category classifications</li> <li>\u2705 Easy to extend with new brands</li> </ul>"},{"location":"labs/lab2-overview/#structure","title":"Structure:","text":"<pre><code>beverages:\n  coca_cola:\n    parent: \"The Coca-Cola Company\"\n    ticker: \"KO\"\n    brands:\n      - name: \"Coca-Cola\"\n        aliases: [\"Coke\", \"Coca Cola\"]\n        category: \"Carbonated Soft Drinks\"\n      - name: \"Sprite\"\n        category: \"Lemon-Lime Soda\"\n</code></pre>"},{"location":"labs/lab2-overview/#usage","title":"Usage:","text":"<p>Ingestion scripts automatically load the taxonomy:</p> <pre><code>from pathlib import Path\nimport yaml\n\n# Try shared config first, fall back to lab-specific\nSHARED_CONFIG = Path(\"../shared/config/brand_taxonomy.yaml\")\ntaxonomy = yaml.safe_load(open(SHARED_CONFIG))\n</code></pre>"},{"location":"labs/lab2-overview/#data-models","title":"Data Models","text":""},{"location":"labs/lab2-overview/#staging-layer","title":"Staging Layer","text":""},{"location":"labs/lab2-overview/#stg_reddit__posts","title":"<code>stg_reddit__posts</code>","text":"<ul> <li>Purpose: Clean and normalize Reddit data</li> <li>Materialization: Ephemeral</li> <li>Key Operations:</li> <li>Quality flagging (NULL_POST_ID, INVALID_SENTIMENT, etc.)</li> <li>Deduplication by <code>(post_id, author, created_at)</code></li> <li>Sentiment score normalization (-1 to 1)</li> <li>Brand name standardization (UPPER, TRIM)</li> <li>Source: <code>data/raw/reddit_brands.csv</code></li> </ul>"},{"location":"labs/lab2-overview/#stg_news__articles","title":"<code>stg_news__articles</code>","text":"<ul> <li>Purpose: Clean and normalize news data</li> <li>Materialization: Ephemeral</li> <li>Key Operations:</li> <li>URL-based deduplication</li> <li>Headline text cleaning</li> <li>Parent company mapping from taxonomy</li> <li>Source: <code>data/raw/news_brands.csv</code></li> </ul>"},{"location":"labs/lab2-overview/#intermediate-layer","title":"Intermediate Layer","text":""},{"location":"labs/lab2-overview/#int_sentiment_unified","title":"<code>int_sentiment_unified</code>","text":"<ul> <li>Purpose: Unified view of all sentiment data with enrichment</li> <li>Materialization: Table</li> <li>Key Features:</li> <li>Surrogate key generation (MD5 hash)</li> <li>Brand key mapping to <code>dim_brands</code></li> <li>Sentiment categorization (positive/neutral/negative)</li> <li>Engagement percentile calculation</li> <li>7-day moving average sentiment</li> <li>Sentiment change tracking</li> <li>Row-level deduplication</li> </ul>"},{"location":"labs/lab2-overview/#dimensions","title":"Dimensions","text":""},{"location":"labs/lab2-overview/#dim_brands","title":"<code>dim_brands</code>","text":"<ul> <li>Grain: One row per brand</li> <li>Purpose: Brand master data</li> <li>Columns:</li> <li><code>brand_key</code> (surrogate key)</li> <li><code>brand</code> (name)</li> <li><code>parent_company</code> (from taxonomy)</li> <li><code>category</code> (product category)</li> <li><code>company_type</code> (Public/Private)</li> </ul>"},{"location":"labs/lab2-overview/#dim_sources","title":"<code>dim_sources</code>","text":"<ul> <li>Grain: One row per data source</li> <li>Purpose: Source metadata</li> <li>Values: Reddit, News</li> </ul>"},{"location":"labs/lab2-overview/#marts-layer","title":"Marts Layer","text":""},{"location":"labs/lab2-overview/#fct_sentiment_events-fact-table","title":"<code>fct_sentiment_events</code> (Fact Table)","text":"<p>Grain: One row per sentiment event (Reddit post or news article)</p> <p>Key Features: - Materialization: Incremental (delete+insert strategy) - Unique Key: <code>sentiment_event_id</code> - Contract: Enforced for data quality - 1-day overlap window for late-arriving data</p> <p>Columns (25+): - Event identifiers - Brand and source foreign keys - Sentiment metrics (score, category) - Engagement metrics - Temporal dimensions (date parts) - Quality flags - Metadata timestamps</p> <p>Use Cases: - Event-level analysis - Drill-down into specific posts/articles - Source of truth for all sentiment data</p>"},{"location":"labs/lab2-overview/#mart_daily_sentiment-daily-aggregates","title":"<code>mart_daily_sentiment</code> (Daily Aggregates)","text":"<p>Grain: One row per (date, brand)</p> <p>Key Features: - Materialization: Incremental - Unique Key: <code>[sentiment_date, brand]</code> - Anomaly Detection: Z-score based flagging</p> <p>Metrics: - Volume: Content count, source diversity - Sentiment: Avg, min, max, stddev, category breakdown - Engagement: Total, average - Quality: Positive ratio, sentiment distribution - Anomalies: Z-score, anomaly flag (NORMAL/ANOMALY)</p> <p>Use Cases: - Daily trend analysis - Time-series visualization - Anomaly alerting</p>"},{"location":"labs/lab2-overview/#mart_brand_competitive_analysis-new","title":"<code>mart_brand_competitive_analysis</code> \u2b50 NEW","text":"<p>Grain: One row per brand</p> <p>Key Features: - Share of voice calculation - Competitive positioning classification - Category benchmarking - Momentum tracking (7d vs 30d)</p> <p>Metrics: - Volume: Total mentions, Reddit vs News split - Share of Voice: % of total mentions - Sentiment: Avg sentiment, volatility, net score - Engagement: Reddit engagement rate - Rankings: Sentiment rank, volume rank (within category) - Positioning: MARKET_LEADER, NICHE_FAVORITE, AT_RISK, LOW_VISIBILITY, MIDDLE_PACK</p> <p>Use Cases: - Competitive intelligence - Market positioning analysis - Executive dashboards</p>"},{"location":"labs/lab2-overview/#mart_trending_topics-new","title":"<code>mart_trending_topics</code> \u2b50 NEW","text":"<p>Grain: One row per (brand, topic)</p> <p>Key Features: - Keyword-based topic extraction - Trending score calculation - Sentiment tone classification</p> <p>Topics Tracked: 1. Product Launch 2. Pricing 3. Quality Issues 4. Sustainability 5. Health &amp; Nutrition 6. Marketing &amp; Advertising 7. Brand Comparison 8. Taste &amp; Flavor</p> <p>Metrics: - Volume: Mention count by source, recency (7d, 14d) - Trending Score: Combines recency, growth, engagement - Sentiment: Avg sentiment, positive/negative breakdown - Classification: HOT, TRENDING, STABLE, EMERGING</p> <p>Use Cases: - Topic monitoring - Content strategy planning - Early warning system</p>"},{"location":"labs/lab2-overview/#key-features","title":"Key Features","text":""},{"location":"labs/lab2-overview/#1-surrogate-key-generation","title":"1. Surrogate Key Generation","text":"<p>Ensures unique event identification using MD5 hashing:</p> <pre><code>MD5(CONCAT(\n    COALESCE(CAST(content_id AS VARCHAR), ''),\n    '|',\n    COALESCE(CAST(published_at AS VARCHAR), ''),\n    '|',\n    COALESCE(CAST(row_num AS VARCHAR), '')\n))\n</code></pre>"},{"location":"labs/lab2-overview/#2-sentiment-categorization","title":"2. Sentiment Categorization","text":"<p>Configurable thresholds via dbt variables:</p> <pre><code>vars:\n  sentiment_threshold_positive: 0.5\n  sentiment_threshold_negative: -0.5\n</code></pre> <p>Categories: - Positive: Score \u2265 0.5 - Negative: Score \u2264 -0.5 - Neutral: -0.5 &lt; Score &lt; 0.5</p>"},{"location":"labs/lab2-overview/#3-quality-flags","title":"3. Quality Flags","text":"<p>Multi-level quality tracking:</p> Flag Description Action <code>VALID</code> Clean, usable data \u2705 Include <code>INVALID_SENTIMENT</code> Score out of range \u274c Exclude <code>NULL_HEADLINE</code> Missing headline \u274c Exclude <code>FUTURE_DATE</code> Published date in future \u274c Exclude"},{"location":"labs/lab2-overview/#4-anomaly-detection","title":"4. Anomaly Detection","text":"<p>Statistical z-score based anomaly flagging:</p> <pre><code>z_score = (daily_avg - brand_avg) / brand_stddev\n\nIF ABS(z_score) &gt; 2 THEN 'ANOMALY' ELSE 'NORMAL'\n</code></pre> <p>Use Case: Alert on days with unusual sentiment patterns (e.g., PR crises, viral posts)</p>"},{"location":"labs/lab2-overview/#5-incremental-processing","title":"5. Incremental Processing","text":"<p>Efficient data processing:</p> <pre><code>{% if is_incremental() %}\n    WHERE published_at &gt;= (\n        SELECT COALESCE(MAX(published_at), '1900-01-01') \n        FROM {{ this }}\n    ) - INTERVAL 1 DAY\n{% endif %}\n</code></pre> <p>Benefits: - Only processes new data - 1-day overlap window captures late-arriving records - Faster pipeline runs (minutes vs hours)</p>"},{"location":"labs/lab2-overview/#6-competitive-positioning","title":"6. Competitive Positioning","text":"<p>Automatic classification based on percentile rankings:</p> Position Criteria Description <code>MARKET_LEADER</code> High sentiment + High volume Top brands with strong positive buzz <code>NICHE_FAVORITE</code> High sentiment + Low volume Well-loved but niche brands <code>AT_RISK</code> Low sentiment + High volume Popular brands facing reputation issues <code>LOW_VISIBILITY</code> Low sentiment + Low volume Struggling brands <code>MIDDLE_PACK</code> Moderate on both Average performing brands"},{"location":"labs/lab2-overview/#7-trending-score-algorithm","title":"7. Trending Score Algorithm","text":"<p>Combines multiple signals:</p> <pre><code>trending_score = \n    (mentions_last_7d * 2.0 / mentions_last_14d) \n    * LOG(mention_count + 1) \n    * (1 + avg_engagement / 100.0)\n</code></pre> <p>Components: - Recency: 7d vs 14d growth - Volume: Logarithmic mention count - Engagement: Normalized engagement boost</p>"},{"location":"labs/lab2-overview/#data-quality","title":"Data Quality","text":""},{"location":"labs/lab2-overview/#dbt-tests-implemented","title":"dbt Tests Implemented","text":"<p>\u2705 20+ tests passing</p>"},{"location":"labs/lab2-overview/#uniqueness-tests","title":"Uniqueness Tests:","text":"<ul> <li><code>sentiment_event_id</code> must be unique</li> <li><code>(sentiment_date, brand)</code> composite key must be unique</li> <li><code>(brand, parent_company)</code> in competitive analysis</li> <li><code>(brand, topic)</code> in trending topics</li> </ul>"},{"location":"labs/lab2-overview/#not-null-tests","title":"Not Null Tests:","text":"<ul> <li>All primary and foreign keys</li> <li>Core metrics (sentiment_score, published_date)</li> </ul>"},{"location":"labs/lab2-overview/#range-tests","title":"Range Tests:","text":"<ul> <li>Sentiment scores: -1 to 1</li> <li>Share of voice: 0 to 100%</li> <li>No future dates allowed</li> </ul>"},{"location":"labs/lab2-overview/#accepted-values-tests","title":"Accepted Values Tests:","text":"<ul> <li><code>quality_flag</code>: ['VALID', 'INVALID_SENTIMENT', 'NULL_HEADLINE']</li> <li><code>anomaly_flag</code>: ['NORMAL', 'ANOMALY']</li> <li><code>competitive_position</code>: ['MARKET_LEADER', 'NICHE_FAVORITE', ...]</li> <li><code>trend_status</code>: ['HOT', 'TRENDING', 'STABLE', 'EMERGING']</li> <li><code>sentiment_tone</code>: ['POSITIVE_BUZZ', 'NEGATIVE_BUZZ', 'NEUTRAL_DISCUSSION']</li> </ul>"},{"location":"labs/lab2-overview/#custom-business-logic-tests","title":"Custom Business Logic Tests:","text":"<ul> <li>All brands from fact table appear in daily mart</li> <li>Sentiment validation macro reusability</li> <li>Fact table grain enforcement</li> </ul>"},{"location":"labs/lab2-overview/#test-results","title":"Test Results","text":"<pre><code>$ dbt test --profiles-dir .\n\nCompleted with 20 total tests, 0 failures, 0 errors, 0 skipped\n</code></pre>"},{"location":"labs/lab2-overview/#dashboards","title":"Dashboards","text":""},{"location":"labs/lab2-overview/#main-dashboard-streamlit_apppy","title":"Main Dashboard (<code>streamlit_app.py</code>)","text":"<p>Features: - \ud83d\udcca KPI metrics with week-over-week deltas - \ud83d\udcc8 Sentiment trend with dual-axis (sentiment + volume) - \ud83d\udcf1 Source distribution (Reddit vs News) - \ud83c\udfe2 Brand sentiment comparison (horizontal bar chart) - \ud83c\udfaf Sentiment category distribution - \ud83d\udcf0 Top content tables (most positive, negative, engaged) - \u26a0\ufe0f Anomaly detection and alerts</p> <p>Filters: - Date range picker - Brand multi-select - Source multi-select - Sentiment category filter</p>"},{"location":"labs/lab2-overview/#competitive-intelligence-page-new","title":"Competitive Intelligence Page \u2b50 NEW","text":"<p>Features: - \ud83d\udce2 Share of Voice analysis (top 15 brands) - \ud83c\udfaf Competitive Positioning Matrix (sentiment vs volume) - \ud83d\udd25 Trending Topics table - Company and category filters</p> <p>Insights: - Identify market leaders vs at-risk brands - Benchmark against category averages - Track momentum (7d vs 30d growth)</p>"},{"location":"labs/lab2-overview/#github-actions-automation","title":"GitHub Actions Automation","text":""},{"location":"labs/lab2-overview/#workflow-configuration","title":"Workflow Configuration","text":"<p>File: <code>.github/workflows/weekly_pipeline.yml</code></p> <pre><code>name: Weekly Sentiment Pipeline\n\non:\n  schedule:\n    - cron: '0 2 * * 0'  # Every Sunday at 2 AM UTC\n  workflow_dispatch:  # Manual trigger\n\njobs:\n  run-pipeline:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v4\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n      - name: Install dependencies\n        run: pip install -r requirements.txt\n      - name: Run pipeline\n        env:\n          REDDIT_CLIENT_ID: ${{ secrets.REDDIT_CLIENT_ID }}\n          REDDIT_CLIENT_SECRET: ${{ secrets.REDDIT_CLIENT_SECRET }}\n          REDDIT_USER_AGENT: ${{ secrets.REDDIT_USER_AGENT }}\n          NEWS_API_KEY: ${{ secrets.NEWS_API_KEY }}\n        run: python orchestrate_weekly.py\n      - name: Upload artifacts\n        uses: actions/upload-artifact@v4\n        with:\n          name: duckdb-database\n          path: data/lab2_market_sentiment.duckdb\n</code></pre>"},{"location":"labs/lab2-overview/#orchestration-orchestrate_weeklypy","title":"Orchestration (<code>orchestrate_weekly.py</code>)","text":"<p>Prefect Flow Steps: 1. \u2705 Check prerequisites (.env, dbt project, scripts) 2. \ud83d\udd04 Ingest data (Reddit + News APIs) 3. \ud83d\udd04 Run dbt models (<code>dbt build</code>) 4. \ud83d\udcca Generate data quality report</p> <p>Error Handling: - Task retries with exponential backoff - Timeout protection (30min ingestion, 10min dbt) - Comprehensive logging - Quality report generation</p> <p>Outputs: - DuckDB database artifact - Execution logs - Data quality metrics</p>"},{"location":"labs/lab2-overview/#metrics-available","title":"Metrics Available","text":""},{"location":"labs/lab2-overview/#event-level-metrics-fct_sentiment_events","title":"Event-Level Metrics (<code>fct_sentiment_events</code>)","text":"<ul> <li>Sentiment score (-1 to 1)</li> <li>Sentiment category (positive/neutral/negative)</li> <li>Engagement count (upvotes, comments, shares)</li> <li>Content quality flag</li> <li>Published date/time (with date parts)</li> <li>Brand, parent company, category</li> <li>Source (Reddit vs News)</li> <li>Headline and body text</li> </ul>"},{"location":"labs/lab2-overview/#daily-aggregate-metrics-mart_daily_sentiment","title":"Daily Aggregate Metrics (<code>mart_daily_sentiment</code>)","text":"<ul> <li>Average, min, max, stddev sentiment</li> <li>Content counts (total, by category)</li> <li>Positive ratio</li> <li>Total and average engagement</li> <li>Source diversity</li> <li>Z-score for anomaly detection</li> <li>Anomaly flag (NORMAL/ANOMALY)</li> </ul>"},{"location":"labs/lab2-overview/#competitive-metrics-mart_brand_competitive_analysis","title":"Competitive Metrics (<code>mart_brand_competitive_analysis</code>)","text":"<ul> <li>Share of voice (%)</li> <li>Net sentiment score (positive % - negative %)</li> <li>Sentiment and volume percentile ranks</li> <li>Engagement rate</li> <li>Momentum (7d vs 30d growth %)</li> <li>Competitive position classification</li> <li>Sentiment vs category benchmark</li> <li>Category and parent company averages</li> </ul>"},{"location":"labs/lab2-overview/#topic-metrics-mart_trending_topics","title":"Topic Metrics (<code>mart_trending_topics</code>)","text":"<ul> <li>Trending score</li> <li>Mention count (total, 7d, 14d)</li> <li>Sentiment breakdown</li> <li>Engagement metrics</li> <li>Trend status (HOT, TRENDING, etc.)</li> <li>Sentiment tone (POSITIVE_BUZZ, etc.)</li> <li>Topic rank within brand</li> </ul>"},{"location":"labs/lab2-overview/#common-issues-solutions","title":"Common Issues &amp; Solutions","text":""},{"location":"labs/lab2-overview/#issue-duplicate-sentiment_event_id-historical","title":"Issue: Duplicate sentiment_event_id (Historical)","text":"<p>Root Cause: Wrong dbt config parameter and double WHERE clause</p> <p>Solution Applied: 1. \u2705 Changed <code>unique_id</code> \u2192 <code>unique_key</code> in model config 2. \u2705 Fixed double WHERE clause with proper CTE structure 3. \u2705 Enhanced surrogate key to include <code>row_num</code> for true uniqueness 4. \u2705 Fixed data type mismatches (INTEGER \u2192 BIGINT)</p> <p>Result: All duplicates resolved, 0 failures in uniqueness tests</p>"},{"location":"labs/lab2-overview/#issue-api-rate-limits","title":"Issue: API Rate Limits","text":"<p>Symptoms: Ingestion fails with 429 errors</p> <p>Solution: - Reddit: Use PRAW's built-in rate limiting - NewsAPI: Implement exponential backoff with retries - Free tier limits: 100 requests/day (NewsAPI), respect Reddit's 60 req/min</p>"},{"location":"labs/lab2-overview/#issue-memory-issues-with-duckdb","title":"Issue: Memory Issues with DuckDB","text":"<p>Symptoms: Out of memory errors on large datasets</p> <p>Solution: <pre><code># Set memory limits in duckdb connection\nconn = duckdb.connect(str(DB_PATH))\nconn.execute(\"SET memory_limit='2GB';\")\nconn.execute(\"SET max_memory='2GB';\")\n</code></pre></p>"},{"location":"labs/lab2-overview/#issue-streamlit-caching","title":"Issue: Streamlit Caching","text":"<p>Symptoms: Dashboard shows stale data</p> <p>Solution: <pre><code>@st.cache_data(ttl=300)  # 5-minute TTL\ndef load_data():\n    # ...\n</code></pre></p>"},{"location":"labs/lab2-overview/#current-status","title":"Current Status","text":""},{"location":"labs/lab2-overview/#completed","title":"\u2705 Completed","text":"<ul> <li>\u2705 Real API integration (Reddit PRAW, NewsAPI)</li> <li>\u2705 Shared brand taxonomy architecture</li> <li>\u2705 GitHub Actions automation</li> <li>\u2705 Prefect orchestration</li> <li>\u2705 VADER sentiment analysis</li> <li>\u2705 All 4 mart tables (events, daily, competitive, topics)</li> <li>\u2705 20+ data quality tests</li> <li>\u2705 Main Streamlit dashboard</li> <li>\u2705 Competitive Intelligence page</li> <li>\u2705 Full documentation</li> </ul>"},{"location":"labs/lab2-overview/#in-progress","title":"\ud83d\udd04 In Progress","text":"<ul> <li>\ud83d\udd04 Advanced NLP topic extraction (vs keyword matching)</li> <li>\ud83d\udd04 Hugging Face transformer sentiment (vs VADER)</li> <li>\ud83d\udd04 Email alerting for anomalies</li> </ul>"},{"location":"labs/lab2-overview/#planned","title":"\ud83d\udccb Planned","text":"<ul> <li>\ud83d\udccb Historical backfill (6+ months of data)</li> <li>\ud83d\udccb ML-based sentiment prediction</li> <li>\ud83d\udccb Correlation analysis (sentiment vs stock prices)</li> <li>\ud83d\udccb Slack/Teams integration for alerts</li> <li>\ud83d\udccb Export to BigQuery for larger-scale analysis</li> </ul>"},{"location":"labs/lab2-overview/#getting-started","title":"Getting Started","text":"<p>See Lab 2 Setup Guide for: - Prerequisites and installation - API key configuration - First pipeline run - Dashboard deployment</p>"},{"location":"labs/lab2-overview/#performance-metrics","title":"Performance Metrics","text":"<p>Pipeline Runtime (typical): - Ingestion: 5-10 minutes (100 posts per brand) - dbt build: 2-3 minutes (incremental) - Total: ~10 minutes end-to-end</p> <p>Data Volumes (typical weekly run): - Reddit posts: 500-1,000 - News articles: 300-500 - Total sentiment events: 800-1,500 - Daily aggregates: 50-100 rows</p> <p>Dashboard Performance: - Initial load: 2-3 seconds - Filter updates: &lt;1 second (cached) - Data refresh: 5 minutes (TTL)</p>"},{"location":"labs/lab2-overview/#related-documentation","title":"Related Documentation","text":"<ul> <li>Lab 2 Setup Guide</li> <li>Lab 2 Troubleshooting</li> <li>dbt Model Documentation</li> <li>API Integration Guide</li> </ul> <p>Lab Owner: narensham Created: November 2025 Last Updated: November 2025 Status: \u2705 Production-Ready</p>"},{"location":"labs/lab2-quick-reference/","title":"Lab 2: Quick Reference","text":"<p>Quick reference for common tasks and commands in the Market Sentiment Analysis pipeline.</p>"},{"location":"labs/lab2-quick-reference/#quick-start","title":"Quick Start","text":"<pre><code># 1. Setup environment\npython3.11 -m venv venv\nsource venv/bin/activate  # or `venv\\Scripts\\activate` on Windows\npip install -r requirements.txt\n\n# 2. Configure API keys\ncp .env.example .env\n# Edit .env with your API keys\n\n# 3. Run initial ingestion\npython pipelines/ingest_real_data.py\n\n# 4. Build dbt models\ncd dbt &amp;&amp; dbt build --profiles-dir .\n\n# 5. Launch dashboard\nstreamlit run app/streamlit_app.py\n</code></pre>"},{"location":"labs/lab2-quick-reference/#common-commands","title":"Common Commands","text":""},{"location":"labs/lab2-quick-reference/#data-ingestion","title":"Data Ingestion","text":"<pre><code># Full ingestion (Reddit + News)\npython pipelines/ingest_real_data.py\n\n# Test APIs only (no save)\npython -c \"from pipelines.ingest_real_data import *; test_apis()\"\n\n# Check raw data\nls -lh data/raw/\nhead -n 5 data/raw/reddit_brands.csv\n</code></pre>"},{"location":"labs/lab2-quick-reference/#dbt-operations","title":"dbt Operations","text":"<pre><code>cd dbt\n\n# Build everything\ndbt build --profiles-dir .\n\n# Run only changed models\ndbt build --select state:modified+ --profiles-dir .\n\n# Run specific model and downstream\ndbt run --select fct_sentiment_events+ --profiles-dir .\n\n# Test everything\ndbt test --profiles-dir .\n\n# Test specific model\ndbt test --select fct_sentiment_events --profiles-dir .\n\n# Full refresh (rebuild from scratch)\ndbt build --profiles-dir . --full-refresh\n\n# Debug model\ndbt compile --select model_name --profiles-dir .\ncat target/compiled/lab2_market_sentiment/models/.../model_name.sql\n</code></pre>"},{"location":"labs/lab2-quick-reference/#dashboard","title":"Dashboard","text":"<pre><code># Start dashboard\nstreamlit run app/streamlit_app.py\n\n# Use custom port\nstreamlit run app/streamlit_app.py --server.port 8502\n\n# Clear cache\n# In browser: Press 'C' key\n</code></pre>"},{"location":"labs/lab2-quick-reference/#database-queries","title":"Database Queries","text":"<pre><code># Connect to DuckDB\nduckdb data/lab2_market_sentiment.duckdb\n\n# Or use Python\npython &lt;&lt; 'EOF'\nimport duckdb\nconn = duckdb.connect('data/lab2_market_sentiment.duckdb')\n\n# Check tables\ntables = conn.execute(\"SHOW TABLES\").fetchall()\nprint(tables)\n\n# Query data\ndf = conn.execute(\"SELECT * FROM fct_sentiment_events LIMIT 10\").df()\nprint(df)\n\nconn.close()\nEOF\n</code></pre>"},{"location":"labs/lab2-quick-reference/#github-actions","title":"GitHub Actions","text":"<pre><code># Trigger manual run\n# Go to: Actions tab \u2192 Weekly Sentiment Pipeline \u2192 Run workflow\n\n# Check workflow status\ngh workflow view \"Weekly Sentiment Pipeline\"\ngh run list --workflow=\"Weekly Sentiment Pipeline\"\n\n# Download artifact\ngh run download &lt;run-id&gt; --name duckdb-database\n</code></pre>"},{"location":"labs/lab2-quick-reference/#file-structure-reference","title":"File Structure Reference","text":"<pre><code>lab2_market_sentiment/\n\u251c\u2500\u2500 .env                          # API keys (gitignored)\n\u251c\u2500\u2500 requirements.txt              # Python dependencies\n\u251c\u2500\u2500 orchestrate_weekly.py         # Prefect orchestration\n\u2502\n\u251c\u2500\u2500 .github/\n\u2502   \u2514\u2500\u2500 workflows/\n\u2502       \u2514\u2500\u2500 weekly_pipeline.yml   # GitHub Actions\n\u2502\n\u251c\u2500\u2500 pipelines/\n\u2502   \u251c\u2500\u2500 ingest_real_data.py      # Main ingestion\n\u2502   \u2514\u2500\u2500 ingest_brands.py          # Brand-aware ingestion\n\u2502\n\u251c\u2500\u2500 dbt/\n\u2502   \u251c\u2500\u2500 dbt_project.yml           # dbt config\n\u2502   \u251c\u2500\u2500 profiles.yml              # Database connection\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u2502   \u251c\u2500\u2500 staging/              # Raw data cleaning\n\u2502   \u2502   \u251c\u2500\u2500 intermediate/         # Business logic\n\u2502   \u2502   \u251c\u2500\u2500 dimensions/           # Dimension tables\n\u2502   \u2502   \u2514\u2500\u2500 mart/                 # Analytics marts\n\u2502   \u2514\u2500\u2500 schema.yml                # Tests &amp; documentation\n\u2502\n\u251c\u2500\u2500 app/\n\u2502   \u251c\u2500\u2500 streamlit_app.py          # Main dashboard\n\u2502   \u2514\u2500\u2500 pages/\n\u2502       \u2514\u2500\u2500 Competitive_Intelligence.py\n\u2502\n\u2514\u2500\u2500 data/\n    \u251c\u2500\u2500 raw/                      # CSV files from APIs\n    \u251c\u2500\u2500 lab2_market_sentiment.duckdb  # Database\n    \u2514\u2500\u2500 cache/                    # API cache (optional)\n</code></pre>"},{"location":"labs/lab2-quick-reference/#environment-variables","title":"Environment Variables","text":"<pre><code># Reddit API\nREDDIT_CLIENT_ID=your_client_id\nREDDIT_CLIENT_SECRET=your_secret\nREDDIT_USER_AGENT=\"CPG Sentiment Tracker v1.0\"\n\n# News API\nNEWS_API_KEY=your_api_key\n</code></pre>"},{"location":"labs/lab2-quick-reference/#dbt-model-relationships","title":"dbt Model Relationships","text":"<pre><code>Raw Data (CSV)\n    \u2193\nstg_reddit__posts \u2500\u2510\nstg_news__articles \u2518\n    \u2193\nint_sentiment_unified\n    \u2193\n    \u251c\u2500\u2192 dim_brands\n    \u251c\u2500\u2192 dim_sources\n    \u251c\u2500\u2192 fct_sentiment_events\n    \u251c\u2500\u2192 mart_daily_sentiment\n    \u251c\u2500\u2192 mart_brand_competitive_analysis\n    \u2514\u2500\u2192 mart_trending_topics\n</code></pre>"},{"location":"labs/lab2-quick-reference/#key-metrics","title":"Key Metrics","text":""},{"location":"labs/lab2-quick-reference/#event-level-fct_sentiment_events","title":"Event-Level (<code>fct_sentiment_events</code>)","text":"<ul> <li><code>sentiment_score</code>: -1 to 1</li> <li><code>sentiment_category</code>: positive/neutral/negative</li> <li><code>engagement_count</code>: upvotes/shares</li> <li><code>published_at</code>: timestamp</li> </ul>"},{"location":"labs/lab2-quick-reference/#daily-aggregates-mart_daily_sentiment","title":"Daily Aggregates (<code>mart_daily_sentiment</code>)","text":"<ul> <li><code>avg_sentiment</code>: daily average</li> <li><code>content_count</code>: posts per day</li> <li><code>positive_ratio</code>: % positive</li> <li><code>z_score_sentiment</code>: anomaly detection</li> <li><code>anomaly_flag</code>: NORMAL/ANOMALY</li> </ul>"},{"location":"labs/lab2-quick-reference/#competitive-mart_brand_competitive_analysis","title":"Competitive (<code>mart_brand_competitive_analysis</code>)","text":"<ul> <li><code>share_of_voice_pct</code>: % of total mentions</li> <li><code>net_sentiment_score</code>: positive % - negative %</li> <li><code>competitive_position</code>: MARKET_LEADER, NICHE_FAVORITE, etc.</li> <li><code>momentum_pct</code>: 7d vs 30d growth</li> </ul>"},{"location":"labs/lab2-quick-reference/#topics-mart_trending_topics","title":"Topics (<code>mart_trending_topics</code>)","text":"<ul> <li><code>trending_score</code>: combined metric</li> <li><code>mention_count</code>: total mentions</li> <li><code>trend_status</code>: HOT/TRENDING/STABLE/EMERGING</li> <li><code>sentiment_tone</code>: POSITIVE_BUZZ/NEGATIVE_BUZZ/NEUTRAL</li> </ul>"},{"location":"labs/lab2-quick-reference/#sql-queries-cheat-sheet","title":"SQL Queries Cheat Sheet","text":"<pre><code>-- Check data freshness\nSELECT MAX(published_at) as latest_data\nFROM fct_sentiment_events;\n\n-- Brand sentiment summary\nSELECT \n    brand,\n    COUNT(*) as mentions,\n    ROUND(AVG(sentiment_score), 3) as avg_sentiment,\n    COUNT(*) FILTER (WHERE sentiment_category = 'positive') as positive\nFROM fct_sentiment_events\nWHERE published_at &gt;= CURRENT_DATE - INTERVAL '7 days'\nGROUP BY brand\nORDER BY mentions DESC;\n\n-- Anomaly detection\nSELECT \n    sentiment_date,\n    brand,\n    avg_sentiment,\n    z_score_sentiment\nFROM mart_daily_sentiment\nWHERE anomaly_flag = 'ANOMALY'\nORDER BY sentiment_date DESC;\n\n-- Top trending topics\nSELECT \n    brand,\n    topic,\n    trending_score,\n    mention_count,\n    sentiment_tone\nFROM mart_trending_topics\nWHERE trend_status IN ('HOT', 'TRENDING')\nORDER BY trending_score DESC\nLIMIT 20;\n\n-- Competitive positioning\nSELECT \n    brand,\n    parent_company,\n    share_of_voice_pct,\n    competitive_position,\n    sentiment_vs_category\nFROM mart_brand_competitive_analysis\nORDER BY share_of_voice_pct DESC;\n</code></pre>"},{"location":"labs/lab2-quick-reference/#troubleshooting-quick-fixes","title":"Troubleshooting Quick Fixes","text":"<pre><code># API rate limit errors\n# \u2192 Reduce limit_per_brand in ingest_real_data.py\n\n# DuckDB locked\nrm data/lab2_market_sentiment.duckdb.wal\n\n# dbt compilation errors\ncd dbt &amp;&amp; dbt clean &amp;&amp; dbt deps &amp;&amp; dbt build --profiles-dir .\n\n# Streamlit port in use\nstreamlit run app/streamlit_app.py --server.port 8502\n\n# Clear Streamlit cache\n# Browser: Press 'C' key\n\n# No data in dashboard\npython pipelines/ingest_real_data.py\ncd dbt &amp;&amp; dbt build --profiles-dir . --full-refresh\n\n# GitHub Actions secrets not working\n# Settings \u2192 Secrets \u2192 Verify names match exactly\n\n# Out of memory\n# Edit dbt/profiles.yml:\n# Add: threads: 2\n# Reduce memory usage\n</code></pre>"},{"location":"labs/lab2-quick-reference/#testing-commands","title":"Testing Commands","text":"<pre><code># Test API connections\npython -c \"\nimport praw, os\nfrom dotenv import load_dotenv\nload_dotenv()\nreddit = praw.Reddit(\n    client_id=os.getenv('REDDIT_CLIENT_ID'),\n    client_secret=os.getenv('REDDIT_CLIENT_SECRET'),\n    user_agent=os.getenv('REDDIT_USER_AGENT')\n)\nprint('\u2705 Reddit connected')\n\"\n\n# Test dbt models\ncd dbt\ndbt run --select model_name --profiles-dir . --debug\n\n# Run unit tests\npython -m pytest tests/  # if tests exist\n\n# Validate data\npython &lt;&lt; 'EOF'\nimport duckdb\nconn = duckdb.connect('data/lab2_market_sentiment.duckdb')\n\n# Check for nulls\nnulls = conn.execute(\"\"\"\n    SELECT \n        COUNT(*) FILTER (WHERE sentiment_score IS NULL) as null_sentiment,\n        COUNT(*) FILTER (WHERE brand IS NULL) as null_brand\n    FROM fct_sentiment_events\n\"\"\").fetchone()\nprint(f\"Null sentiment: {nulls[0]}, Null brand: {nulls[1]}\")\nEOF\n</code></pre>"},{"location":"labs/lab2-quick-reference/#performance-optimization","title":"Performance Optimization","text":"<pre><code># Reduce ingestion time\n# \u2192 Lower limit_per_brand in scripts\n\n# Speed up dbt\ncd dbt\ndbt run --profiles-dir . --threads 4\n\n# Optimize dashboard queries\n# \u2192 Use mart tables instead of fact table\n# \u2192 Add date filters to limit data\n# \u2192 Increase cache TTL\n\n# Reduce memory usage\n# \u2192 Set memory_limit in DuckDB connection\n# \u2192 Process data in chunks\n# \u2192 Use columnar selection in queries\n</code></pre>"},{"location":"labs/lab2-quick-reference/#maintenance-tasks","title":"Maintenance Tasks","text":"<pre><code># Weekly\npython pipelines/ingest_real_data.py\ncd dbt &amp;&amp; dbt build --profiles-dir .\n\n# Monthly\n# 1. Review brand taxonomy for new brands\n# 2. Check API usage and costs\n# 3. Analyze anomalies and trends\n# 4. Update documentation\n\n# Quarterly\n# 1. Full data refresh\ncd dbt &amp;&amp; dbt build --profiles-dir . --full-refresh\n\n# 2. Review and optimize slow queries\n# 3. Update Python dependencies\npip install -r requirements.txt --upgrade\n\n# 4. Clean up old cache files\nrm -rf data/cache/*\n</code></pre>"},{"location":"labs/lab2-quick-reference/#useful-links","title":"Useful Links","text":"<ul> <li>Documentation Home: Lab 2 Overview</li> <li>Setup Guide: Lab 2 Setup</li> <li>Troubleshooting: Lab 2 Troubleshooting</li> <li>API Guide: API Integration</li> <li>dbt Docs: Run <code>dbt docs generate &amp;&amp; dbt docs serve</code> in dbt/</li> <li>Reddit API: https://www.reddit.com/dev/api</li> <li>News API: https://newsapi.org/docs</li> <li>PRAW Docs: https://praw.readthedocs.io/</li> <li>DuckDB Docs: https://duckdb.org/docs/</li> </ul>"},{"location":"labs/lab2-quick-reference/#getting-help","title":"Getting Help","text":"<pre><code># Check logs\ntail -f ingestion.log\ncat dbt/logs/dbt.log\n\n# Enable debug mode\nexport DBT_DEBUG=1\ndbt run --select model_name --profiles-dir .\n\n# Get model SQL\ndbt compile --select model_name --profiles-dir .\ncat target/compiled/.../model_name.sql\n\n# Check dbt version\ndbt --version\n\n# Check Python packages\npip list\n</code></pre> <p>Last Updated: November 2025 Version: 2.0 Maintainer: narensham</p>"},{"location":"labs/lab2-setup/","title":"Lab 2: Setup Guide","text":"<p>This guide will help you set up the Market Sentiment Analysis pipeline from scratch.</p>"},{"location":"labs/lab2-setup/#prerequisites","title":"Prerequisites","text":""},{"location":"labs/lab2-setup/#system-requirements","title":"System Requirements","text":"<ul> <li>Python: 3.11 or higher</li> <li>Git: For cloning the repository</li> <li>GitHub Account: For Actions automation (optional but recommended)</li> <li>4GB RAM minimum: For DuckDB operations</li> </ul>"},{"location":"labs/lab2-setup/#api-keys-required","title":"API Keys Required","text":"<p>You'll need to obtain API keys from the following services:</p> <ol> <li>Reddit API (free)</li> <li>News API (free tier available)</li> </ol>"},{"location":"labs/lab2-setup/#step-1-clone-the-repository","title":"Step 1: Clone the Repository","text":"<pre><code># Clone the main repository\ngit clone https://github.com/yourusername/buildcpg-labs.git\ncd buildcpg-labs/lab2_market_sentiment\n</code></pre>"},{"location":"labs/lab2-setup/#step-2-set-up-python-environment","title":"Step 2: Set Up Python Environment","text":""},{"location":"labs/lab2-setup/#option-a-using-venv-recommended","title":"Option A: Using venv (Recommended)","text":"<pre><code># Create virtual environment\npython3.11 -m venv venv\n\n# Activate it\n# On macOS/Linux:\nsource venv/bin/activate\n# On Windows:\n# venv\\Scripts\\activate\n\n# Install dependencies\npip install -r requirements.txt\n</code></pre>"},{"location":"labs/lab2-setup/#option-b-using-conda","title":"Option B: Using conda","text":"<pre><code># Create conda environment\nconda create -n lab2 python=3.11\nconda activate lab2\n\n# Install dependencies\npip install -r requirements.txt\n</code></pre>"},{"location":"labs/lab2-setup/#verify-installation","title":"Verify Installation","text":"<pre><code># Check installations\npython -c \"import duckdb; print(f'DuckDB: {duckdb.__version__}')\"\npython -c \"import streamlit; print(f'Streamlit: {streamlit.__version__}')\"\ndbt --version\n</code></pre> <p>Expected output: <pre><code>DuckDB: 0.9.1\nStreamlit: 1.28.1\nCore:\n  - installed: 1.7.0\n  - latest:    X.X.X\n</code></pre></p>"},{"location":"labs/lab2-setup/#step-3-obtain-api-keys","title":"Step 3: Obtain API Keys","text":""},{"location":"labs/lab2-setup/#reddit-api-setup","title":"Reddit API Setup","text":"<ol> <li>Go to: https://www.reddit.com/prefs/apps</li> <li>Click: \"Create App\" or \"Create Another App\"</li> <li>Fill in:</li> <li>Name: <code>CPG Sentiment Tracker</code></li> <li>App type: Choose \"script\"</li> <li>Description: <code>Market sentiment analysis for CPG brands</code></li> <li>About URL: (leave blank)</li> <li>Redirect URI: <code>http://localhost:8080</code></li> <li>Click: \"Create app\"</li> <li>Save:</li> <li>Client ID: The string under \"personal use script\"</li> <li>Client Secret: The \"secret\" value</li> </ol>"},{"location":"labs/lab2-setup/#news-api-setup","title":"News API Setup","text":"<ol> <li>Go to: https://newsapi.org/register</li> <li>Register for a free account</li> <li>Verify your email</li> <li>Copy your API key from the dashboard</li> </ol> <p>Free Tier Limits: - 100 requests per day - Up to 100 results per request - Historical data: Last 30 days</p>"},{"location":"labs/lab2-setup/#step-4-configure-environment-variables","title":"Step 4: Configure Environment Variables","text":""},{"location":"labs/lab2-setup/#create-env-file","title":"Create <code>.env</code> File","text":"<pre><code># Create .env file in lab2_market_sentiment directory\ncat &gt; .env &lt;&lt; 'EOF'\n# Reddit API Credentials\nREDDIT_CLIENT_ID=your_reddit_client_id_here\nREDDIT_CLIENT_SECRET=your_reddit_client_secret_here\nREDDIT_USER_AGENT=\"CPG Sentiment Tracker v1.0 by /u/yourusername\"\n\n# News API Credentials\nNEWS_API_KEY=your_newsapi_key_here\nEOF\n</code></pre>"},{"location":"labs/lab2-setup/#verify-configuration","title":"Verify Configuration","text":"<pre><code># Test that environment variables load correctly\npython -c \"from dotenv import load_dotenv; import os; load_dotenv(); print('Reddit ID:', os.getenv('REDDIT_CLIENT_ID')[:8] + '...')\"\n</code></pre>"},{"location":"labs/lab2-setup/#step-5-set-up-shared-brand-taxonomy","title":"Step 5: Set Up Shared Brand Taxonomy","text":"<p>The brand taxonomy is shared across all labs in <code>buildcpg-labs/shared/config/</code>.</p>"},{"location":"labs/lab2-setup/#verify-shared-config-exists","title":"Verify Shared Config Exists","text":"<pre><code># Check if shared config exists\nls -la ../shared/config/brand_taxonomy.yaml\n</code></pre>"},{"location":"labs/lab2-setup/#optional-create-lab-specific-overrides","title":"Optional: Create Lab-Specific Overrides","text":"<p>If you want to add lab-specific brands without modifying the shared config:</p> <pre><code># Create lab-specific config directory\nmkdir -p config\n\n# Create override file\ncat &gt; config/brand_taxonomy.yaml &lt;&lt; 'EOF'\n# Lab 2 specific brand additions\nbeverages:\n  local_brewery:\n    parent: \"Local Craft Brewery\"\n    brands:\n      - name: \"Local IPA\"\n        category: \"Craft Beer\"\nEOF\n</code></pre> <p>The ingestion script will automatically: 1. Load shared config first 2. Merge with lab-specific overrides (if they exist)</p>"},{"location":"labs/lab2-setup/#step-6-initialize-duckdb-database","title":"Step 6: Initialize DuckDB Database","text":""},{"location":"labs/lab2-setup/#create-data-directories","title":"Create Data Directories","text":"<pre><code># Create all required directories\nmkdir -p data/raw data/bronze data/silver data/gold\n</code></pre>"},{"location":"labs/lab2-setup/#initialize-dbt","title":"Initialize dbt","text":"<pre><code># Navigate to dbt directory\ncd dbt\n\n# Install dbt packages (dbt_expectations)\ndbt deps\n\n# Run initial build (will create database and all tables)\ndbt build --profiles-dir .\n</code></pre> <p>Expected output: <pre><code>Completed successfully\n\nDone. PASS=25 WARN=0 ERROR=0 SKIP=0 TOTAL=25\n</code></pre></p>"},{"location":"labs/lab2-setup/#verify-database-creation","title":"Verify Database Creation","text":"<pre><code># Check if database was created\nls -lh ../data/lab2_market_sentiment.duckdb\n\n# Should show file size (typically 100KB-1MB empty)\n</code></pre>"},{"location":"labs/lab2-setup/#step-7-run-initial-data-ingestion","title":"Step 7: Run Initial Data Ingestion","text":""},{"location":"labs/lab2-setup/#test-api-connections","title":"Test API Connections","text":"<pre><code># Return to lab root directory\ncd ..\n\n# Test Reddit API\npython -c \"\nimport praw, os\nfrom dotenv import load_dotenv\nload_dotenv()\nreddit = praw.Reddit(\n    client_id=os.getenv('REDDIT_CLIENT_ID'),\n    client_secret=os.getenv('REDDIT_CLIENT_SECRET'),\n    user_agent=os.getenv('REDDIT_USER_AGENT')\n)\nprint('\u2705 Reddit API connected')\nprint(f'User: {reddit.read_only}')\n\"\n\n# Test News API\npython -c \"\nfrom newsapi import NewsApiClient\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\napi = NewsApiClient(api_key=os.getenv('NEWS_API_KEY'))\nresponse = api.get_top_headlines(language='en', page_size=1)\nprint('\u2705 News API connected')\nprint(f'Status: {response.get(\\\"status\\\")}')\n\"\n</code></pre>"},{"location":"labs/lab2-setup/#run-full-ingestion","title":"Run Full Ingestion","text":"<pre><code># Run the ingestion pipeline\npython pipelines/ingest_real_data.py\n</code></pre> <p>Expected output: <pre><code>INFO - \ud83d\udd04 Fetching Reddit data...\nINFO -   Searching for: Coca-Cola\nINFO -   Searching for: PepsiCo\n...\nINFO - \u2705 Reddit data ingested: 500 posts \u2192 data/raw/reddit_brands.csv\nINFO - \ud83d\udd04 Fetching news data...\nINFO - \u2705 News data ingested: 300 articles \u2192 data/raw/news_brands.csv\n</code></pre></p>"},{"location":"labs/lab2-setup/#run-dbt-transformations","title":"Run dbt Transformations","text":"<pre><code># Navigate to dbt directory\ncd dbt\n\n# Run full build (incremental models will process new data)\ndbt build --profiles-dir .\n\n# Check results\ndbt test --profiles-dir .\n</code></pre>"},{"location":"labs/lab2-setup/#verify-data","title":"Verify Data","text":"<pre><code># Quick data check using Python\ncd ..\npython &lt;&lt; 'EOF'\nimport duckdb\nconn = duckdb.connect('data/lab2_market_sentiment.duckdb')\n\n# Check record counts\ntables = {\n    'fct_sentiment_events': 'SELECT COUNT(*) FROM fct_sentiment_events',\n    'mart_daily_sentiment': 'SELECT COUNT(*) FROM mart_daily_sentiment',\n    'mart_brand_competitive_analysis': 'SELECT COUNT(*) FROM mart_brand_competitive_analysis',\n    'mart_trending_topics': 'SELECT COUNT(*) FROM mart_trending_topics'\n}\n\nprint(\"\\n\ud83d\udcca Data Summary:\")\nfor table, query in tables.items():\n    count = conn.execute(query).fetchone()[0]\n    print(f\"  {table}: {count:,} rows\")\n\n# Check brands\nbrands = conn.execute(\"SELECT COUNT(DISTINCT brand) FROM fct_sentiment_events\").fetchone()[0]\nprint(f\"\\n  Unique brands: {brands}\")\n\nconn.close()\nEOF\n</code></pre> <p>Expected output: <pre><code>\ud83d\udcca Data Summary:\n  fct_sentiment_events: 800 rows\n  mart_daily_sentiment: 75 rows\n  mart_brand_competitive_analysis: 5 rows\n  mart_trending_topics: 40 rows\n\n  Unique brands: 5\n</code></pre></p>"},{"location":"labs/lab2-setup/#step-8-launch-streamlit-dashboard","title":"Step 8: Launch Streamlit Dashboard","text":""},{"location":"labs/lab2-setup/#start-the-dashboard","title":"Start the Dashboard","text":"<pre><code># From lab root directory\nstreamlit run app/streamlit_app.py\n</code></pre> <p>The dashboard will open in your browser at <code>http://localhost:8501</code></p>"},{"location":"labs/lab2-setup/#test-dashboard-features","title":"Test Dashboard Features","text":"<ol> <li>\u2705 Main Dashboard: Check KPIs, charts load correctly</li> <li>\u2705 Filters: Try filtering by date, brand, source</li> <li>\u2705 Competitive Intelligence Page: Navigate via sidebar</li> <li>\u2705 Data Refresh: Modify filters and verify updates</li> </ol>"},{"location":"labs/lab2-setup/#step-9-set-up-github-actions-optional","title":"Step 9: Set Up GitHub Actions (Optional)","text":""},{"location":"labs/lab2-setup/#prerequisites_1","title":"Prerequisites","text":"<ul> <li>GitHub repository for your project</li> <li>Repository secrets configured</li> </ul>"},{"location":"labs/lab2-setup/#add-github-secrets","title":"Add GitHub Secrets","text":"<ol> <li>Go to your repository on GitHub</li> <li>Navigate to: Settings \u2192 Secrets and variables \u2192 Actions</li> <li>Click New repository secret for each:</li> <li><code>REDDIT_CLIENT_ID</code></li> <li><code>REDDIT_CLIENT_SECRET</code></li> <li><code>REDDIT_USER_AGENT</code></li> <li><code>NEWS_API_KEY</code></li> </ol>"},{"location":"labs/lab2-setup/#push-workflow-file","title":"Push Workflow File","text":"<pre><code># Ensure workflow file is in correct location\nls -la .github/workflows/weekly_pipeline.yml\n\n# Commit and push\ngit add .github/workflows/weekly_pipeline.yml\ngit commit -m \"Add weekly sentiment pipeline workflow\"\ngit push origin main\n</code></pre>"},{"location":"labs/lab2-setup/#test-manual-trigger","title":"Test Manual Trigger","text":"<ol> <li>Go to your repository \u2192 Actions tab</li> <li>Select Weekly Sentiment Pipeline</li> <li>Click Run workflow dropdown</li> <li>Click green Run workflow button</li> </ol> <p>Monitor the run in real-time. If successful, you'll see: - \u2705 All steps completed - Artifact available for download (DuckDB database)</p>"},{"location":"labs/lab2-setup/#schedule-verification","title":"Schedule Verification","text":"<p>The workflow will now run automatically every Sunday at 2 AM UTC.</p>"},{"location":"labs/lab2-setup/#step-10-optional-enhancements","title":"Step 10: Optional Enhancements","text":""},{"location":"labs/lab2-setup/#set-up-prefect-cloud-for-better-observability","title":"Set Up Prefect Cloud (For Better Observability)","text":"<pre><code># Install Prefect\npip install prefect\n\n# Login to Prefect Cloud (free tier)\nprefect cloud login\n\n# Deploy flow\npython orchestrate_weekly.py\n</code></pre>"},{"location":"labs/lab2-setup/#configure-email-alerts","title":"Configure Email Alerts","text":"<p>Add to <code>orchestrate_weekly.py</code>:</p> <pre><code>from prefect.blocks.notifications.email import EmailServerBlock\n\n# In your flow, add error handling:\ntry:\n    sentiment_pipeline()\nexcept Exception as e:\n    email_block = EmailServerBlock.load(\"email-alerts\")\n    email_block.notify(\n        subject=\"\u274c Sentiment Pipeline Failed\",\n        body=f\"Error: {str(e)}\"\n    )\n    raise\n</code></pre>"},{"location":"labs/lab2-setup/#add-more-brands","title":"Add More Brands","text":"<p>Edit <code>shared/config/brand_taxonomy.yaml</code>:</p> <pre><code>beverages:\n  starbucks:\n    parent: \"Starbucks Corporation\"\n    ticker: \"SBUX\"\n    brands:\n      - name: \"Starbucks\"\n        aliases: [\"Starbucks Coffee\", \"SBUX\"]\n        category: \"Coffee\"\n</code></pre> <p>Then re-run ingestion.</p>"},{"location":"labs/lab2-setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"labs/lab2-setup/#issue-api-rate-limits","title":"Issue: API Rate Limits","text":"<p>Error: <code>429 Too Many Requests</code></p> <p>Solution: <pre><code># Reduce posts_per_brand in ingest_real_data.py\nlimit_per_brand = 10  # Instead of 20\n</code></pre></p>"},{"location":"labs/lab2-setup/#issue-duckdb-file-locked","title":"Issue: DuckDB File Locked","text":"<p>Error: <code>IO Error: Could not set lock on file</code></p> <p>Solution: <pre><code># Close all connections, then:\nrm data/lab2_market_sentiment.duckdb.wal\n</code></pre></p>"},{"location":"labs/lab2-setup/#issue-dbt-models-fail","title":"Issue: dbt Models Fail","text":"<p>Error: <code>Compilation Error</code></p> <p>Solution: <pre><code># Clean and rebuild\ncd dbt\ndbt clean\ndbt deps\ndbt build --profiles-dir . --full-refresh\n</code></pre></p>"},{"location":"labs/lab2-setup/#issue-streamlit-port-already-in-use","title":"Issue: Streamlit Port Already in Use","text":"<p>Error: <code>Address already in use</code></p> <p>Solution: <pre><code># Use different port\nstreamlit run app/streamlit_app.py --server.port 8502\n</code></pre></p>"},{"location":"labs/lab2-setup/#issue-no-data-in-dashboard","title":"Issue: No Data in Dashboard","text":"<p>Symptoms: Empty charts, \"No data\" messages</p> <p>Solution: 1. Check if ingestion ran: <code>ls -la data/raw/</code> 2. Check if dbt ran: <code>duckdb data/lab2_market_sentiment.duckdb \"SELECT COUNT(*) FROM fct_sentiment_events\"</code> 3. Check Streamlit logs for errors</p>"},{"location":"labs/lab2-setup/#verification-checklist","title":"Verification Checklist","text":"<p>Before considering setup complete, verify:</p> <ul> <li> \u2705 Python 3.11+ installed</li> <li> \u2705 All dependencies installed (<code>pip list</code>)</li> <li> \u2705 API keys configured in <code>.env</code></li> <li> \u2705 Reddit and News API connections tested</li> <li> \u2705 dbt models built successfully</li> <li> \u2705 All dbt tests passing (20/20)</li> <li> \u2705 Data ingested (800+ sentiment events)</li> <li> \u2705 Streamlit dashboard loads</li> <li> \u2705 Both pages accessible (Main + Competitive Intelligence)</li> <li> \u2705 Filters work correctly</li> <li> \u2705 GitHub Actions workflow configured (optional)</li> <li> \u2705 Manual workflow run successful (optional)</li> </ul>"},{"location":"labs/lab2-setup/#next-steps","title":"Next Steps","text":"<p>Once setup is complete:</p> <ol> <li>\ud83d\udcd6 Read Lab 2 Overview for architecture details</li> <li>\ud83d\udd0d Explore dbt Model Documentation</li> <li>\ud83d\udcca Customize dashboards in <code>app/</code> directory</li> <li>\ud83d\udd27 Add more brands to taxonomy</li> <li>\ud83d\ude80 Deploy to production (e.g., Streamlit Cloud)</li> </ol>"},{"location":"labs/lab2-setup/#support","title":"Support","text":"<p>If you encounter issues not covered here:</p> <ol> <li>Check Lab 2 Troubleshooting Guide</li> <li>Review GitHub Issues</li> <li>Contact: narensham@example.com</li> </ol> <p>Setup Time: ~30 minutes Difficulty: Intermediate Last Updated: November 2025</p>"},{"location":"labs/lab2-troubleshooting/","title":"Lab 2: Troubleshooting Guide","text":"<p>Common issues and solutions for the Market Sentiment Analysis pipeline.</p>"},{"location":"labs/lab2-troubleshooting/#table-of-contents","title":"Table of Contents","text":"<ol> <li>API Issues</li> <li>Database Issues</li> <li>dbt Issues</li> <li>Dashboard Issues</li> <li>GitHub Actions Issues</li> <li>Performance Issues</li> <li>Data Quality Issues</li> </ol>"},{"location":"labs/lab2-troubleshooting/#api-issues","title":"API Issues","text":""},{"location":"labs/lab2-troubleshooting/#reddit-api-401-unauthorized","title":"Reddit API: 401 Unauthorized","text":"<p>Symptoms: <pre><code>prawcore.exceptions.ResponseException: received 401 HTTP response\n</code></pre></p> <p>Causes: - Invalid credentials - Credentials not loaded from <code>.env</code></p> <p>Solutions:</p> <pre><code># 1. Verify .env file exists and has correct values\ncat .env | grep REDDIT\n\n# 2. Test credentials directly\npython &lt;&lt; 'EOF'\nimport praw\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nreddit = praw.Reddit(\n    client_id=os.getenv('REDDIT_CLIENT_ID'),\n    client_secret=os.getenv('REDDIT_CLIENT_SECRET'),\n    user_agent=os.getenv('REDDIT_USER_AGENT')\n)\n\nprint(f\"Read only: {reddit.read_only}\")  # Should be True\nprint(\"\u2705 Credentials valid\")\nEOF\n\n# 3. If still failing, regenerate credentials at:\n# https://www.reddit.com/prefs/apps\n</code></pre>"},{"location":"labs/lab2-troubleshooting/#reddit-api-429-too-many-requests","title":"Reddit API: 429 Too Many Requests","text":"<p>Symptoms: <pre><code>prawcore.exceptions.TooManyRequests: received 429 HTTP response\n</code></pre></p> <p>Causes: - Exceeded rate limit (60 requests per minute)</p> <p>Solutions:</p> <pre><code># Option 1: Reduce requests in ingest_real_data.py\nLIMIT_PER_BRAND = 10  # Instead of 20\n\n# Option 2: Add delays between requests\nimport time\nfor brand in BRANDS:\n    # ... fetch logic ...\n    time.sleep(1)  # 1 second delay\n\n# Option 3: Use PRAW's built-in rate limiting (automatic)\n# PRAW handles this by default, but you can be explicit:\nreddit = praw.Reddit(\n    client_id=os.getenv('REDDIT_CLIENT_ID'),\n    client_secret=os.getenv('REDDIT_CLIENT_SECRET'),\n    user_agent=os.getenv('REDDIT_USER_AGENT'),\n    ratelimit_seconds=600  # Wait 10 min if rate limited\n)\n</code></pre>"},{"location":"labs/lab2-troubleshooting/#news-api-426-upgrade-required","title":"News API: 426 Upgrade Required","text":"<p>Symptoms: <pre><code>newsapi.newsapi_exception.NewsAPIException: 426 - Upgrade Required\n</code></pre></p> <p>Causes: - Exceeded free tier limit (100 requests/day) - Trying to access articles older than 30 days</p> <p>Solutions:</p> <pre><code># Option 1: Reduce requests per brand\narticles_per_brand = 10  # Instead of 20\n\n# Option 2: Only ingest on certain days\nfrom datetime import datetime\nif datetime.utcnow().weekday() == 6:  # Only on Sundays\n    ingest_news_data()\n\n# Option 3: Upgrade to paid plan\n# See: https://newsapi.org/pricing\n</code></pre>"},{"location":"labs/lab2-troubleshooting/#news-api-no-results-found","title":"News API: No Results Found","text":"<p>Symptoms: - API returns successfully but <code>articles</code> list is empty</p> <p>Causes: - Search query too specific - Date range has no matching articles - Language filter too restrictive</p> <p>Solutions:</p> <pre><code># 1. Broaden search query\nq = brand  # Instead of f\"{brand} AND (product OR launch)\"\n\n# 2. Increase date range\nfrom_date = (datetime.utcnow() - timedelta(days=30)).strftime('%Y-%m-%d')\n\n# 3. Add debug logging\nresponse = newsapi.get_everything(q=brand, from_param=from_date, language='en')\nlogger.info(f\"Articles found for {brand}: {len(response.get('articles', []))}\")\nif len(response.get('articles', [])) == 0:\n    logger.warning(f\"No articles for {brand} - try adjusting search\")\n</code></pre>"},{"location":"labs/lab2-troubleshooting/#database-issues","title":"Database Issues","text":""},{"location":"labs/lab2-troubleshooting/#duckdb-file-locked-error","title":"DuckDB: File Locked Error","text":"<p>Symptoms: <pre><code>IO Error: Could not set lock on file \"lab2_market_sentiment.duckdb\": Resource temporarily unavailable\n</code></pre></p> <p>Causes: - Another process has the database open - Stale lock file from crashed process</p> <p>Solutions:</p> <pre><code># 1. Close all connections\n# - Stop Streamlit dashboard\n# - Stop any Python scripts\n# - Close any database clients\n\n# 2. Remove lock file\nrm data/lab2_market_sentiment.duckdb.wal\n\n# 3. If issue persists, find and kill process\nlsof data/lab2_market_sentiment.duckdb\n# Then: kill -9 &lt;PID&gt;\n\n# 4. Last resort: Create new database\ncd dbt\ndbt build --profiles-dir . --full-refresh\n</code></pre>"},{"location":"labs/lab2-troubleshooting/#duckdb-out-of-memory","title":"DuckDB: Out of Memory","text":"<p>Symptoms: <pre><code>Error: Out of Memory Error: failed to allocate data of size X\n</code></pre></p> <p>Causes: - Dataset too large for available RAM - Memory leak in query</p> <p>Solutions:</p> <pre><code># 1. Set memory limits in connection\nimport duckdb\nconn = duckdb.connect('data/lab2_market_sentiment.duckdb')\nconn.execute(\"SET memory_limit='2GB';\")\nconn.execute(\"SET max_memory='2GB';\")\n\n# 2. Process data in chunks\nchunk_size = 1000\nfor offset in range(0, total_rows, chunk_size):\n    df = conn.execute(f\"SELECT * FROM table LIMIT {chunk_size} OFFSET {offset}\").df()\n    # Process chunk...\n\n# 3. Use streaming in Streamlit\n@st.cache_data(ttl=300)\ndef load_data_chunked():\n    return conn.execute(\"SELECT * FROM table LIMIT 10000\").df()\n</code></pre>"},{"location":"labs/lab2-troubleshooting/#duckdb-corrupt-database","title":"DuckDB: Corrupt Database","text":"<p>Symptoms: <pre><code>Catalog Error: Table with name X does not exist\n</code></pre> OR tables missing after successful dbt run</p> <p>Solutions:</p> <pre><code># 1. Backup existing database (if partially working)\ncp data/lab2_market_sentiment.duckdb data/lab2_market_sentiment.duckdb.backup\n\n# 2. Full refresh rebuild\ncd dbt\ndbt clean\ndbt deps\ndbt build --profiles-dir . --full-refresh\n\n# 3. If still failing, delete and recreate\nrm data/lab2_market_sentiment.duckdb*\ndbt build --profiles-dir . --full-refresh\n\n# 4. Restore from GitHub Actions artifact (if available)\n# Download artifact from successful workflow run\nunzip duckdb-database.zip\nmv lab2_market_sentiment.duckdb data/\n</code></pre>"},{"location":"labs/lab2-troubleshooting/#dbt-issues","title":"dbt Issues","text":""},{"location":"labs/lab2-troubleshooting/#dbt-compilation-error-column-not-found","title":"dbt: Compilation Error - Column Not Found","text":"<p>Symptoms: <pre><code>Compilation Error in model X\n  column \"Y\" does not exist\n</code></pre></p> <p>Causes: - Upstream model changed schema - Typo in column name - Missing column in raw data</p> <p>Solutions:</p> <pre><code># 1. Check upstream model output\ndbt run --select int_sentiment_unified --profiles-dir .\ndbt show --select int_sentiment_unified --profiles-dir .\n\n# 2. Verify raw data has expected columns\npython &lt;&lt; 'EOF'\nimport pandas as pd\ndf = pd.read_csv('data/raw/reddit_brands.csv')\nprint(\"Columns:\", df.columns.tolist())\nEOF\n\n# 3. Fix column reference in SQL\n# Check schema.yml for correct column names\n\n# 4. Full refresh if schema changed\ndbt build --select +model_name+ --profiles-dir . --full-refresh\n</code></pre>"},{"location":"labs/lab2-troubleshooting/#dbt-test-failed-uniqueness-violation","title":"dbt: Test Failed - Uniqueness Violation","text":"<p>Symptoms: <pre><code>Failure in test unique_sentiment_event_id (models/schema.yml)\n  Got 5 results, configured to fail if != 0\n</code></pre></p> <p>Causes: - Duplicate data in source files - Surrogate key generation not unique enough - Incremental logic issue</p> <p>Solutions:</p> <pre><code># 1. Identify duplicates\ncd dbt\ndbt test --select fct_sentiment_events --store-failures\n\n# 2. Query failed test results\npython &lt;&lt; 'EOF'\nimport duckdb\nconn = duckdb.connect('data/lab2_market_sentiment.duckdb')\ndupes = conn.execute(\"\"\"\n    SELECT sentiment_event_id, COUNT(*) as cnt\n    FROM fct_sentiment_events\n    GROUP BY sentiment_event_id\n    HAVING COUNT(*) &gt; 1\n\"\"\").df()\nprint(dupes)\nEOF\n\n# 3. Fix surrogate key generation\n# Add more fields to make unique: row_num, ingested_at, etc.\n\n# 4. Full refresh to clear duplicates\ndbt run --select fct_sentiment_events --profiles-dir . --full-refresh\n</code></pre>"},{"location":"labs/lab2-troubleshooting/#dbt-package-not-found","title":"dbt: Package Not Found","text":"<p>Symptoms: <pre><code>Package dbt_expectations not found\n</code></pre></p> <p>Causes: - <code>dbt_packages/</code> directory missing - Package not installed</p> <p>Solutions:</p> <pre><code>cd dbt\n\n# 1. Install packages\ndbt deps\n\n# 2. Verify packages installed\nls dbt_packages/\n\n# Should see:\n# - dbt_expectations/\n# - dbt_utils/\n# etc.\n\n# 3. If deps fails, check packages.yml\ncat packages.yml\n\n# 4. Try clean install\nrm -rf dbt_packages/\ndbt deps\n</code></pre>"},{"location":"labs/lab2-troubleshooting/#dbt-incremental-model-not-updating","title":"dbt: Incremental Model Not Updating","text":"<p>Symptoms: - New data ingested but mart tables don't update</p> <p>Causes: - Incremental logic filtering out new data - Missing <code>--full-refresh</code> after schema change</p> <p>Solutions:</p> <pre><code># 1. Check incremental logic\n# Look for WHERE clause in model\n\n# 2. Force full refresh\ndbt run --select model_name --profiles-dir . --full-refresh\n\n# 3. Check max date in table\npython &lt;&lt; 'EOF'\nimport duckdb\nconn = duckdb.connect('data/lab2_market_sentiment.duckdb')\nmax_date = conn.execute(\"SELECT MAX(published_at) FROM fct_sentiment_events\").fetchone()[0]\nprint(f\"Max date in table: {max_date}\")\nEOF\n\n# 4. Verify new data exists in raw files\nhead -n 5 data/raw/reddit_brands.csv\n</code></pre>"},{"location":"labs/lab2-troubleshooting/#dashboard-issues","title":"Dashboard Issues","text":""},{"location":"labs/lab2-troubleshooting/#streamlit-port-already-in-use","title":"Streamlit: Port Already in Use","text":"<p>Symptoms: <pre><code>OSError: [Errno 48] Address already in use\n</code></pre></p> <p>Solutions:</p> <pre><code># Option 1: Use different port\nstreamlit run app/streamlit_app.py --server.port 8502\n\n# Option 2: Kill process using port 8501\nlsof -ti:8501 | xargs kill -9\n\n# Option 3: Find and kill specific process\nps aux | grep streamlit\nkill -9 &lt;PID&gt;\n</code></pre>"},{"location":"labs/lab2-troubleshooting/#streamlit-no-data-showing","title":"Streamlit: No Data Showing","text":"<p>Symptoms: - Dashboard loads but shows \"No data found\" or empty charts</p> <p>Causes: - Database empty or missing tables - Database path incorrect - Caching issue</p> <p>Solutions:</p> <pre><code># 1. Verify database path in streamlit_app.py\nDB_PATH = Path(__file__).parent.parent / \"data\" / \"lab2_market_sentiment.duckdb\"\nprint(f\"Looking for DB at: {DB_PATH}\")\nprint(f\"DB exists: {DB_PATH.exists()}\")\n\n# 2. Check table exists\nimport duckdb\nconn = duckdb.connect(str(DB_PATH))\ntables = conn.execute(\"SHOW TABLES\").fetchall()\nprint(\"Tables:\", tables)\n\n# 3. Clear Streamlit cache\n# In browser: C (clear cache)\n# Or add to code:\nst.cache_data.clear()\n\n# 4. Verify data in database\ncount = conn.execute(\"SELECT COUNT(*) FROM fct_sentiment_events\").fetchone()[0]\nprint(f\"Events count: {count}\")\n</code></pre>"},{"location":"labs/lab2-troubleshooting/#streamlit-slow-performance","title":"Streamlit: Slow Performance","text":"<p>Symptoms: - Dashboard takes 10+ seconds to load - Filters lag when updating</p> <p>Solutions:</p> <pre><code># 1. Increase cache TTL\n@st.cache_data(ttl=600)  # 10 minutes instead of 5\ndef load_data():\n    # ...\n\n# 2. Limit data loaded\ndef load_data():\n    # Only load last 90 days\n    query = \"\"\"\n        SELECT * FROM fct_sentiment_events\n        WHERE published_at &gt;= CURRENT_DATE - INTERVAL '90 days'\n    \"\"\"\n    return conn.execute(query).df()\n\n# 3. Use aggregated data for charts\ndef load_daily_data():\n    # Use mart_daily_sentiment instead of fact table\n    return conn.execute(\"SELECT * FROM mart_daily_sentiment\").df()\n\n# 4. Add pagination to tables\ndf_display = df.head(100)  # Only show first 100 rows\n</code></pre>"},{"location":"labs/lab2-troubleshooting/#streamlit-module-not-found","title":"Streamlit: Module Not Found","text":"<p>Symptoms: <pre><code>ModuleNotFoundError: No module named 'plotly'\n</code></pre></p> <p>Solutions:</p> <pre><code># 1. Verify virtual environment is activated\nwhich python\n# Should show venv/bin/python\n\n# 2. Install missing module\npip install plotly\n\n# 3. Or reinstall all requirements\npip install -r requirements.txt --upgrade\n\n# 4. Check installed packages\npip list | grep plotly\n</code></pre>"},{"location":"labs/lab2-troubleshooting/#github-actions-issues","title":"GitHub Actions Issues","text":""},{"location":"labs/lab2-troubleshooting/#workflow-failing-on-dependency-installation","title":"Workflow: Failing on Dependency Installation","text":"<p>Symptoms: <pre><code>ERROR: Could not find a version that satisfies the requirement X\n</code></pre></p> <p>Solutions:</p> <pre><code># 1. Update actions versions in workflow\n- uses: actions/checkout@v4  # Not v3\n- uses: actions/setup-python@v5  # Not v4\n\n# 2. Pin Python version\npython-version: '3.11'  # Explicit version\n\n# 3. Upgrade pip before installing\n- name: Install dependencies\n  run: |\n    python -m pip install --upgrade pip\n    pip install -r requirements.txt\n\n# 4. Add system dependencies if needed (for ARM/M1)\n- name: Install dependencies\n  run: |\n    brew install gcc  # macOS only\n    pip install -r requirements.txt\n</code></pre>"},{"location":"labs/lab2-troubleshooting/#workflow-secrets-not-found","title":"Workflow: Secrets Not Found","text":"<p>Symptoms: <pre><code>Error: REDDIT_CLIENT_ID not set\n</code></pre></p> <p>Solutions:</p> <ol> <li>Add secrets to repository:</li> <li>Go to Settings \u2192 Secrets and variables \u2192 Actions</li> <li>Click \"New repository secret\"</li> <li> <p>Add each secret</p> </li> <li> <p>Verify secret names match:    <pre><code>env:\n  REDDIT_CLIENT_ID: ${{ secrets.REDDIT_CLIENT_ID }}\n  # Make sure secret name matches exactly\n</code></pre></p> </li> <li> <p>Check secret values:</p> </li> <li>No quotes around values in GitHub UI</li> <li>No trailing spaces</li> <li>Copy-paste carefully</li> </ol>"},{"location":"labs/lab2-troubleshooting/#workflow-dbt-build-fails","title":"Workflow: dbt Build Fails","text":"<p>Symptoms: <pre><code>dbt: command not found\n</code></pre> OR <pre><code>Could not find profile named 'lab2_market_sentiment'\n</code></pre></p> <p>Solutions:</p> <pre><code># 1. Ensure dbt is in requirements.txt\n# Already should be:\n# dbt-core==1.7.0\n# dbt-duckdb==1.7.0\n\n# 2. Fix dbt command\nrun: |\n  cd dbt\n  dbt deps  # Install packages first\n  dbt build --profiles-dir .\n\n# 3. Verify profiles.yml exists\n# Should be in dbt/ directory\n\n# 4. Use absolute path if needed\nrun: |\n  cd dbt\n  python -m dbt build --profiles-dir .\n</code></pre>"},{"location":"labs/lab2-troubleshooting/#workflow-timeout","title":"Workflow: Timeout","text":"<p>Symptoms: <pre><code>The job was canceled because it exceeded the maximum execution time\n</code></pre></p> <p>Solutions:</p> <pre><code># 1. Increase job timeout (default is 360 minutes)\njobs:\n  run-pipeline:\n    runs-on: ubuntu-latest\n    timeout-minutes: 60  # 1 hour\n\n# 2. Reduce data ingested\n# In ingest_real_data.py:\nlimit_per_brand = 10  # Instead of 20\n\n# 3. Add timeout to individual steps\n- name: Run pipeline\n  timeout-minutes: 30\n  run: python orchestrate_weekly.py\n</code></pre>"},{"location":"labs/lab2-troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"labs/lab2-troubleshooting/#ingestion-taking-too-long","title":"Ingestion: Taking Too Long","text":"<p>Symptoms: - Ingestion script runs for 20+ minutes</p> <p>Solutions:</p> <pre><code># 1. Reduce posts per brand\nlimit_per_brand = 10  # Default was 20\n\n# 2. Use concurrent requests (careful with rate limits)\nfrom concurrent.futures import ThreadPoolExecutor\n\ndef fetch_brand(brand):\n    # ... fetch logic ...\n\nwith ThreadPoolExecutor(max_workers=3) as executor:\n    results = list(executor.map(fetch_brand, BRANDS))\n\n# 3. Skip already-ingested data\nexisting_ids = set(pd.read_csv('data/raw/reddit_brands.csv')['post_id'])\nif post.id not in existing_ids:\n    # Process...\n\n# 4. Limit time filter\nsubreddit.search(brand, limit=20, time_filter='week')  # Not 'month'\n</code></pre>"},{"location":"labs/lab2-troubleshooting/#dbt-slow-build-times","title":"dbt: Slow Build Times","text":"<p>Symptoms: - <code>dbt build</code> takes 5+ minutes</p> <p>Solutions:</p> <pre><code># 1. Run only changed models\ndbt build --select state:modified+ --profiles-dir .\n\n# 2. Use threads\ndbt build --profiles-dir . --threads 4\n\n# 3. Skip tests during development\ndbt run --profiles-dir .  # No tests\n\n# 4. Profile slow models\ndbt run --select model_name --profiles-dir . --debug\n\n# Look for slow SQL operations in logs\n</code></pre>"},{"location":"labs/lab2-troubleshooting/#query-slow-dashboard-queries","title":"Query: Slow Dashboard Queries","text":"<p>Symptoms: - Streamlit charts take 5+ seconds to render</p> <p>Solutions:</p> <pre><code># 1. Add indexes (DuckDB automatically optimizes, but explicit helps)\nconn.execute(\"CREATE INDEX IF NOT EXISTS idx_brand ON fct_sentiment_events(brand)\")\nconn.execute(\"CREATE INDEX IF NOT EXISTS idx_date ON fct_sentiment_events(published_at)\")\n\n# 2. Use aggregated marts\n# Instead of querying fact table, use mart_daily_sentiment\n\n# 3. Filter data early\nquery = \"\"\"\n    SELECT * FROM fct_sentiment_events\n    WHERE published_at &gt;= CURRENT_DATE - INTERVAL '30 days'\n      AND brand IN ('Coca-Cola', 'PepsiCo')\n\"\"\"\n\n# 4. Use columnar selection\nquery = \"\"\"\n    SELECT brand, sentiment_score, published_at  -- Only needed columns\n    FROM fct_sentiment_events\n    WHERE ...\n\"\"\"\n</code></pre>"},{"location":"labs/lab2-troubleshooting/#data-quality-issues","title":"Data Quality Issues","text":""},{"location":"labs/lab2-troubleshooting/#issue-sentiment-scores-out-of-range","title":"Issue: Sentiment Scores Out of Range","text":"<p>Symptoms: - Test failure: <code>sentiment_score</code> not between -1 and 1</p> <p>Solutions:</p> <pre><code># 1. Check VADER output\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nanalyzer = SentimentIntensityAnalyzer()\nscores = analyzer.polarity_scores(\"This is terrible!\")\nprint(scores)  # compound should be -1 to 1\n\n# 2. Add normalization in staging\nROUND(\n    CASE \n        WHEN sentiment_score &gt; 1 THEN 1.0\n        WHEN sentiment_score &lt; -1 THEN -1.0\n        ELSE sentiment_score\n    END,\n    3\n) as sentiment_score\n\n# 3. Add validation in Python\ndef normalize_sentiment(score):\n    return max(-1.0, min(1.0, score))\n</code></pre>"},{"location":"labs/lab2-troubleshooting/#issue-missing-brand-data","title":"Issue: Missing Brand Data","text":"<p>Symptoms: - Some brands have 0 mentions - Competitive analysis incomplete</p> <p>Solutions:</p> <pre><code># 1. Check if brand in taxonomy\nwith open('shared/config/brand_taxonomy.yaml') as f:\n    taxonomy = yaml.safe_load(f)\n# Verify brand listed\n\n# 2. Check if brand detection working\ntext = \"I love Coca-Cola\"\nmentioned = detect_brand_mentions(text, brands_list)\nprint(f\"Detected: {mentioned}\")\n\n# 3. Add brand aliases\n# In taxonomy:\nbrands:\n  - name: \"Coca-Cola\"\n    aliases: [\"Coke\", \"Coca Cola\", \"CocaCola\"]\n\n# 4. Check API search results\n# Reddit: Is brand term too specific?\n# News: Adjust search query\n</code></pre>"},{"location":"labs/lab2-troubleshooting/#issue-duplicate-sentiment-events","title":"Issue: Duplicate Sentiment Events","text":"<p>Symptoms: - Test failure: Multiple rows with same <code>sentiment_event_id</code></p> <p>Solutions:</p> <pre><code>-- 1. Identify duplicates\nSELECT sentiment_event_id, COUNT(*) as cnt\nFROM fct_sentiment_events\nGROUP BY sentiment_event_id\nHAVING COUNT(*) &gt; 1;\n\n-- 2. Enhance surrogate key uniqueness\n{{ generate_surrogate_key([\n    'content_id', \n    'published_at', \n    'row_num',  -- Add this\n    'ingested_at'  -- And this if needed\n]) }}\n\n-- 3. Add deduplication CTE\nWITH deduped AS (\n    SELECT *,\n        ROW_NUMBER() OVER (\n            PARTITION BY sentiment_event_id\n            ORDER BY ingested_at DESC\n        ) as rn\n    FROM base_data\n)\nSELECT * FROM deduped WHERE rn = 1\n</code></pre>"},{"location":"labs/lab2-troubleshooting/#issue-anomalies-not-detected","title":"Issue: Anomalies Not Detected","text":"<p>Symptoms: - No anomalies flagged despite obvious sentiment spikes</p> <p>Solutions:</p> <pre><code>-- 1. Check z-score calculation\nSELECT \n    sentiment_date,\n    brand,\n    avg_sentiment,\n    AVG(avg_sentiment) OVER (PARTITION BY brand) as brand_avg,\n    STDDEV(avg_sentiment) OVER (PARTITION BY brand) as brand_stddev,\n    z_score_sentiment,\n    anomaly_flag\nFROM mart_daily_sentiment\nWHERE brand = 'Coca-Cola'\nORDER BY sentiment_date DESC;\n\n-- 2. Adjust threshold\n-- In mart_daily_sentiment.sql:\nCASE \n    WHEN ABS(z_score) &gt; 1.5  -- Less strict (was 2)\n    THEN 'ANOMALY'\n    ELSE 'NORMAL'\nEND\n\n-- 3. Ensure enough historical data\n-- Need 30+ days for meaningful stddev\n</code></pre>"},{"location":"labs/lab2-troubleshooting/#getting-more-help","title":"Getting More Help","text":"<p>If issues persist:</p> <ol> <li> <p>Check Logs:    <pre><code># Ingestion logs\npython pipelines/ingest_real_data.py 2&gt;&amp;1 | tee ingestion.log\n\n# dbt logs\ncat dbt/logs/dbt.log\n\n# Streamlit logs\n# Check terminal where streamlit is running\n</code></pre></p> </li> <li> <p>Enable Debug Mode:    <pre><code># dbt debug mode\ndbt run --select model_name --profiles-dir . --debug\n\n# Python debug logging\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre></p> </li> <li> <p>Contact Support:</p> </li> <li>GitHub Issues: [repository]/issues</li> <li>Email: narensham@example.com</li> <li>Include: error message, logs, steps to reproduce</li> </ol> <p>Last Updated: November 2025 Maintainer: narensham</p>"},{"location":"phases/phase1-foundation/","title":"Phase 1: Foundation","text":"<p>Establish shared utilities and central configuration that all labs use.</p>"},{"location":"phases/phase1-foundation/#what-phase-1-delivers","title":"What Phase 1 Delivers","text":"<p>\u2705 Shared Utilities - DataInspector, CSVMonitor, Config Loaders \u2705 Central Configuration - Lab registry, path management \u2705 Lab1 Scripts - Data inspection and monitoring \u2705 Clean Git History - No build artifacts or venv  </p>"},{"location":"phases/phase1-foundation/#what-gets-created","title":"What Gets Created","text":"<pre><code>buildcpg-labs/\n\u251c\u2500\u2500 shared/                          \u2190 NEW\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 utils/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 data_inspector.py       # Database inspection\n\u2502   \u2502   \u251c\u2500\u2500 csv_monitor.py          # Detect new data\n\u2502   \u2502   \u2514\u2500\u2500 config_loader.py        # Load configurations\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 data_quality/\n\u2502   \u2502   \u251c\u2500\u2500 validators.py           # Validation rules\n\u2502   \u2502   \u2514\u2500\u2500 expectations.py         # Data expectations\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 templates/                  \u2190 For Phase 2\n\u2502       \u2514\u2500\u2500 (templates will go here)\n\u2502\n\u251c\u2500\u2500 config/                          \u2190 NEW\n\u2502   \u251c\u2500\u2500 labs_config.yaml            # Lab registry\n\u2502   \u2514\u2500\u2500 paths.py                    # Path helpers\n\u2502\n\u2514\u2500\u2500 lab1_sales_performance/         \u2190 UPDATED\n    \u251c\u2500\u2500 scripts/\n    \u2502   \u251c\u2500\u2500 inspect_data.py         # NEW - uses shared utilities\n    \u2502   \u2514\u2500\u2500 check_for_new_data.py   # NEW - uses shared utilities\n    \u2514\u2500\u2500 .gitignore                  # NEW - ignore venv, artifacts\n</code></pre>"},{"location":"phases/phase1-foundation/#step-by-step-setup","title":"Step-by-Step Setup","text":""},{"location":"phases/phase1-foundation/#step-1-create-directory-structure","title":"Step 1: Create Directory Structure","text":"<pre><code>cd buildcpg-labs\nmkdir -p shared/utils\nmkdir -p shared/data_quality\nmkdir -p shared/templates\nmkdir -p config\n\n# Create __init__.py files\ntouch shared/__init__.py\ntouch shared/utils/__init__.py\ntouch shared/data_quality/__init__.py\n</code></pre>"},{"location":"phases/phase1-foundation/#step-2-create-shared-utilities","title":"Step 2: Create Shared Utilities","text":"<p>Create these files in <code>shared/utils/</code>:</p> <ul> <li><code>data_inspector.py</code> - Inspect databases</li> <li><code>csv_monitor.py</code> - Monitor CSV changes</li> <li><code>config_loader.py</code> - Load lab configurations</li> </ul> <p>See documentation links below.</p>"},{"location":"phases/phase1-foundation/#step-3-create-configuration","title":"Step 3: Create Configuration","text":"<p>Create these files in <code>config/</code>:</p> <ul> <li><code>labs_config.yaml</code> - Central lab registry</li> <li><code>paths.py</code> - Path helpers for accessing labs</li> </ul>"},{"location":"phases/phase1-foundation/#step-4-create-lab1-scripts","title":"Step 4: Create Lab1 Scripts","text":"<p>Create these files in <code>lab1_sales_performance/scripts/</code>:</p> <ul> <li><code>inspect_data.py</code> - Inspect lab1 database using shared DataInspector</li> <li><code>check_for_new_data.py</code> - Monitor lab1 CSV using shared CSVMonitor</li> </ul>"},{"location":"phases/phase1-foundation/#step-5-add-git-ignore","title":"Step 5: Add Git Ignore","text":"<p>Create <code>.gitignore</code> at root and <code>lab1_sales_performance/.gitignore</code></p>"},{"location":"phases/phase1-foundation/#step-6-test-everything-works","title":"Step 6: Test Everything Works","text":"<pre><code># Test shared utilities\npython shared/utils/data_inspector.py\npython shared/utils/csv_monitor.py\n\n# Test config\npython config/paths.py\n\n# Test lab1 scripts\ncd lab1_sales_performance\npython scripts/inspect_data.py\npython scripts/check_for_new_data.py\n</code></pre>"},{"location":"phases/phase1-foundation/#step-7-commit-to-git","title":"Step 7: Commit to Git","text":"<pre><code>git add shared/\ngit add config/\ngit add lab1_sales_performance/scripts/\ngit add .gitignore\ngit add lab1_sales_performance/.gitignore\n\ngit commit -m \"Phase 1: Create shared utilities and foundational structure\"\ngit push origin main\n</code></pre>"},{"location":"phases/phase1-foundation/#how-it-works","title":"How It Works","text":""},{"location":"phases/phase1-foundation/#shared-utilities","title":"Shared Utilities","text":"<p>Written once in <code>shared/</code>, used by all labs:</p> <pre><code># In lab1_sales_performance/scripts/inspect_data.py\nimport sys\nsys.path.insert(0, '../..')  # Go to root\nfrom shared.utils.data_inspector import DataInspector\n\ninspector = DataInspector('data/lab1_sales_performance.duckdb')\nquality = inspector.get_quality_score('gold', 'gold_orders_summary')\n</code></pre> <pre><code># In lab2_forecast_model/scripts/inspect_data.py (FUTURE)\nimport sys\nsys.path.insert(0, '../..')  # Go to root\nfrom shared.utils.data_inspector import DataInspector\n\ninspector = DataInspector('data/lab2_forecast_model.duckdb')\nquality = inspector.get_quality_score('gold', 'forecast_summary')\n# Same code, different database\n</code></pre>"},{"location":"phases/phase1-foundation/#central-configuration","title":"Central Configuration","text":"<p>All labs registered in one place:</p> <pre><code># config/labs_config.yaml\nlabs:\n  lab1_sales_performance:\n    path: lab1_sales_performance\n    db_path: lab1_sales_performance/data/lab1_sales_performance.duckdb\n    dbt_path: lab1_sales_performance/dbt\n\n  lab2_forecast_model:  # When created\n    path: lab2_forecast_model\n    db_path: lab2_forecast_model/data/lab2_forecast_model.duckdb\n    dbt_path: lab2_forecast_model/dbt\n</code></pre>"},{"location":"phases/phase1-foundation/#verification-checklist","title":"Verification Checklist","text":"<p>After Phase 1 completion, verify:</p> <ul> <li> <code>shared/</code> directory exists with utilities</li> <li> <code>config/</code> directory exists with labs_config.yaml and paths.py</li> <li> <code>lab1_sales_performance/scripts/inspect_data.py</code> works</li> <li> <code>lab1_sales_performance/scripts/check_for_new_data.py</code> works</li> <li> <code>python config/paths.py</code> returns \u2705</li> <li> <code>git status</code> shows clean working tree</li> <li> Changes committed to git</li> </ul>"},{"location":"phases/phase1-foundation/#whats-not-included-yet","title":"What's NOT Included Yet","text":"<p>Phase 1 focuses on foundation. These come later:</p> <ul> <li>\u274c Makefile (Phase 2)</li> <li>\u274c Bootstrap script (Phase 2)</li> <li>\u274c Orchestration/Airflow (Phase 3)</li> <li>\u274c Advanced monitoring (Phase 3+)</li> </ul>"},{"location":"phases/phase1-foundation/#troubleshooting-phase-1","title":"Troubleshooting Phase 1","text":""},{"location":"phases/phase1-foundation/#import-error-no-module-named-shared","title":"Import Error: \"No module named 'shared'\"","text":"<p>Make sure you're running scripts from the right directory: <pre><code>cd lab1_sales_performance\npython scripts/inspect_data.py  # Works\npython -c \"import scripts.inspect_data\"  # Also works\n</code></pre></p>"},{"location":"phases/phase1-foundation/#file-not-found-labs_configyaml","title":"File Not Found: \"labs_config.yaml\"","text":"<p>Verify the path exists: <pre><code>ls -la config/labs_config.yaml\n</code></pre></p>"},{"location":"phases/phase1-foundation/#datainspector-returns-empty-schemas","title":"DataInspector returns empty schemas","text":"<p>Database might not have tables yet. Create some with dbt: <pre><code>cd lab1_sales_performance/dbt\ndbt run\n</code></pre></p> <p>See Troubleshooting for more help.</p>"},{"location":"phases/phase1-foundation/#next-phase","title":"Next Phase","text":"<p>Phase 2 adds automation: - Makefile for standardized commands - Bootstrap script for creating labs - Testing framework</p> <p>See Phase 2: Automation</p>"}]}