{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"BuildCPG Labs","text":"<p>A modern, scalable data engineering platform for managing multiple independent labs using dbt, DuckDB, and Python.</p>"},{"location":"#overview","title":"Overview","text":"<p>BuildCPG Labs enables you to:</p> <ul> <li>Run independent labs - Each lab has its own database, data, and Python environment</li> <li>Share utilities - Common code used by all labs without duplication</li> <li>Scale easily - Create new labs in minutes using templates</li> <li>Maintain quality - Built-in data inspection and automated quality checks</li> <li>Work efficiently - Per-lab virtual environments with standardized workflows</li> </ul>"},{"location":"#architecture","title":"Architecture","text":"<pre><code>graph TB\n    subgraph \"BuildCPG Labs Platform\"\n        S[shared/&lt;br/&gt;Utilities &amp; Templates]\n\n        subgraph \"Lab 1 - Sales Performance\"\n            L1[dbt Models]\n            L1D[DuckDB Database]\n            L1E[lab1_env]\n        end\n\n        subgraph \"Lab 2 - Market Sentiment\"\n            L2[dbt Models]\n            L2D[DuckDB Database]\n            L2E[lab2_env]\n        end\n\n        subgraph \"Lab 3 - Customer Segmentation\"\n            L3[dbt Models]\n            L3D[DuckDB Database]\n            L3E[lab3_env]\n        end\n    end\n\n    S -.-&gt;|Shared Utils| L1\n    S -.-&gt;|Shared Utils| L2\n    S -.-&gt;|Shared Utils| L3\n\n    style S fill:#e0e7ff\n    style L1 fill:#dbeafe\n    style L2 fill:#dcfce7\n    style L3 fill:#fef3c7</code></pre>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#multi-lab-architecture-with-per-lab-environments","title":"Multi-Lab Architecture with Per-Lab Environments","text":"<p>Each lab is completely independent:</p> <pre><code>buildcpg-labs/\n\u251c\u2500\u2500 shared/                     # Reusable utilities (ALL labs)\n\u2502   \u251c\u2500\u2500 utils/                  # DataInspector, CSVMonitor\n\u2502   \u2514\u2500\u2500 config/                 # Central configuration\n\u2502\n\u251c\u2500\u2500 lab1_sales_performance/     # Lab 1 (Independent)\n\u2502   \u251c\u2500\u2500 lab1_env/              # Own virtual environment\n\u2502   \u251c\u2500\u2500 data/                  # Own database\n\u2502   \u2514\u2500\u2500 dbt/                   # Own models\n\u2502\n\u251c\u2500\u2500 lab2_market_sentiment/      # Lab 2 (Independent)\n\u2502   \u251c\u2500\u2500 lab2_env/              # Own virtual environment\n\u2502   \u251c\u2500\u2500 data/                  # Own database\n\u2502   \u2514\u2500\u2500 dbt/                   # Own models\n\u2502\n\u2514\u2500\u2500 lab3_customer_segmentation/ # Lab 3 (Independent)\n    \u251c\u2500\u2500 lab3_env/              # Own virtual environment\n    \u251c\u2500\u2500 data/                  # Own database\n    \u2514\u2500\u2500 dbt/                   # Own models\n</code></pre> <p>Architecture Benefits: - \u2705 Complete isolation - Each lab has dedicated environment - \u2705 Independent data - Each lab has its own DuckDB database - \u2705 Dependency freedom - Labs can use different package versions - \u2705 No conflicts - Work on multiple labs simultaneously - \u2705 Shared utilities - Common code available to all labs</p>"},{"location":"#current-labs","title":"Current Labs","text":""},{"location":"#lab-1-sales-performance-analysis","title":"Lab 1: Sales Performance Analysis","text":"<p>Status: \u2705 Active Purpose: Analyze sales data with medallion architecture (Bronze \u2192 Silver \u2192 Gold) Features: - Sales data processing - Performance metrics - Trend analysis</p> <p>Documentation: See Lab 1 Overview</p>"},{"location":"#lab-2-market-sentiment-analysis","title":"Lab 2: Market Sentiment Analysis","text":"<p>Status: \u2705 Active Development Purpose: Real-time CPG brand reputation monitoring Features: - Reddit &amp; News sentiment ingestion - 5 dbt models with incremental processing - 14 automated data quality tests - Anomaly detection via z-scores - Daily sentiment aggregations</p> <p>Key Metrics: - \ud83d\udcca 800 sentiment events - \ud83e\uddea 14/14 tests passing - \ud83c\udfaf 5 CPG brands monitored - \u26a1 ~3 second build time</p> <p>Technology Stack: <pre><code>graph LR\n    A[Python 3.11] --&gt; B[dbt 1.7.0]\n    B --&gt; C[DuckDB 0.9.1]\n    C --&gt; D[Data Quality Tests]\n\n    style A fill:#3b82f6\n    style B fill:#10b981\n    style C fill:#f59e0b\n    style D fill:#8b5cf6</code></pre></p> <p>Documentation:  - Lab 2 Overview - Architecture &amp; features - Lab 2 Setup - Installation guide - Lab 2 Data Models - Complete model reference - Lab 2 Troubleshooting - Issue solutions - Lab 2 Quick Reference - Common commands</p>"},{"location":"#lab-3-customer-segmentation","title":"Lab 3: Customer Segmentation","text":"<p>Status: \ud83d\udccb Planned Purpose: Customer behavior analysis and segmentation Features: (Coming soon)</p>"},{"location":"#platform-statistics","title":"Platform Statistics","text":"Metric Value Active Labs 2 Total Data Models 10+ Data Quality Tests 20+ Shared Utilities 5 Documentation Pages 15+"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#1-clone-repository","title":"1. Clone Repository","text":"<pre><code>git clone https://github.com/narensham/buildcpg-labs.git\ncd buildcpg-labs\n</code></pre>"},{"location":"#2-work-with-lab-2-example","title":"2. Work with Lab 2 (Example)","text":"<pre><code># Navigate to lab\ncd lab2_market_sentiment\n\n# Create virtual environment\npython3 -m venv lab2_env\nsource lab2_env/bin/activate\n\n# Install dependencies\npip install -r requirements.txt\n\n# Setup dbt\ncd dbt\ndbt deps\ndbt debug\n\n# Generate sample data\ncd ..\npython pipelines/ingest_sentiment.py\n\n# Run pipeline\ncd dbt\ndbt build\n</code></pre>"},{"location":"#3-verify-success","title":"3. Verify Success","text":"<pre><code># All tests should pass\ndbt test\n\n# Expected output:\n# \u2705 PASS=14 WARN=0 ERROR=0 SKIP=0 TOTAL=14\n</code></pre>"},{"location":"#platform-structure","title":"Platform Structure","text":"<pre><code>buildcpg-labs/\n\u2502\n\u251c\u2500\u2500 shared/                          # Shared utilities\n\u2502   \u251c\u2500\u2500 utils/\n\u2502   \u2502   \u251c\u2500\u2500 data_inspector.py       # Database inspection\n\u2502   \u2502   \u251c\u2500\u2500 csv_monitor.py          # Data change detection\n\u2502   \u2502   \u2514\u2500\u2500 config_loader.py        # Configuration management\n\u2502   \u251c\u2500\u2500 config/\n\u2502   \u2502   \u251c\u2500\u2500 labs_config.yaml        # Lab registry\n\u2502   \u2502   \u2514\u2500\u2500 paths.py                # Path helpers\n\u2502   \u2514\u2500\u2500 templates/                  # Lab templates\n\u2502\n\u251c\u2500\u2500 lab1_sales_performance/         # Independent Lab 1\n\u2502   \u251c\u2500\u2500 lab1_env/                   # Virtual environment\n\u2502   \u251c\u2500\u2500 data/\n\u2502   \u2502   \u251c\u2500\u2500 raw/                    # Source data\n\u2502   \u2502   \u2514\u2500\u2500 lab1.duckdb             # Database\n\u2502   \u251c\u2500\u2500 dbt/\n\u2502   \u2502   \u251c\u2500\u2500 models/                 # Transformations\n\u2502   \u2502   \u251c\u2500\u2500 tests/                  # Quality tests\n\u2502   \u2502   \u2514\u2500\u2500 profiles.yml            # DB connection\n\u2502   \u251c\u2500\u2500 pipelines/                  # Data ingestion\n\u2502   \u2514\u2500\u2500 requirements.txt            # Dependencies\n\u2502\n\u251c\u2500\u2500 lab2_market_sentiment/          # Independent Lab 2\n\u2502   \u251c\u2500\u2500 lab2_env/                   # Virtual environment\n\u2502   \u251c\u2500\u2500 data/\n\u2502   \u2502   \u251c\u2500\u2500 raw/                    # Source data\n\u2502   \u2502   \u2514\u2500\u2500 lab2.duckdb             # Database\n\u2502   \u251c\u2500\u2500 dbt/\n\u2502   \u2502   \u251c\u2500\u2500 models/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 staging/           # Bronze layer\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 intermediate/      # Silver layer\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 mart/              # Gold layer\n\u2502   \u2502   \u251c\u2500\u2500 macros/                # Reusable SQL\n\u2502   \u2502   \u251c\u2500\u2500 tests/                 # Quality tests\n\u2502   \u2502   \u2514\u2500\u2500 schema.yml             # Contracts\n\u2502   \u251c\u2500\u2500 pipelines/\n\u2502   \u2502   \u2514\u2500\u2500 ingest_sentiment.py   # Data generation\n\u2502   \u2514\u2500\u2500 requirements.txt           # Dependencies\n\u2502\n\u251c\u2500\u2500 docs/                           # Documentation (MkDocs)\n\u2502   \u251c\u2500\u2500 architecture/\n\u2502   \u251c\u2500\u2500 getting-started/\n\u2502   \u251c\u2500\u2500 labs/\n\u2502   \u2502   \u251c\u2500\u2500 lab1-*.md\n\u2502   \u2502   \u2514\u2500\u2500 lab2-*.md              # Lab 2 documentation\n\u2502   \u2514\u2500\u2500 utilities/\n\u2502\n\u251c\u2500\u2500 .github/\n\u2502   \u2514\u2500\u2500 workflows/\n\u2502       \u2514\u2500\u2500 docs.yml               # Auto-deploy docs\n\u2502\n\u251c\u2500\u2500 mkdocs.yml                      # Documentation config\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"#technology-stack","title":"Technology Stack","text":"Layer Technology Purpose Database DuckDB 0.9.1 Embedded, Mac-compatible, no Docker Transformation dbt 1.7.0 Data modeling &amp; testing Language Python 3.11+ Scripting &amp; ingestion Testing dbt_expectations Data quality validation Documentation MkDocs Material Auto-generated docs CI/CD GitHub Actions Automated deployments Orchestration Manual (Airflow planned) Pipeline scheduling"},{"location":"#design-principles","title":"Design Principles","text":""},{"location":"#1-lab-independence","title":"1. Lab Independence","text":"<p><pre><code>graph LR\n    L1[Lab 1] -.-&gt;|Uses| S[Shared Utils]\n    L2[Lab 2] -.-&gt;|Uses| S\n    L3[Lab 3] -.-&gt;|Uses| S\n\n    L1 x--x L2\n    L2 x--x L3\n    L3 x--x L1\n\n    style S fill:#e0e7ff\n    style L1 fill:#dbeafe\n    style L2 fill:#dcfce7\n    style L3 fill:#fef3c7</code></pre> - Each lab has own database - Each lab has own environment - Labs don't interfere with each other</p>"},{"location":"#2-medallion-architecture","title":"2. Medallion Architecture","text":"<p>All labs follow Bronze \u2192 Silver \u2192 Gold pattern: - Bronze (Staging): Raw data, minimal transformation - Silver (Intermediate): Cleaned, enriched, business logic - Gold (Marts): Analytics-ready aggregates</p>"},{"location":"#3-data-quality-first","title":"3. Data Quality First","text":"<ul> <li>Automated tests on every run</li> <li>Contract enforcement where needed</li> <li>Quality flags and validation</li> <li>Comprehensive test coverage</li> </ul>"},{"location":"#4-documentation-driven","title":"4. Documentation Driven","text":"<ul> <li>Every lab fully documented</li> <li>Architecture diagrams</li> <li>Troubleshooting guides</li> <li>Quick reference cards</li> <li>Auto-deployed to GitHub Pages</li> </ul>"},{"location":"#workflow-example-lab-2","title":"Workflow Example (Lab 2)","text":"<pre><code>sequenceDiagram\n    participant D as Developer\n    participant P as Python Script\n    participant CSV as CSV Files\n    participant DBT as dbt\n    participant DB as DuckDB\n    participant T as Tests\n\n    D-&gt;&gt;P: python ingest_sentiment.py\n    P-&gt;&gt;CSV: Generate data\n    D-&gt;&gt;DBT: dbt run\n    DBT-&gt;&gt;CSV: Read raw data\n    DBT-&gt;&gt;DB: Transform (5 models)\n    D-&gt;&gt;DBT: dbt test\n    DBT-&gt;&gt;T: Run 14 tests\n    T--&gt;&gt;D: \u2705 All pass</code></pre>"},{"location":"#common-tasks","title":"Common Tasks","text":""},{"location":"#check-lab-status","title":"Check Lab Status","text":"<pre><code># From buildcpg-labs root\nls -la | grep lab\n\n# Lab specific status\ncd lab2_market_sentiment\ndbt debug\ndbt list\n</code></pre>"},{"location":"#run-specific-lab","title":"Run Specific Lab","text":"<pre><code>cd lab2_market_sentiment\nsource lab2_env/bin/activate\ncd dbt\ndbt run\ndbt test\n</code></pre>"},{"location":"#view-documentation","title":"View Documentation","text":"<pre><code># Serve locally\nmkdocs serve\n\n# Visit: http://127.0.0.1:8000\n</code></pre>"},{"location":"#create-new-lab-future","title":"Create New Lab (Future)","text":"<pre><code>./setup_new_lab.sh lab3_customer_segmentation\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":""},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Quick Start Guide</li> <li>Installation Guide</li> </ul>"},{"location":"#architecture_1","title":"Architecture","text":"<ul> <li>System Overview</li> <li>Multi-Lab Design</li> <li>Medallion Architecture</li> <li>Shared vs Lab-Specific</li> </ul>"},{"location":"#lab-documentation","title":"Lab Documentation","text":"<ul> <li>Lab 1:</li> <li>Overview</li> <li>Setup</li> <li> <p>Data Models</p> </li> <li> <p>Lab 2: \u2b50 NEW</p> </li> <li>Overview</li> <li>Setup</li> <li>Data Models</li> <li>Troubleshooting</li> <li>Quick Reference</li> </ul>"},{"location":"#utilities","title":"Utilities","text":"<ul> <li>Data Inspector</li> <li>CSV Monitor</li> </ul>"},{"location":"#support","title":"Support","text":"<ul> <li>Troubleshooting</li> <li>FAQ</li> </ul>"},{"location":"#project-status","title":"Project Status","text":""},{"location":"#phase-1-foundation-complete","title":"Phase 1: Foundation \u2705 COMPLETE","text":"<ul> <li>\u2705 Shared utilities (DataInspector, CSVMonitor)</li> <li>\u2705 Central configuration</li> <li>\u2705 Lab1 working</li> <li>\u2705 Lab2 working with full documentation</li> </ul>"},{"location":"#phase-2-enhanced-lab-2-complete","title":"Phase 2: Enhanced Lab 2 \u2705 COMPLETE","text":"<ul> <li>\u2705 Market sentiment analysis pipeline</li> <li>\u2705 5 dbt models with incremental processing</li> <li>\u2705 14 automated data quality tests</li> <li>\u2705 Comprehensive documentation with diagrams</li> <li>\u2705 Troubleshooting guide</li> <li>\u2705 Quick reference card</li> <li>\u2705 Reference architecture diagrams</li> </ul>"},{"location":"#phase-3-platform-expansion-in-progress","title":"Phase 3: Platform Expansion \ud83d\udd04 IN PROGRESS","text":"<ul> <li>\ud83d\udd04 Lab 3 (Customer Segmentation)</li> <li>\ud83d\udd04 Streamlit dashboards</li> <li>\ud83d\udd04 Real API integrations (PRAW, NewsAPI)</li> <li>\ud83d\udccb Advanced sentiment analysis (Hugging Face)</li> </ul>"},{"location":"#phase-4-production-readiness-planned","title":"Phase 4: Production Readiness \ud83d\udccb PLANNED","text":"<ul> <li>\ud83d\udccb Airflow orchestration</li> <li>\ud83d\udccb CI/CD for data pipelines</li> <li>\ud83d\udccb Monitoring &amp; alerting</li> <li>\ud83d\udccb Data quality gates</li> <li>\ud83d\udccb Performance optimization</li> </ul>"},{"location":"#whats-new-in-lab-2","title":"What's New in Lab 2","text":""},{"location":"#key-achievements","title":"Key Achievements","text":"<ol> <li>Complete Sentiment Pipeline</li> <li>Reddit + News data ingestion</li> <li>5-layer transformation (Staging \u2192 Intermediate \u2192 Marts)</li> <li> <p>Incremental processing for efficiency</p> </li> <li> <p>Robust Data Quality</p> </li> <li>14 automated tests (100% passing)</li> <li>Contract enforcement on critical models</li> <li>Quality flags and validation</li> <li> <p>Anomaly detection</p> </li> <li> <p>Comprehensive Documentation</p> </li> <li>Full architecture diagrams (Mermaid)</li> <li>Step-by-step setup guide</li> <li>Complete model reference with schemas</li> <li>Troubleshooting for all issues encountered</li> <li> <p>Quick reference card</p> </li> <li> <p>Battle-Tested Solutions</p> </li> <li>Resolved duplicate key issues</li> <li>Fixed contract enforcement problems</li> <li>Solved CTE reference errors</li> <li>Optimized incremental logic</li> </ol>"},{"location":"#lab-2-metrics","title":"Lab 2 Metrics","text":"<ul> <li>Models: 5 (2 staging, 1 intermediate, 2 marts)</li> <li>Tests: 14 (all passing)</li> <li>Data Quality: 100%</li> <li>Documentation: 5 comprehensive guides</li> <li>Build Time: ~3 seconds</li> <li>Code Coverage: All models documented</li> </ul>"},{"location":"#learning-path","title":"Learning Path","text":""},{"location":"#beginners","title":"Beginners","text":"<ol> <li>Read Quick Start</li> <li>Complete Lab 1 setup</li> <li>Understand Medallion Architecture</li> </ol>"},{"location":"#intermediate","title":"Intermediate","text":"<ol> <li>Complete Lab 2 setup</li> <li>Study Lab 2 Data Models</li> <li>Learn dbt best practices</li> </ol>"},{"location":"#advanced","title":"Advanced","text":"<ol> <li>Customize Lab 2 for real data sources</li> <li>Build Lab 3 from scratch</li> <li>Implement Airflow orchestration</li> <li>Create production dashboards</li> </ol>"},{"location":"#contributing","title":"Contributing","text":""},{"location":"#report-issues","title":"Report Issues","text":"<p>Found a bug or have a suggestion? - Create a GitHub issue - Include error messages and steps to reproduce - Reference relevant documentation</p>"},{"location":"#improve-documentation","title":"Improve Documentation","text":"<ul> <li>Fix typos or unclear sections</li> <li>Add examples or diagrams</li> <li>Share your use cases</li> </ul>"},{"location":"#support_1","title":"Support","text":"<ul> <li>\ud83d\udcd6 Read the docs</li> <li>\u2753 Check FAQ</li> <li>\ud83d\udd27 Troubleshooting guides</li> <li>\ud83d\udc1b GitHub Issues</li> </ul>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Python: 3.11+</li> <li>OS: Mac 11+, Linux, or Windows with WSL2</li> <li>Memory: 2GB minimum</li> <li>Disk: 1GB base + data per lab</li> <li>Skills: Basic terminal, SQL, Python</li> </ul>"},{"location":"#quick-wins","title":"Quick Wins","text":"<p>Get started in 10 minutes: <pre><code># Clone\ngit clone https://github.com/narensham/buildcpg-labs.git\ncd buildcpg-labs/lab2_market_sentiment\n\n# Setup\npython3 -m venv lab2_env &amp;&amp; source lab2_env/bin/activate\npip install -r requirements.txt\n\n# Run\npython pipelines/ingest_sentiment.py\ncd dbt &amp;&amp; dbt deps &amp;&amp; dbt build\n\n# Success!\n# \u2705 14/14 tests passing\n</code></pre></p> <p>Platform: Multi-Lab Data Engineering Architecture: Independent labs + shared utilities Current Labs: 2 active, 1 planned Last Updated: November 2025 Maintainer: narensham Repository: GitHub Documentation: GitHub Pages</p>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#general-questions","title":"General Questions","text":""},{"location":"faq/#what-is-buildcpg-labs","title":"What is BuildCPG Labs?","text":"<p>A data engineering platform for managing multiple independent data labs using dbt, DuckDB, and Python. Each lab is isolated but shares common utilities.</p>"},{"location":"faq/#why-multiple-labs-instead-of-one-big-project","title":"Why multiple labs instead of one big project?","text":"<p>Different labs often have: - Different data sources and formats - Different transformation logic - Different schedules and SLAs - Independent teams working on them</p> <p>Keeping them separate prevents one from breaking others.</p>"},{"location":"faq/#do-i-need-to-use-all-labs","title":"Do I need to use all labs?","text":"<p>No. You can start with lab1 and create additional labs as needed. Each lab works independently.</p>"},{"location":"faq/#can-labs-share-data","title":"Can labs share data?","text":"<p>Currently, each lab has its own database. Future versions will support lab-to-lab data sharing through view exports.</p>"},{"location":"faq/#does-this-work-on-windows","title":"Does this work on Windows?","text":"<p>Yes, with WSL2 (Windows Subsystem for Linux). Mac 11+ and Linux are natively supported. No Docker required.</p>"},{"location":"faq/#architecture-questions","title":"Architecture Questions","text":""},{"location":"faq/#why-duckdb-instead-of-other-databases","title":"Why DuckDB instead of other databases?","text":"<ul> <li>No server needed (embedded)</li> <li>Works on older Macs (Mac 11+)</li> <li>No Docker required</li> <li>Small, easy to manage</li> <li>Perfect for single-machine setups</li> <li>Can scale to multi-machine later</li> </ul>"},{"location":"faq/#whats-the-difference-between-shared-and-lab-specific-files","title":"What's the difference between shared and lab-specific files?","text":"<p>Shared: Code used by ALL labs (DataInspector, CSVMonitor) Lab-specific: Code only that lab uses (models, dashboards, data)</p>"},{"location":"faq/#how-do-labs-communicate","title":"How do labs communicate?","text":"<p>They don't (by design). Future phases will add data export/import between labs.</p>"},{"location":"faq/#can-i-use-different-python-versions-per-lab","title":"Can I use different Python versions per lab?","text":"<p>Yes. Each lab has its own venv, so different packages/versions are allowed.</p>"},{"location":"faq/#setup-questions","title":"Setup Questions","text":""},{"location":"faq/#how-long-does-setup-take","title":"How long does setup take?","text":"<ul> <li>First time: 10-15 minutes (download, install dependencies)</li> <li>Each additional lab: 2 minutes (automatic bootstrap)</li> </ul>"},{"location":"faq/#can-i-skip-phase-1","title":"Can I skip Phase 1?","text":"<p>Not recommended. Phase 1 creates the foundation that Phases 2+ depend on.</p>"},{"location":"faq/#what-if-i-only-want-phase-1","title":"What if I only want Phase 1?","text":"<p>That's fine. Phase 1 is fully functional for basic lab work. Phase 2 adds convenience but isn't required.</p>"},{"location":"faq/#can-i-customize-the-templates","title":"Can I customize the templates?","text":"<p>Yes. Edit files in <code>shared/templates/</code> and they'll be used for new labs created after that.</p>"},{"location":"faq/#usage-questions","title":"Usage Questions","text":""},{"location":"faq/#what-does-make-setup-do","title":"What does <code>make setup</code> do?","text":"<ol> <li>Creates Python virtual environment (venv)</li> <li>Installs dependencies from requirements.txt</li> <li>Runs dbt debug to verify setup</li> </ol>"},{"location":"faq/#what-does-make-run-do","title":"What does <code>make run</code> do?","text":"<p>Executes <code>dbt run</code> which transforms data through bronze \u2192 silver \u2192 gold layers.</p>"},{"location":"faq/#how-do-i-see-what-changed-in-my-database","title":"How do I see what changed in my database?","text":"<p>Run <code>make inspect</code> to see schemas, tables, row counts, and quality scores.</p>"},{"location":"faq/#can-i-use-dbt-commands-directly","title":"Can I use dbt commands directly?","text":"<p>Yes. <code>cd lab/dbt &amp;&amp; dbt run</code> works the same as <code>make run</code>. Makefile just automates activation.</p>"},{"location":"faq/#how-do-i-add-a-new-model-to-lab1","title":"How do I add a new model to lab1?","text":"<pre><code>cd lab1_sales_performance/dbt/models\n# Create .sql file\n# Run: make run\n</code></pre> <p>See dbt documentation for model details.</p>"},{"location":"faq/#shared-utilities-questions","title":"Shared Utilities Questions","text":""},{"location":"faq/#how-do-i-use-datainspector-in-my-code","title":"How do I use DataInspector in my code?","text":"<pre><code>import sys\nsys.path.insert(0, '../..')\nfrom shared.utils.data_inspector import DataInspector\n\ninspector = DataInspector('data/lab1_sales_performance.duckdb')\nschemas = inspector.get_all_schemas()\n</code></pre>"},{"location":"faq/#can-i-modify-shared-utilities","title":"Can I modify shared utilities?","text":"<p>Yes, but test thoroughly. Changes affect ALL labs.</p>"},{"location":"faq/#how-do-i-add-a-new-shared-utility","title":"How do I add a new shared utility?","text":"<ol> <li>Create file in <code>shared/utils/</code></li> <li>Import it in your lab scripts</li> <li>Test thoroughly</li> <li>Commit to git</li> </ol>"},{"location":"faq/#do-shared-utilities-get-committed-to-git","title":"Do shared utilities get committed to git?","text":"<p>Yes. They're in the git repo so everyone gets them.</p>"},{"location":"faq/#data-questions","title":"Data Questions","text":""},{"location":"faq/#where-is-my-data-stored","title":"Where is my data stored?","text":"<p>In <code>lab/data/lab_name.duckdb</code> (DuckDB database file).</p>"},{"location":"faq/#how-do-i-backup-my-data","title":"How do I backup my data?","text":"<pre><code># Simply copy the .duckdb file\ncp lab1_sales_performance/data/lab1_sales_performance.duckdb lab1_sales_performance/data/lab1_sales_performance.duckdb.backup\n</code></pre>"},{"location":"faq/#how-large-can-databases-get","title":"How large can databases get?","text":"<p>DuckDB on a single machine can handle multi-GB databases easily. For 100GB+, consider upgrading infrastructure.</p>"},{"location":"faq/#can-i-read-external-data-in-dbt","title":"Can I read external data in dbt?","text":"<p>Yes. dbt supports reading from CSVs, APIs, and other sources via Jinja2 macros.</p>"},{"location":"faq/#how-do-i-connect-to-real-databases-not-duckdb","title":"How do I connect to real databases (not DuckDB)?","text":"<p>Future versions will support PostgreSQL, MySQL, etc. Currently limited to DuckDB.</p>"},{"location":"faq/#git-questions","title":"Git Questions","text":""},{"location":"faq/#can-i-commit-my-venv","title":"Can I commit my venv?","text":"<p>No. It's huge and system-specific. Use <code>.gitignore</code> to exclude it.</p>"},{"location":"faq/#can-i-commit-my-duckdb-files","title":"Can I commit my .duckdb files?","text":"<p>For development: no (they change constantly). For tracked data: yes (if you need version history).</p>"},{"location":"faq/#how-do-i-prevent-accidental-commits","title":"How do I prevent accidental commits?","text":"<p><code>.gitignore</code> automatically excludes venv, .duckdb, and build artifacts.</p>"},{"location":"faq/#what-should-i-commit","title":"What should I commit?","text":"<ul> <li>dbt models (.sql files)</li> <li>Python scripts (.py files)</li> <li>Configuration files (.yml, .yaml)</li> <li>Documentation (.md files)</li> <li>requirements.txt</li> </ul>"},{"location":"faq/#what-shouldnt-i-commit","title":"What shouldn't I commit?","text":"<ul> <li>venv/ directories</li> <li>.duckdb files (unless intentional)</li> <li>dbt/target/ and dbt/logs/</li> <li>.DS_Store</li> <li>IDE settings</li> </ul>"},{"location":"faq/#performance-questions","title":"Performance Questions","text":""},{"location":"faq/#how-fast-does-dbt-run","title":"How fast does dbt run?","text":"<p>Depends on data size. With ~3K rows: 1-2 seconds. Scale linearly with data.</p>"},{"location":"faq/#can-i-make-dbt-faster","title":"Can I make dbt faster?","text":"<ul> <li>Incremental materialization (load only new data)</li> <li>Optimize SQL queries</li> <li>Add indexes (if using real databases)</li> </ul>"},{"location":"faq/#will-my-laptop-slow-down-with-multiple-labs","title":"Will my laptop slow down with multiple labs?","text":"<p>Each lab runs independently. Multiple venvs take ~500MB each. Not a concern for 10 labs.</p>"},{"location":"faq/#scaling-questions","title":"Scaling Questions","text":""},{"location":"faq/#can-i-scale-to-50-labs","title":"Can I scale to 50 labs?","text":"<p>Yes. Structure supports unlimited labs. You'll just need: - Disk space (~1GB per lab average) - Orchestration tool (Airflow) to manage all runs</p>"},{"location":"faq/#whats-the-limit","title":"What's the limit?","text":"<p>No hard limit. Practical limit depends on infrastructure.</p>"},{"location":"faq/#can-i-migrate-to-cloud-later","title":"Can I migrate to cloud later?","text":"<p>Yes. The architecture is platform-agnostic. Move to Snowflake/BigQuery/etc. when needed.</p>"},{"location":"faq/#phase-questions","title":"Phase Questions","text":""},{"location":"faq/#whats-the-difference-between-phases","title":"What's the difference between phases?","text":"<ul> <li>Phase 1: Shared utilities + configuration</li> <li>Phase 2: Makefile + bootstrap (automation)</li> <li>Phase 3+: Orchestration + monitoring</li> </ul>"},{"location":"faq/#can-i-use-phase-1-without-phase-2","title":"Can I use Phase 1 without Phase 2?","text":"<p>Yes. Phase 1 is standalone and functional.</p>"},{"location":"faq/#when-should-i-use-phase-2","title":"When should I use Phase 2?","text":"<p>When you want <code>make</code> commands and automated lab creation. Highly recommended.</p>"},{"location":"faq/#when-should-i-use-phase-3","title":"When should I use Phase 3?","text":"<p>When managing 3+ labs and want automated scheduling (Airflow).</p>"},{"location":"faq/#team-questions","title":"Team Questions","text":""},{"location":"faq/#can-multiple-people-work-on-same-lab","title":"Can multiple people work on same lab?","text":"<p>Yes. Use git branches for features.</p>"},{"location":"faq/#can-different-people-own-different-labs","title":"Can different people own different labs?","text":"<p>Yes. Each lab is independent.</p>"},{"location":"faq/#how-do-we-prevent-conflicts","title":"How do we prevent conflicts?","text":"<ul> <li>Use git for coordination</li> <li>Clear lab ownership</li> <li>Communication on shared utilities</li> </ul>"},{"location":"faq/#can-we-code-review-lab-changes","title":"Can we code review lab changes?","text":"<p>Yes. Pull requests work for both lab-specific and shared code.</p>"},{"location":"faq/#troubleshooting-questions","title":"Troubleshooting Questions","text":""},{"location":"faq/#my-database-is-corrupted","title":"My database is corrupted","text":"<pre><code># Delete it and recreate\nrm lab1_sales_performance/data/lab1_sales_performance.duckdb\ncd lab1_sales_performance/dbt\ndbt run  # Recreates from source\n</code></pre>"},{"location":"faq/#i-accidentally-deleted-a-model","title":"I accidentally deleted a model","text":"<pre><code>git checkout &lt;file&gt;  # Restore from git\ncd dbt\ndbt run  # Recreate tables\n</code></pre>"},{"location":"faq/#everything-is-broken","title":"Everything is broken","text":"<pre><code>git status  # See what changed\ngit diff   # See details\ngit reset --hard  # Go back to last commit\n</code></pre>"},{"location":"faq/#who-do-i-ask-for-help","title":"Who do I ask for help?","text":"<p>See Troubleshooting guide or ask maintainer.</p>"},{"location":"faq/#feature-questions","title":"Feature Questions","text":""},{"location":"faq/#can-i-add-monitoringalerting","title":"Can I add monitoring/alerting?","text":"<p>Yes, in Phase 3+. Currently manual checks only.</p>"},{"location":"faq/#can-i-use-python-notebooks","title":"Can I use Python notebooks?","text":"<p>Yes. Put them in <code>lab/notebooks/</code>.</p>"},{"location":"faq/#can-i-have-dashboards","title":"Can I have dashboards?","text":"<p>Yes. Connect BI tools (Tableau/Looker) to lab databases.</p>"},{"location":"faq/#can-i-do-ml","title":"Can I do ML?","text":"<p>Yes. Add models to <code>lab/pipelines/</code> or use Jupyter.</p>"},{"location":"faq/#still-have-questions","title":"Still Have Questions?","text":"<ul> <li>Check Getting Started</li> <li>Check Troubleshooting</li> <li>Check Architecture</li> <li>Ask in GitHub Issues</li> </ul>"},{"location":"troubleshooting/","title":"Troubleshooting Guide","text":"<p>Solutions to common issues in BuildCPG Labs.</p>"},{"location":"troubleshooting/#installation-issues","title":"Installation Issues","text":""},{"location":"troubleshooting/#python-version-error","title":"Python Version Error","text":"<p>Problem: <code>python</code> command not found or shows Python 2.x</p> <p>Solution: <pre><code># Check available versions\nwhich python3\npython3 --version\n\n# Use python3 instead of python\npython3 -m venv venv\npython3 -m pip install pyyaml\n</code></pre></p>"},{"location":"troubleshooting/#virtual-environment-not-activating","title":"Virtual Environment Not Activating","text":"<p>Problem: <code>source venv/bin/activate</code> doesn't work on Mac</p> <p>Solution: <pre><code># Try full path\nsource ./venv/bin/activate\n\n# Or use this directly\n./venv/bin/python --version\n\n# For Zsh on newer Mac:\nsource venv/bin/activate\n</code></pre></p>"},{"location":"troubleshooting/#pip-command-not-found","title":"pip: command not found","text":"<p>Problem: <code>pip install</code> doesn't work</p> <p>Solution: <pre><code># Use python -m pip instead\npython -m pip install pyyaml duckdb pandas\n\n# Or upgrade pip\npython -m pip install --upgrade pip\n</code></pre></p>"},{"location":"troubleshooting/#permission-denied-error","title":"Permission Denied Error","text":"<p>Problem: <code>permission denied: ./setup.sh</code></p> <p>Solution: <pre><code>chmod +x setup.sh\n./setup.sh\n</code></pre></p>"},{"location":"troubleshooting/#dbt-issues","title":"dbt Issues","text":""},{"location":"troubleshooting/#dbt-command-not-found","title":"dbt: command not found","text":"<p>Problem: <code>dbt run</code> or <code>dbt debug</code> doesn't work</p> <p>Solution: <pre><code># Make sure you're in lab directory\ncd lab1_sales_performance\n\n# Make sure venv is activated\nsource venv/bin/activate\n\n# Try again\ndbt --version\n\n# If still fails, reinstall\npip install dbt-duckdb\n</code></pre></p>"},{"location":"troubleshooting/#dbt-debug-shows-errors","title":"dbt debug shows errors","text":"<p>Problem: <code>dbt debug</code> fails with configuration errors</p> <p>Solution: <pre><code># Check you're in dbt directory\ncd lab1_sales_performance/dbt\n\n# Verify profiles.yml exists\nls -la profiles.yml\n\n# Check dbt_project.yml\nls -la dbt_project.yml\n\n# Run debug again\ndbt debug\n</code></pre></p>"},{"location":"troubleshooting/#profile-not-found","title":"Profile not found","text":"<p>Problem: <code>Profile 'lab1_duckdb' not found</code></p> <p>Solution: <pre><code># Check profiles.yml path\ncat ~/.dbt/profiles.yml\n\n# Or ensure it's in the lab directory\ncd lab1_sales_performance/dbt\ncat profiles.yml\n\n# Update dbt_project.yml to point to correct profile\n</code></pre></p>"},{"location":"troubleshooting/#data-inspector-issues","title":"Data Inspector Issues","text":""},{"location":"troubleshooting/#modulenotfounderror-no-module-named-shared","title":"ModuleNotFoundError: No module named 'shared'","text":"<p>Problem: Python can't find shared utilities</p> <p>Solution: <pre><code># Make sure you're in lab directory\ncd lab1_sales_performance\n\n# Run from correct location\npython scripts/inspect_data.py\n\n# Verify path in script\nhead -20 scripts/inspect_data.py\n# Should show: sys.path.insert(0, '../..')\n</code></pre></p>"},{"location":"troubleshooting/#filenotfounderror-labs_configyaml","title":"FileNotFoundError: labs_config.yaml","text":"<p>Problem: Configuration file not found</p> <p>Solution: <pre><code># Check file exists\nls -la config/labs_config.yaml\n\n# Create if missing\ntouch config/labs_config.yaml\n\n# Add content\ncat &gt; config/labs_config.yaml &lt;&lt; 'EOF'\nlabs:\n  lab1_sales_performance:\n    path: lab1_sales_performance\n    db_path: lab1_sales_performance/data/lab1_sales_performance.duckdb\n    dbt_path: lab1_sales_performance/dbt\nEOF\n</code></pre></p>"},{"location":"troubleshooting/#database-file-not-found","title":"Database file not found","text":"<p>Problem: <code>Cannot open file \".../lab1_sales_performance.duckdb\"</code></p> <p>Solution: <pre><code># Check database exists\nls -la lab1_sales_performance/data/\n\n# Create database by running dbt\ncd lab1_sales_performance/dbt\ndbt run\n\n# Then try inspection again\ncd ..\npython scripts/inspect_data.py\n</code></pre></p>"},{"location":"troubleshooting/#git-issues","title":"Git Issues","text":""},{"location":"troubleshooting/#too-many-loose-objects-in-repository","title":"Too many loose objects in repository","text":"<p>Problem: <code>warning: There are too many unreachable loose objects</code></p> <p>Solution: <pre><code># Remove gc.log if it exists\nrm .git/gc.log 2&gt;/dev/null || true\n\n# Prune repository\ngit prune\n\n# Optimize repository\ngit gc --aggressive\n</code></pre></p>"},{"location":"troubleshooting/#venv-directory-accidentally-committed","title":"venv directory accidentally committed","text":"<p>Problem: Tried to push and got huge file size</p> <p>Solution: <pre><code># Remove from git tracking (keeps files on disk)\ngit rm -r --cached venv/\ngit rm -r --cached .venv/\ngit rm -r --cached dbt/target/\n\n# Update .gitignore\necho \"venv/\" &gt;&gt; .gitignore\necho \".venv/\" &gt;&gt; .gitignore\necho \"dbt/target/\" &gt;&gt; .gitignore\n\n# Commit cleanup\ngit add .gitignore\ngit commit -m \"Remove build artifacts from git\"\n</code></pre></p>"},{"location":"troubleshooting/#makefile-issues","title":"Makefile Issues","text":""},{"location":"troubleshooting/#make-command-not-found","title":"make: command not found","text":"<p>Problem: <code>make</code> is not installed</p> <p>Solution (Mac): <pre><code># Install Xcode command line tools\nxcode-select --install\n\n# Or install make directly\nbrew install make\n</code></pre></p> <p>Solution (Linux): <pre><code>sudo apt-get update\nsudo apt-get install make build-essential\n</code></pre></p>"},{"location":"troubleshooting/#tab-indentation-error-in-makefile","title":"Tab indentation error in Makefile","text":"<p>Problem: <code>missing separator. Stop.</code> when running make</p> <p>Solution: <pre><code># Makefiles require TAB indentation, not spaces\n# Edit Makefile and ensure all recipe lines start with TAB\n\n# In vim:\n# :set noexpandtab\n\n# In nano:\n# Ctrl+I to insert tab\n\n# Visual check:\ncat -A Makefile | head -20\n# Tabs show as ^I, spaces show as regular spaces\n</code></pre></p>"},{"location":"troubleshooting/#venv-not-found-when-running-make","title":"venv not found when running make","text":"<p>Problem: <code>venv/bin/python: No such file or directory</code></p> <p>Solution: <pre><code># Run setup first\nmake setup\n\n# This creates venv and installs dependencies\n</code></pre></p>"},{"location":"troubleshooting/#common-data-issues","title":"Common Data Issues","text":""},{"location":"troubleshooting/#no-schemas-in-database","title":"No schemas in database","text":"<p>Problem: <code>inspect_data.py</code> shows only 'main' and 'information_schema'</p> <p>Solution: <pre><code># Tables haven't been created yet\n# Create them with dbt\ncd lab1_sales_performance/dbt\ndbt run\n\n# Then inspect again\ncd ..\npython scripts/inspect_data.py\n</code></pre></p>"},{"location":"troubleshooting/#quality-score-shows-0","title":"Quality score shows 0%","text":"<p>Problem: <code>get_quality_score</code> returns 0 even with good data</p> <p>Solution: <pre><code># Check table has rows\npython -c \"\nimport sys\nsys.path.insert(0, '../..')\nfrom shared.utils.data_inspector import DataInspector\ninspector = DataInspector('data/lab1_sales_performance.duckdb')\nprint(inspector.get_table_stats('raw', 'sales_data'))\n\"\n\n# If rows = 0, data wasn't loaded\n# Load it with dbt:\ncd dbt\ndbt run\n</code></pre></p>"},{"location":"troubleshooting/#csv-says-no-new-data-but-file-was-updated","title":"CSV says \"no new data\" but file was updated","text":"<p>Problem: <code>check_for_new_data.py</code> doesn't detect changes</p> <p>Solution: <pre><code># State file might be outdated\n# Remove it to reset detection\nrm lab1_sales_performance/.csv_state.json\n\n# Run again\ncd lab1_sales_performance\npython scripts/check_for_new_data.py\n</code></pre></p>"},{"location":"troubleshooting/#getting-help","title":"Getting Help","text":""},{"location":"troubleshooting/#enable-debug-logging","title":"Enable Debug Logging","text":"<pre><code># For dbt:\ndbt run --debug\n\n# For Python scripts:\n# Add this to beginning of script:\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre>"},{"location":"troubleshooting/#check-file-permissions","title":"Check File Permissions","text":"<pre><code># Verify script is executable\nls -la scripts/inspect_data.py\n\n# Make executable if needed\nchmod +x scripts/inspect_data.py\n</code></pre>"},{"location":"troubleshooting/#verify-directory-structure","title":"Verify Directory Structure","text":"<pre><code># From buildcpg-labs root:\ntree -L 2 -I '__pycache__|*.pyc|venv'\n\n# Or list key files:\nfind . -name \"dbt_project.yml\" -o -name \"labs_config.yaml\" -o -name \"Makefile\"\n</code></pre>"},{"location":"troubleshooting/#check-disk-space","title":"Check Disk Space","text":"<pre><code># Verify you have space\ndf -h\n\n# Check database size\ndu -sh lab1_sales_performance/data/\n</code></pre>"},{"location":"troubleshooting/#still-stuck","title":"Still Stuck?","text":"<ol> <li>Check FAQ for common questions</li> <li>Review Quick Start</li> <li>Check Architecture to understand the design</li> <li>Review the exact error message and search in this guide</li> <li>Check git commit history for clues: <code>git log --oneline</code> </li> </ol>"},{"location":"architecture/medallion-architecture/","title":"Medallion Architecture","text":""},{"location":"architecture/medallion-architecture/#overview","title":"Overview","text":"<p>The Medallion Architecture organizes data pipelines into progressive layers to ensure quality, reliability, and efficiency in analytics.</p>"},{"location":"architecture/medallion-architecture/#layers","title":"Layers","text":"<ol> <li> <p>Bronze (Raw)    Raw ingested data from source systems. Minimal transformation, only cleaned for parsing errors.</p> </li> <li> <p>Silver (Cleaned / Curated)    Data is cleaned, deduplicated, and enriched. Suitable for basic analytics and reporting.</p> </li> <li> <p>Gold (Business-ready / Aggregated)    Fully transformed data optimized for dashboards, ML, and advanced analytics.</p> </li> </ol>"},{"location":"architecture/medallion-architecture/#principles","title":"Principles","text":"<ul> <li>Incremental processing: Each layer adds value without overwriting prior stages.</li> <li>Traceability: Easy to trace data lineage from Gold \u2192 Silver \u2192 Bronze.</li> <li>Reusability: Models at Silver and Gold layers can be shared across multiple labs.</li> </ul>"},{"location":"architecture/medallion-architecture/#benefits","title":"Benefits","text":"<ul> <li>Reduces data quality issues downstream.</li> <li>Enables faster experimentation and analytics.</li> <li>Facilitates modular ETL development in a multi-lab environment.</li> </ul>"},{"location":"architecture/medallion-architecture/#visual-diagram","title":"Visual Diagram","text":""},{"location":"architecture/multi-lab-design/","title":"Multi-Lab Design","text":""},{"location":"architecture/multi-lab-design/#overview","title":"Overview","text":"<p>The Multi-Lab Design organizes a data engineering playground into multiple independent labs, each representing a self-contained ETL pipeline or analytical experiment. This design ensures modularity, safety, and flexibility, allowing teams or individuals to experiment without affecting other projects.</p>"},{"location":"architecture/multi-lab-design/#goals","title":"Goals","text":"<ul> <li>Isolation: Each lab operates independently with its own datasets, models, and transformations.</li> <li>Modularity: Labs can be added, removed, or updated without impacting the broader system.</li> <li>Reusability: Shared components or datasets can be referenced across labs while maintaining separation of experimental data.</li> </ul>"},{"location":"architecture/multi-lab-design/#lab-structure","title":"Lab Structure","text":""},{"location":"architecture/multi-lab-design/#typical-lab-folder","title":"Typical Lab Folder","text":"<pre><code>lab1_sales_performance/\n\u251c\u2500\u2500 raw/\n\u251c\u2500\u2500 bronze/\n\u251c\u2500\u2500 silver/\n\u251c\u2500\u2500 gold/\n\u251c\u2500\u2500 dbt/\n\u2502 \u251c\u2500\u2500 models/\n\u2502 \u251c\u2500\u2500 snapshots/\n\u2502 \u2514\u2500\u2500 seeds/\n\u2514\u2500\u2500 README.md\n</code></pre> <ul> <li>raw: Ingested source data (unchanged).  </li> <li>bronze: Cleaned, minimally transformed data.  </li> <li>silver: Enriched and validated datasets.  </li> <li>gold: Business-ready, aggregated tables for analysis or reporting.  </li> <li>dbt: Contains lab-specific transformations and metadata.  </li> </ul>"},{"location":"architecture/multi-lab-design/#shared-vs-lab-specific","title":"Shared vs Lab-Specific","text":"<ul> <li>Lab-specific datasets: Only relevant to a single lab; changes are isolated.  </li> <li>Shared datasets: Common reference tables or utilities used across multiple labs, stored in a central <code>shared/</code> directory.  </li> </ul> <p>\ud83d\udca1 Tip: Only include data in <code>shared/</code> if multiple labs depend on it. Avoid making everything shared to prevent accidental coupling.</p>"},{"location":"architecture/multi-lab-design/#benefits-of-multi-lab-design","title":"Benefits of Multi-Lab Design","text":"<ul> <li>Safe experimentation: Teams can iterate freely without breaking other pipelines.  </li> <li>Scalability: Easily add new labs for different datasets or analytical scenarios.  </li> <li>Reproducibility: Clear boundaries make it easy to reproduce experiments or roll back changes.  </li> <li>Collaboration: Multiple contributors can work in parallel without conflicts.</li> </ul>"},{"location":"architecture/multi-lab-design/#best-practices","title":"Best Practices","text":"<ol> <li>Maintain consistent folder structures across labs.  </li> <li>Clearly document each lab\u2019s purpose in its README.  </li> <li>Version shared datasets and transformations carefully.  </li> <li>Use naming conventions to avoid collisions (e.g., <code>lab1_sales_2025_bronze</code>).  </li> <li>Leverage automation (e.g., CI/CD pipelines) to validate transformations before deployment.</li> </ol>"},{"location":"architecture/multi-lab-design/#example-diagram","title":"Example Diagram","text":"<p><pre><code>+----------------+     +----------------+     +----------------+\n|   Lab 1        |     |   Lab 2        |     |   Lab 3        |\n|   raw \u2192 bronze |     | raw \u2192 bronze   |     | raw \u2192 bronze   |\n|   \u2192 silver     |     | \u2192 silver       |     | \u2192 silver       |\n|   \u2192 gold       |     | \u2192 gold         |     | \u2192 gold         |\n+----------------+     +----------------+     +----------------+\n         \\                   |                     /\n          \\                  |                    /\n           \\                 |                   /\n            \\                |                  /\n             \\               |                 /\n              +--------------------------------+\n              |           Shared Data          |\n              +--------------------------------+\n</code></pre> This diagram shows independent labs consuming shared datasets while maintaining isolated pipelines.</p>"},{"location":"architecture/multi-lab-design/#summary","title":"Summary","text":"<p>The Multi-Lab Design is ideal for learning, experimentation, and scalable analytics. By enforcing modularity and isolation, it enables reproducible workflows and safe collaboration while maintaining the flexibility to share data and reusable transformations across projects.</p>"},{"location":"architecture/overview/","title":"Architecture Overview","text":""},{"location":"architecture/overview/#system-design","title":"System Design","text":"<p>BuildCPG Labs uses a multi-lab, shared-utilities architecture designed for scalability and independence.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         ORCHESTRATION LAYER (Airflow/Prefect)           \u2502\n\u2502  Schedules pipelines, monitors health, handles alerts   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              DATA INGESTION LAYER                        \u2502\n\u2502  \u2022 CSV monitoring \u2022 API polling \u2022 Data validation       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         dbt TRANSFORMATION LAYER (Per Lab)              \u2502\n\u2502  \u2022 Bronze (Raw) \u2192 Silver (Cleaned) \u2192 Gold (Analytics)  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              QUALITY ASSURANCE LAYER                     \u2502\n\u2502  \u2022 dbt tests \u2022 Data freshness \u2022 Quality scoring         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         SHARED UTILITIES LAYER (Root Level)             \u2502\n\u2502  \u2022 DataInspector \u2022 CSVMonitor \u2022 Path Helpers            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/overview/#directory-structure-current-setup","title":"Directory Structure (Current Setup)","text":"<pre><code>buildcpg-labs/\n\u2502\n\u251c\u2500\u2500 .venv/                           # SINGLE venv (shared by all labs)\n\u2502   \u251c\u2500\u2500 bin/\n\u2502   \u251c\u2500\u2500 lib/\n\u2502   \u2514\u2500\u2500 ...\n\u2502\n\u251c\u2500\u2500 shared/                          # Shared across ALL labs\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 utils/\n\u2502   \u2502   \u251c\u2500\u2500 data_inspector.py       # Inspect databases\n\u2502   \u2502   \u251c\u2500\u2500 csv_monitor.py          # Detect new data\n\u2502   \u2502   \u2514\u2500\u2500 config_loader.py        # Load configurations\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 config/                      # Config inside shared (not at root)\n\u2502   \u2502   \u251c\u2500\u2500 labs_config.yaml        # Central lab registry\n\u2502   \u2502   \u2514\u2500\u2500 paths.py                # Path helpers\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 data_quality/\n\u2502   \u2502   \u251c\u2500\u2500 validators.py           # Quality validators\n\u2502   \u2502   \u2514\u2500\u2500 expectations.py         # Data expectations\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 templates/\n\u2502       \u251c\u2500\u2500 Makefile_template       # Template Makefile\n\u2502       \u251c\u2500\u2500 requirements_template.txt\n\u2502       \u251c\u2500\u2500 dbt_project_template.yml\n\u2502       \u2514\u2500\u2500 .gitignore_template\n\u2502\n\u251c\u2500\u2500 lab1_sales_performance/         # LAB 1 (Independent data, shares venv)\n\u2502   \u251c\u2500\u2500 dbt/\n\u2502   \u2502   \u251c\u2500\u2500 dbt_project.yml\n\u2502   \u2502   \u251c\u2500\u2500 profiles.yml            # Manually edited when switching labs\n\u2502   \u2502   \u251c\u2500\u2500 models/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 staging/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 intermediate/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 marts/\n\u2502   \u2502   \u2514\u2500\u2500 tests/\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 data/\n\u2502   \u2502   \u251c\u2500\u2500 raw/\n\u2502   \u2502   \u2514\u2500\u2500 lab1_sales_performance.duckdb\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 scripts/\n\u2502   \u2502   \u251c\u2500\u2500 inspect_data.py\n\u2502   \u2502   \u2514\u2500\u2500 check_for_new_data.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 pipelines/\n\u2502   \u2502   \u251c\u2500\u2500 data_ingestion.py\n\u2502   \u2502   \u2514\u2500\u2500 data_quality.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 requirements.txt            # Shared dependencies\n\u2502   \u2514\u2500\u2500 .gitignore\n\u2502\n\u251c\u2500\u2500 lab2_forecast_model/            # LAB 2 (Independent data, shares venv)\n\u2502   \u2514\u2500\u2500 (Same structure as lab1)\n\u2502\n\u251c\u2500\u2500 lab3_customer_segmentation/     # LAB 3 (Independent data, shares venv)\n\u2502   \u2514\u2500\u2500 (Same structure as lab1)\n\u2502\n\u251c\u2500\u2500 orchestration/\n\u2502   \u2514\u2500\u2500 airflow_dags.py             # Multi-lab orchestration\n\u2502\n\u251c\u2500\u2500 docs/                           # This documentation\n\u251c\u2500\u2500 setup_new_lab.sh                # Bootstrap new labs\n\u251c\u2500\u2500 mkdocs.yml                      # Documentation config\n\u251c\u2500\u2500 .gitignore\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"architecture/overview/#key-concepts","title":"Key Concepts","text":""},{"location":"architecture/overview/#single-virtual-environment-approach","title":"Single Virtual Environment Approach","text":"<p>Current Setup: - ONE <code>.venv/</code> at root shared by all labs - All labs use same Python packages - Switch between labs by changing dbt profiles manually</p> <p>Advantages: - Less disk space (~500MB vs ~500MB per lab) - Consistent package versions across all labs - Simpler initial setup</p> <p>Trade-offs: - Cannot have labs with conflicting dependencies - Must manually edit <code>profiles.yml</code> when switching labs - Risk of profile switching errors (writing to wrong database) - No concurrent work on different labs</p> <p>\u26a0\ufe0f See Current Setup Details for full pros/cons analysis</p>"},{"location":"architecture/overview/#labs-have-independent-data","title":"Labs Have Independent Data","text":"<ul> <li>Each lab has its own database (lab1.duckdb, lab2.duckdb)</li> <li>Each lab has its own dbt project (dbt/models/, dbt/dbt_project.yml)</li> <li>Each lab has its own raw data (data/raw/)</li> <li>Labs share Python environment but NOT data</li> </ul>"},{"location":"architecture/overview/#shared-utilities","title":"Shared Utilities","text":"<ul> <li>DataInspector - Check database quality (used by all labs)</li> <li>CSVMonitor - Detect new data in CSVs (used by all labs)</li> <li>Config Paths - Get paths for any lab (used by all labs)</li> <li>Written once in <code>shared/</code>, used by all labs</li> <li>Bug fix in shared code fixes all labs</li> </ul>"},{"location":"architecture/overview/#configuration-location","title":"Configuration Location","text":"<p>Config is inside shared/config/ (not at root level):</p> <pre><code># Import pattern for lab scripts\nimport sys\nsys.path.insert(0, '../..')\nfrom shared.config.paths import get_lab_db_path  # Note: shared.config\n\n# NOT this:\n# from config.paths import get_lab_db_path  # \u274c Wrong path\n</code></pre> <pre><code># shared/config/labs_config.yaml\nlabs:\n  lab1_sales_performance:\n    path: lab1_sales_performance\n    db_path: lab1_sales_performance/data/lab1_sales_performance.duckdb\n    dbt_path: lab1_sales_performance/dbt\n\n  lab2_forecast_model:\n    path: lab2_forecast_model\n    db_path: lab2_forecast_model/data/lab2_forecast_model.duckdb\n    dbt_path: lab2_forecast_model/dbt\n</code></pre> <p>This registry tells the system where each lab is and how to find its resources.</p>"},{"location":"architecture/overview/#data-flow","title":"Data Flow","text":""},{"location":"architecture/overview/#single-lab-example","title":"Single Lab Example","text":"<pre><code>CSV Input\n    \u2193\ndbt Load (raw schema)\n    \u2193\ndbt Transform (bronze \u2192 silver \u2192 gold)\n    \u2193\nDuckDB Tables\n    \u2193\nDataInspector (quality check)\n    \u2193\nBI Tool (Tableau/Looker)\n</code></pre>"},{"location":"architecture/overview/#multiple-labs-orchestrated","title":"Multiple Labs Orchestrated","text":"<pre><code>Airflow DAG (Daily 12 AM)\n    \u251c\u2500\u2500 Lab1: Check CSV \u2192 Load \u2192 Transform \u2192 Test \u2192 Inspect\n    \u251c\u2500\u2500 Lab2: Check CSV \u2192 Load \u2192 Transform \u2192 Test \u2192 Inspect\n    \u2514\u2500\u2500 Lab3: Check CSV \u2192 Load \u2192 Transform \u2192 Test \u2192 Inspect\n    \u2193\nAll results aggregated\n    \u2193\nAlert team if any lab fails\n</code></pre>"},{"location":"architecture/overview/#workflow-switching-between-labs","title":"Workflow: Switching Between Labs","text":"<pre><code># 1. Activate shared venv (once per session)\ncd buildcpg-labs\nsource .venv/bin/activate\n\n# 2. Work on lab1\ncd lab1_sales_performance/dbt\n# profiles.yml should point to: ../data/lab1_sales_performance.duckdb\ndbt debug  # Verify correct database\ndbt run\n\n# 3. Switch to lab2\ncd ../../lab2_forecast_model/dbt\n# Edit profiles.yml to point to: ../data/lab2_forecast_model.duckdb\nvim profiles.yml  # Update path\ndbt debug  # Verify correct database\ndbt run\n</code></pre> <p>\u26a0\ufe0f Critical: Always run <code>dbt debug</code> before <code>dbt run</code> to verify you're pointing to the correct database.</p>"},{"location":"architecture/overview/#technology-stack","title":"Technology Stack","text":"Layer Technology Database DuckDB (embedded, Mac compatible) Transformation dbt (data build tool) Scripting Python 3.11+ Environment Single venv (shared) Orchestration Airflow (optional, future) Version Control Git Documentation MkDocs"},{"location":"architecture/overview/#design-principles","title":"Design Principles","text":""},{"location":"architecture/overview/#1-data-independence","title":"1. Data Independence","text":"<p>Labs have separate databases and raw data. One lab's data corruption doesn't affect others.</p>"},{"location":"architecture/overview/#2-shared-python-environment","title":"2. Shared Python Environment","text":"<p>All labs use same venv for consistency and space efficiency (with trade-offs).</p>"},{"location":"architecture/overview/#3-reusability","title":"3. Reusability","text":"<p>Code written for shared utilities is used by all labs without duplication.</p>"},{"location":"architecture/overview/#4-scalability","title":"4. Scalability","text":"<p>Adding lab 10 takes same effort as adding lab 2 (but dependency conflicts may limit this).</p>"},{"location":"architecture/overview/#5-clarity","title":"5. Clarity","text":"<p>Each lab's purpose is clear. Shared code's purpose is clear.</p>"},{"location":"architecture/overview/#6-manual-coordination","title":"6. Manual Coordination","text":"<p>Profile switching requires discipline and verification steps.</p>"},{"location":"architecture/overview/#comparison-current-vs-alternative-architectures","title":"Comparison: Current vs Alternative Architectures","text":""},{"location":"architecture/overview/#current-setup-single-venv","title":"Current Setup (Single venv)","text":"<p>\u2705 Space efficient (one venv) \u2705 Consistent packages \u2705 Simple setup \u274c Dependency conflicts possible \u274c Manual profile switching \u274c No concurrent work  </p>"},{"location":"architecture/overview/#alternative-per-lab-venvs","title":"Alternative: Per-Lab venvs","text":"<p>\u274c More disk space \u274c More complex setup \u2705 Complete isolation \u2705 Different dependencies per lab \u2705 Automatic profile management \u2705 Concurrent work safe  </p> <p>When to migrate: See Current Setup Analysis</p>"},{"location":"architecture/overview/#migration-path","title":"Migration Path","text":""},{"location":"architecture/overview/#current-state-phase-1-single-venv","title":"Current State: Phase 1 (Single venv)","text":"<ul> <li>Shared utilities \u2705</li> <li>Central configuration \u2705</li> <li>Lab1 working \u2705</li> <li>Single venv \u2705</li> </ul>"},{"location":"architecture/overview/#future-phase-2-optional-migration-to-per-lab-venvs","title":"Future: Phase 2 (Optional migration to per-lab venvs)","text":"<p>When you hit: - 3+ labs - Dependency conflicts - Multiple team members - Production requirements</p> <p>Then consider: Per-lab venvs + automated profile management</p>"},{"location":"architecture/overview/#see-also","title":"See Also","text":"<ul> <li>Current Setup Details - Full analysis of single venv approach</li> <li>Shared vs Lab-Specific - What goes where</li> <li>Phase 1 Guide - Implementation details</li> </ul>"},{"location":"architecture/shared-vs-lab-specific/","title":"Shared vs Lab-Specific Data","text":""},{"location":"architecture/shared-vs-lab-specific/#overview","title":"Overview","text":"<p>In a multi-lab environment, some data is shared across labs, while other datasets remain lab-specific. Proper organization is crucial for maintainability and scalability.</p>"},{"location":"architecture/shared-vs-lab-specific/#lab-specific-data","title":"Lab-Specific Data","text":"<ul> <li>Stored within the lab\u2019s dedicated folder.</li> <li>Includes lab experiments, transformations, and temporary datasets.</li> <li>Changes affect only the corresponding lab.</li> </ul>"},{"location":"architecture/shared-vs-lab-specific/#shared-data","title":"Shared Data","text":"<ul> <li>Placed in a centralized <code>shared</code> directory.</li> <li>Includes:</li> <li>Standardized reference datasets (e.g., product catalogs, master lists)</li> <li>Common transformations or utility tables</li> <li>Any updates must consider impact on all labs consuming the shared data.</li> </ul>"},{"location":"architecture/shared-vs-lab-specific/#guidelines","title":"Guidelines","text":"<ol> <li>Only include truly reusable datasets in <code>shared</code>.</li> <li>Maintain versioning for shared datasets to prevent breaking changes.</li> <li>Document dependencies clearly in lab README files.</li> </ol>"},{"location":"architecture/shared-vs-lab-specific/#benefits","title":"Benefits","text":"<ul> <li>Prevents accidental data contamination.</li> <li>Encourages modularity and reuse.</li> <li>Simplifies debugging and troubleshooting across labs.</li> </ul>"},{"location":"getting-started/installation/","title":"Installation Guide","text":"<p>Complete setup instructions for BuildCPG Labs.</p>"},{"location":"getting-started/installation/#system-requirements","title":"System Requirements","text":"<ul> <li>Python: 3.11 or higher</li> <li>OS: Mac 11+ or Linux (Windows with WSL2)</li> <li>Memory: 2GB minimum</li> <li>Disk: 1GB for base installation + data</li> </ul>"},{"location":"getting-started/installation/#step-1-install-prerequisites","title":"Step 1: Install Prerequisites","text":""},{"location":"getting-started/installation/#install-homebrew-mac-only","title":"Install Homebrew (Mac Only)","text":"<pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre>"},{"location":"getting-started/installation/#install-python-311","title":"Install Python 3.11+","text":"<p>Using Homebrew (Mac): <pre><code>brew install python@3.11\n</code></pre></p> <p>Using System Package Manager (Linux): <pre><code>sudo apt-get update\nsudo apt-get install python3.11 python3.11-venv\n</code></pre></p>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>python3 --version\n# Should show: Python 3.11.x or higher\n\npip --version\n# Should show: pip 24.0 or higher\n</code></pre>"},{"location":"getting-started/installation/#step-2-clone-repository","title":"Step 2: Clone Repository","text":""},{"location":"getting-started/installation/#using-https","title":"Using HTTPS","text":"<pre><code>git clone https://github.com/narensham/buildcpg-labs.git\ncd buildcpg-labs\n</code></pre>"},{"location":"getting-started/installation/#or-using-ssh","title":"Or Using SSH","text":"<pre><code>git clone git@github.com:narensham/buildcpg-labs.git\ncd buildcpg-labs\n</code></pre>"},{"location":"getting-started/installation/#step-3-install-global-dependencies","title":"Step 3: Install Global Dependencies","text":"<p>These packages are needed at the root level:</p> <pre><code>pip install --upgrade pip\npip install pyyaml duckdb pandas mkdocs mkdocs-material\n</code></pre>"},{"location":"getting-started/installation/#step-4-verify-setup","title":"Step 4: Verify Setup","text":""},{"location":"getting-started/installation/#check-python","title":"Check Python","text":"<pre><code>python --version\n# Should show: Python 3.11.x or higher\n</code></pre>"},{"location":"getting-started/installation/#check-git","title":"Check Git","text":"<pre><code>git --version\n# Should show: git version 2.x or higher\n</code></pre>"},{"location":"getting-started/installation/#check-directory-structure","title":"Check Directory Structure","text":"<pre><code>ls -la\n# Should show: shared/, config/, lab1_sales_performance/, mkdocs.yml, etc.\n</code></pre>"},{"location":"getting-started/installation/#test-config","title":"Test Config","text":"<pre><code>python config/paths.py\n# Expected output:\n# \u2705 Lab1 config loaded: ...\n# \u2705 Lab1 DB path: /path/to/lab1_sales_performance/data/lab1_sales_performance.duckdb\n</code></pre>"},{"location":"getting-started/installation/#step-5-set-up-lab-1","title":"Step 5: Set Up Lab 1","text":""},{"location":"getting-started/installation/#create-lab-environment","title":"Create Lab Environment","text":"<pre><code>cd lab1_sales_performance\npython3 -m venv venv\nsource venv/bin/activate\n\n# If using Mac Zsh and venv doesn't activate:\nsource venv/bin/activate\n</code></pre>"},{"location":"getting-started/installation/#install-lab-dependencies","title":"Install Lab Dependencies","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"getting-started/installation/#verify-dbt","title":"Verify dbt","text":"<pre><code>dbt --version\n# Should show: dbt version 1.x.x\n</code></pre>"},{"location":"getting-started/installation/#check-dbt-configuration","title":"Check dbt Configuration","text":"<pre><code>cd dbt\ndbt debug\n# Should show: \u2705 All checks passed!\n</code></pre>"},{"location":"getting-started/installation/#step-6-verify-everything-works","title":"Step 6: Verify Everything Works","text":""},{"location":"getting-started/installation/#test-data-inspector","title":"Test Data Inspector","text":"<pre><code>cd /path/to/buildcpg-labs/lab1_sales_performance\npython scripts/inspect_data.py\n# Should show: Schemas, tables, row counts\n</code></pre>"},{"location":"getting-started/installation/#test-csv-monitor","title":"Test CSV Monitor","text":"<pre><code>python scripts/check_for_new_data.py\n# Should show: CSV file info and last modification\n</code></pre>"},{"location":"getting-started/installation/#test-config-access","title":"Test Config Access","text":"<pre><code>cd /path/to/buildcpg-labs\npython config/paths.py\n# Should show: \u2705 Config loaded successfully\n</code></pre>"},{"location":"getting-started/installation/#alternative-automated-setup-script","title":"Alternative: Automated Setup Script","text":"<p>If you want to automate steps 1-5, create this script:</p> <p>File: <code>setup.sh</code> <pre><code>#!/bin/bash\nset -e\n\necho \"Setting up BuildCPG Labs...\"\n\n# Install global dependencies\necho \"Installing global dependencies...\"\npip install --upgrade pip\npip install pyyaml duckdb pandas mkdocs mkdocs-material\n\n# Setup lab1\necho \"Setting up lab1_sales_performance...\"\ncd lab1_sales_performance\npython3 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n\n# Verify\necho \"Verifying setup...\"\ncd dbt\ndbt debug\n\necho \"\u2705 Setup complete!\"\necho \"\"\necho \"Next steps:\"\necho \"  cd ../.. (go to root)\"\necho \"  cd lab1_sales_performance\"\necho \"  python scripts/inspect_data.py\"\n</code></pre></p> <p>Run it: <pre><code>chmod +x setup.sh\n./setup.sh\n</code></pre></p>"},{"location":"getting-started/installation/#troubleshooting-installation","title":"Troubleshooting Installation","text":""},{"location":"getting-started/installation/#python-version-error","title":"Python version error","text":"<pre><code># If python3 not found\nwhich python3\npython3 --version\n\n# Try python (might be Python 2)\npython --version\n</code></pre>"},{"location":"getting-started/installation/#pip-command-not-found","title":"pip command not found","text":"<pre><code># Use python -m pip instead\npython -m pip install pyyaml\n</code></pre>"},{"location":"getting-started/installation/#venv-activation-fails-on-mac","title":"venv activation fails on Mac","text":"<pre><code># Try direct path\nsource ./venv/bin/activate\n\n# Or use Python's venv module directly\npython -m venv venv\nsource venv/bin/activate\n</code></pre>"},{"location":"getting-started/installation/#dbt-debug-fails","title":"dbt debug fails","text":"<pre><code># Make sure you're in the dbt directory\ncd lab1_sales_performance/dbt\ndbt debug\n\n# If still fails, check dbt installation\npip show dbt-core dbt-duckdb\n</code></pre>"},{"location":"getting-started/installation/#permission-denied-on-setupsh","title":"Permission denied on setup.sh","text":"<pre><code>chmod +x setup.sh\n./setup.sh\n</code></pre>"},{"location":"getting-started/installation/#post-installation","title":"Post-Installation","text":"<p>After successful installation:</p> <ol> <li>Read Quick Start</li> <li>Read Architecture Overview</li> <li>Try your</li> </ol>"},{"location":"getting-started/quick-start/","title":"Quick Start Guide","text":"<p>Get up and running with BuildCPG Labs in 10 minutes.</p>"},{"location":"getting-started/quick-start/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11+</li> <li>Mac 11+ or Linux</li> <li>Git</li> <li>Basic terminal knowledge</li> </ul>"},{"location":"getting-started/quick-start/#installation","title":"Installation","text":""},{"location":"getting-started/quick-start/#1-clone-repository","title":"1. Clone Repository","text":"<pre><code>git clone https://github.com/narensham/buildcpg-labs.git\ncd buildcpg-labs\n</code></pre>"},{"location":"getting-started/quick-start/#2-install-dependencies","title":"2. Install Dependencies","text":"<pre><code>pip install pyyaml duckdb pandas\n</code></pre>"},{"location":"getting-started/quick-start/#3-verify-setup","title":"3. Verify Setup","text":"<pre><code>python config/paths.py\n# Output: \u2705 Lab1 config loaded: ...\n</code></pre>"},{"location":"getting-started/quick-start/#your-first-lab-run","title":"Your First Lab Run","text":""},{"location":"getting-started/quick-start/#setup-lab1-one-time","title":"Setup Lab1 (One Time)","text":"<pre><code>cd lab1_sales_performance\nmake setup\n# Creates venv, installs dependencies, runs dbt debug\n</code></pre>"},{"location":"getting-started/quick-start/#run-the-pipeline","title":"Run the Pipeline","text":"<pre><code>make run\n# Executes: dbt run\n</code></pre>"},{"location":"getting-started/quick-start/#inspect-your-data","title":"Inspect Your Data","text":"<pre><code>make inspect\n# Shows: schemas, tables, row counts, quality scores\n</code></pre>"},{"location":"getting-started/quick-start/#run-tests","title":"Run Tests","text":"<pre><code>make test\n# Executes: dbt test\n</code></pre>"},{"location":"getting-started/quick-start/#create-a-new-lab","title":"Create a New Lab","text":""},{"location":"getting-started/quick-start/#bootstrap-lab2","title":"Bootstrap Lab2","text":"<pre><code>cd ..\n./setup_new_lab.sh lab2_forecast_model\n</code></pre>"},{"location":"getting-started/quick-start/#use-lab2-immediately","title":"Use Lab2 Immediately","text":"<pre><code>cd lab2_forecast_model\nmake run\n</code></pre>"},{"location":"getting-started/quick-start/#common-commands","title":"Common Commands","text":""},{"location":"getting-started/quick-start/#working-on-lab1","title":"Working on Lab1","text":"<pre><code>cd lab1_sales_performance\n\nmake setup      # Initialize lab (one time)\nmake run        # Run dbt pipeline\nmake test       # Run dbt tests\nmake inspect    # Check data quality\nmake clean      # Clean build artifacts\nmake docs       # Generate dbt documentation\n</code></pre>"},{"location":"getting-started/quick-start/#across-all-labs","title":"Across All Labs","text":"<pre><code># Check all labs\nls -la | grep lab\n\n# Status of each lab\ncat config/labs_config.yaml\n\n# Check paths\npython config/paths.py\n</code></pre>"},{"location":"getting-started/quick-start/#what-happened","title":"What Happened?","text":"<p>When you ran <code>make setup</code> in lab1:</p> <ol> <li>Created venv - Isolated Python environment</li> <li>Installed dependencies - From requirements.txt</li> <li>Ran dbt debug - Verified dbt setup</li> </ol> <p>When you ran <code>make run</code>:</p> <ol> <li>Activated venv - Automatically (no need for <code>source venv/bin/activate</code>)</li> <li>Changed to dbt directory - <code>cd dbt</code></li> <li>Ran dbt run - Executed transformation pipeline</li> <li>Created/updated tables - In DuckDB database</li> </ol> <p>When you ran <code>make inspect</code>:</p> <ol> <li>Connected to database - lab1_sales_performance.duckdb</li> <li>Listed all schemas - raw, raw_bronze, raw_gold, raw_silver</li> <li>Showed table stats - Row counts, column counts</li> <li>Calculated quality - Checked for nulls, duplicates</li> </ol>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<ul> <li>Read Lab Structure to understand directory layout</li> <li>Read Shared vs Lab-Specific to understand how code is organized</li> <li>Read Creating New Lab for detailed lab creation guide</li> <li>Read Troubleshooting if you hit any issues</li> </ul>"},{"location":"getting-started/quick-start/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/quick-start/#command-not-found-make","title":"\"command not found: make\"","text":"<p>You need to install <code>make</code>. On Mac: <pre><code>xcode-select --install\n</code></pre></p>"},{"location":"getting-started/quick-start/#venv-not-found","title":"\"venv not found\"","text":"<p>Run <code>make setup</code> first to create the virtual environment.</p>"},{"location":"getting-started/quick-start/#dbt-command-not-found","title":"\"dbt: command not found\"","text":"<p>Make sure you ran <code>make setup</code> to install dbt in the venv.</p> <p>For more help, see Troubleshooting.</p>"},{"location":"labs/lab2-architecture/","title":"Lab 2: Market Sentiment Architecture","text":""},{"location":"labs/lab2-architecture/#overview","title":"Overview","text":"<p>The Market Sentiment Analysis Architecture enables near-real-time ingestion, transformation, and aggregation of public sentiment toward Consumer Packaged Goods (CPG) brands.</p> <p>It\u2019s designed around the Medallion Architecture \u2014 Raw \u2192 Bronze \u2192 Silver \u2192 Gold \u2014 and optimized for local-first analytics using DuckDB and dbt.</p>"},{"location":"labs/lab2-architecture/#high-level-data-flow","title":"High-Level Data Flow","text":"<p>```mermaid     flowchart LR     %% Layer titles     subgraph INGESTION[\"\ud83d\udd39 Ingestion Layer\"]         A1[Reddit API]:::source \u2192 B[Python Ingestion(pipelines/ingest_sentiment.py)]:::process         A2[News API]:::source \u2192 B     end</p> <pre><code>subgraph RAW[\"\ud83d\uddc2\ufe0f Raw Data\"]\n    B --&gt; C[Raw CSV Files&lt;br&gt;(data/raw/)]:::storage\nend\n\nsubgraph BRONZE[\"\ud83e\udd49 Bronze Layer\"]\n    C --&gt; D[dbt Staging Models&lt;br&gt;(stg_reddit__*, stg_news__*)]:::bronze\nend\n\nsubgraph SILVER[\"\ud83e\udd48 Silver Layer\"]\n    D --&gt; E[dbt Intermediate Models&lt;br&gt;(int_sentiment_unified)]:::silver\nend\n\nsubgraph GOLD[\"\ud83e\udd47 Gold Layer\"]\n    E --&gt; F[dbt Marts Models&lt;br&gt;(mart_daily_sentiment, fct_sentiment_events)]:::gold\nend\n\nsubgraph ANALYTICS[\"\ud83d\udcca Analytics Layer\"]\n    F --&gt; G[Streamlit / DuckDB Dashboards]:::viz\nend\n\n%% Styling for classes\nclassDef source fill:#e3f2fd,stroke:#1565c0,stroke-width:2px,color:#0d47a1,font-weight:bold;\nclassDef process fill:#fff3e0,stroke:#ef6c00,stroke-width:2px,color:#4e342e,font-weight:bold;\nclassDef storage fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px,color:#1b5e20,font-weight:bold;\nclassDef bronze fill:#fbe9e7,stroke:#bf360c,stroke-width:2px,color:#3e2723,font-weight:bold;\nclassDef silver fill:#eceff1,stroke:#607d8b,stroke-width:2px,color:#263238,font-weight:bold;\nclassDef gold fill:#fff8e1,stroke:#f9a825,stroke-width:2px,color:#5f370e,font-weight:bold;\nclassDef viz fill:#fce4ec,stroke:#ad1457,stroke-width:2px,color:#4a148c,font-weight:bold;\n</code></pre>"},{"location":"labs/lab2-architecture/#architecture-summary","title":"Architecture Summary","text":"Layer Description Example Models Raw External Reddit &amp; News data sources <code>reddit_real.csv</code>, <code>news_real.csv</code> Bronze (Staging) Cleans &amp; standardizes raw data <code>stg_reddit__posts</code>, <code>stg_news__articles</code> Silver (Intermediate) Combines and enriches sources <code>int_sentiment_unified</code> Gold (Marts) Business-ready analytics tables <code>fct_sentiment_events</code>, <code>mart_daily_sentiment</code>"},{"location":"labs/lab2-architecture/#technology-stack","title":"Technology Stack","text":"Component Tool Purpose Database DuckDB Lightweight analytical engine Transformation dbt ELT orchestration and testing Ingestion Python 3.11+ Reddit + News API collection Sentiment Analysis VADER / TextBlob Text polarity scoring Orchestration Manual (Airflow planned) Future automation Visualization Streamlit Interactive dashboard layer"},{"location":"labs/lab2-architecture/#key-features","title":"Key Features","text":"<ul> <li>Unified Sentiment Model: Combines Reddit and News sentiment in one schema.</li> <li>Surrogate Keys: Ensures event-level uniqueness.</li> <li>Categorized Sentiment: Groups scores into positive, neutral, or negative.</li> <li>Incremental Loads: Processes only new data after each run.</li> <li>Data Contracts: Enforces consistent schema and data types.</li> <li>Comprehensive Testing: Uniqueness, nulls, ranges, and accepted values validated.</li> </ul>"},{"location":"labs/lab2-architecture/#data-quality-tests","title":"Data Quality Tests","text":"Type Target Columns Description Unique <code>sentiment_event_id</code>, <code>(sentiment_date, brand)</code> Ensures grain-level uniqueness Not Null <code>brand_key</code>, <code>sentiment_score</code>, <code>published_date</code> Core attributes must exist Range <code>sentiment_score</code> Must be between -1 and 1 Accepted Values <code>quality_flag</code>, <code>anomaly_flag</code> Restricts allowed values <p>All core tests are passing as of the latest dbt run \u2705</p>"},{"location":"labs/lab2-architecture/#example-model-flow","title":"Example Model Flow","text":"<ol> <li> <p>Ingestion: <code>ingest_sentiment.py</code> pulls new posts/articles and stores them under <code>/data/raw/</code></p> </li> <li> <p>Transformation:    dbt staging \u2192 intermediate \u2192 marts flow using DuckDB</p> </li> <li> <p>Analytics:    Aggregate daily sentiment and engagement data for dashboards</p> </li> </ol>"},{"location":"labs/lab2-architecture/#current-status","title":"Current Status","text":"Area Progress Notes Data Ingestion \u2705 Completed Synthetic Reddit + News data dbt Models \u2705 Stable Full medallion flow implemented Data Quality \u2705 All tests passing Custom tests working Dashboard \ud83d\udd04 Planned Streamlit interface next Airflow Integration \ud83d\udd04 Planned Orchestration phase"},{"location":"labs/lab2-architecture/#related-documentation","title":"Related Documentation","text":"<ul> <li>Lab 2 Setup Guide</li> <li>Lab 2 Data Models</li> <li>Lab 2 Troubleshooting</li> <li>Architecture Overview</li> </ul> <p>Author: narensham Last Updated: November 2025 Status: Active Development</p>"},{"location":"labs/lab2-data-models/","title":"Lab 2: Data Models Reference","text":"<p>Complete reference for all data models in the Market Sentiment Analysis lab with schemas, lineage, and examples.</p>"},{"location":"labs/lab2-data-models/#model-hierarchy","title":"Model Hierarchy","text":"<pre><code>graph TB\n    subgraph \"Data Sources\"\n        A[reddit_real.csv]\n        B[news_real.csv]\n    end\n\n    subgraph \"Staging Layer\"\n        C[stg_reddit__posts]\n        D[stg_news__articles]\n    end\n\n    subgraph \"Intermediate Layer\"\n        E[int_sentiment_unified]\n    end\n\n    subgraph \"Mart Layer\"\n        F[fct_sentiment_events]\n        G[mart_daily_sentiment]\n    end\n\n    A --&gt; C\n    B --&gt; D\n    C --&gt; E\n    D --&gt; E\n    E --&gt; F\n    F --&gt; G\n\n    style A fill:#fee2e2,stroke:#dc2626\n    style B fill:#fee2e2,stroke:#dc2626\n    style C fill:#dbeafe,stroke:#2563eb\n    style D fill:#dbeafe,stroke:#2563eb\n    style E fill:#fef3c7,stroke:#f59e0b\n    style F fill:#dcfce7,stroke:#16a34a\n    style G fill:#dcfce7,stroke:#16a34a</code></pre>"},{"location":"labs/lab2-data-models/#architecture-overview","title":"Architecture Overview","text":"<pre><code>flowchart LR\n    subgraph Sources\n        R[Reddit API&lt;br/&gt;~500 records]\n        N[News API&lt;br/&gt;~300 records]\n    end\n\n    subgraph Bronze[\"Bronze Layer (Staging)\"]\n        SR[stg_reddit__posts&lt;br/&gt;VIEW]\n        SN[stg_news__articles&lt;br/&gt;VIEW]\n    end\n\n    subgraph Silver[\"Silver Layer (Intermediate)\"]\n        IU[int_sentiment_unified&lt;br/&gt;TABLE&lt;br/&gt;800 rows]\n    end\n\n    subgraph Gold[\"Gold Layer (Marts)\"]\n        FCT[fct_sentiment_events&lt;br/&gt;INCREMENTAL&lt;br/&gt;Fact Table]\n        MART[mart_daily_sentiment&lt;br/&gt;INCREMENTAL&lt;br/&gt;Aggregate]\n    end\n\n    R --&gt;|CSV| SR\n    N --&gt;|CSV| SN\n    SR --&gt; IU\n    SN --&gt; IU\n    IU --&gt; FCT\n    FCT --&gt; MART\n\n    style Sources fill:#fef3c7\n    style Bronze fill:#dbeafe\n    style Silver fill:#fef9c3\n    style Gold fill:#dcfce7</code></pre>"},{"location":"labs/lab2-data-models/#staging-layer","title":"Staging Layer","text":""},{"location":"labs/lab2-data-models/#stg_reddit__posts","title":"stg_reddit__posts","text":"<p>Purpose: Clean and standardize Reddit post data from raw CSV files.</p> <p>Materialization: View (real-time freshness)</p> <p>Source: <code>data/raw/reddit_real.csv</code></p> <p>Transformations: - Column renaming to snake_case - Type casting for proper data types - Timestamp parsing and validation - Basic null handling</p> <pre><code>graph LR\n    A[reddit_real.csv] --&gt; B[Column Rename]\n    B --&gt; C[Type Casting]\n    C --&gt; D[Timestamp Parse]\n    D --&gt; E[stg_reddit__posts]\n\n    style A fill:#fee2e2\n    style E fill:#dbeafe</code></pre> <p>Schema:</p> Column Type Nullable Description Example post_id VARCHAR No Unique Reddit post identifier <code>reddit_00042</code> author VARCHAR Yes Reddit username <code>user_5234</code> brand VARCHAR No Brand mentioned <code>Coca-Cola</code> title VARCHAR Yes Post title <code>Post about brand 42</code> body VARCHAR Yes Post body text <code>Discussion about CPG...</code> upvotes BIGINT Yes Number of upvotes <code>3452</code> comments_count BIGINT Yes Number of comments <code>127</code> created_at TIMESTAMP No Post creation time <code>2024-11-15 14:23:11</code> sentiment_score DOUBLE No Sentiment (-1 to 1) <code>0.435</code> source VARCHAR No Always 'reddit' <code>reddit</code> ingested_at TIMESTAMP No Ingestion timestamp <code>2025-01-15 10:30:00</code> <p>Row Count: ~500 records</p> <p>Sample SQL: <pre><code>-- View Reddit sentiment by brand\nSELECT \n    brand,\n    COUNT(*) as post_count,\n    AVG(sentiment_score) as avg_sentiment,\n    AVG(upvotes) as avg_upvotes,\n    AVG(comments_count) as avg_comments\nFROM {{ ref('stg_reddit__posts') }}\nGROUP BY brand\nORDER BY avg_sentiment DESC;\n</code></pre></p> <p>Quality Checks: - No NULL post_ids - Valid timestamps (not future dates) - Sentiment scores in valid range</p>"},{"location":"labs/lab2-data-models/#stg_news__articles","title":"stg_news__articles","text":"<p>Purpose: Clean and standardize news article data from raw CSV files.</p> <p>Materialization: View (real-time freshness)</p> <p>Source: <code>data/raw/news_real.csv</code></p> <p>Transformations: - Column renaming to snake_case - Type casting for proper data types - URL validation - Timestamp parsing</p> <pre><code>graph LR\n    A[news_real.csv] --&gt; B[Column Rename]\n    B --&gt; C[Type Casting]\n    C --&gt; D[URL Validate]\n    D --&gt; E[Timestamp Parse]\n    E --&gt; F[stg_news__articles]\n\n    style A fill:#fee2e2\n    style F fill:#dbeafe</code></pre> <p>Schema:</p> Column Type Nullable Description Example article_id VARCHAR No Unique article identifier <code>news_00123</code> publication VARCHAR Yes News source <code>Forbes</code> brand VARCHAR No Brand mentioned <code>PepsiCo</code> headline VARCHAR Yes Article headline <code>News: PepsiCo announces...</code> body VARCHAR Yes Article body <code>Article content...</code> url VARCHAR Yes Article URL <code>https://example.com/article-123</code> published_at TIMESTAMP No Publication time <code>2024-12-01 09:15:00</code> sentiment_score DOUBLE No Sentiment (-1 to 1) <code>-0.234</code> source VARCHAR No Always 'news' <code>news</code> ingested_at TIMESTAMP No Ingestion timestamp <code>2025-01-15 10:30:00</code> <p>Row Count: ~300 records</p> <p>Sample SQL: <pre><code>-- Top publications by sentiment\nSELECT \n    publication,\n    COUNT(*) as article_count,\n    AVG(sentiment_score) as avg_sentiment,\n    MIN(sentiment_score) as min_sentiment,\n    MAX(sentiment_score) as max_sentiment\nFROM {{ ref('stg_news__articles') }}\nGROUP BY publication\nORDER BY article_count DESC;\n</code></pre></p> <p>Quality Checks: - No NULL article_ids - Valid URLs format - Valid timestamps</p>"},{"location":"labs/lab2-data-models/#intermediate-layer","title":"Intermediate Layer","text":""},{"location":"labs/lab2-data-models/#int_sentiment_unified","title":"int_sentiment_unified","text":"<p>Purpose: Unified sentiment data with business logic, enrichment, and quality validation.</p> <p>Materialization: Table (full refresh)</p> <p>Sources:  - {{ ref('stg_reddit__posts') }} - {{ ref('stg_news__articles') }}</p> <p>Key Features: - \ud83d\udd11 Surrogate key generation for unique identification - \ud83d\udcca Sentiment categorization (positive/negative/neutral) - \ud83d\udcc8 Engagement metrics and percentiles - \ud83d\udcc9 7-day moving averages - \u2705 Quality validation and flags - \ud83d\udd04 Window functions for trends</p> <pre><code>graph TB\n    A[stg_reddit__posts] --&gt; B[Union]\n    C[stg_news__articles] --&gt; B\n    B --&gt; D[Generate Surrogate Keys]\n    D --&gt; E[Sentiment Categorization]\n    E --&gt; F[Engagement Metrics]\n    F --&gt; G[Moving Averages]\n    G --&gt; H[Quality Validation]\n    H --&gt; I[int_sentiment_unified]\n\n    style I fill:#fef3c7</code></pre> <p>Schema:</p> Column Type Description Business Logic Keys &amp; Identifiers sentiment_event_id VARCHAR Unique event identifier <code>md5(content_id + published_at + source)</code> brand_key VARCHAR Brand surrogate key <code>md5(brand)</code> source_key INTEGER Source identifier <code>1=reddit, 2=news</code> content_id VARCHAR Source-specific ID From source system content_hash VARCHAR Content hash For deduplication Core Data creator VARCHAR Author/publication Username or publication name brand VARCHAR Brand name <code>Coca-Cola</code>, <code>PepsiCo</code>, etc headline VARCHAR Title/headline Post title or article headline body_text VARCHAR Full content Complete text content Sentiment Metrics sentiment_score DOUBLE Sentiment score <code>-1</code> to <code>1</code> range sentiment_category VARCHAR Category <code>positive/negative/neutral</code> Engagement Metrics engagement_count BIGINT Engagement metric Upvotes/shares/likes engagement_percentile DOUBLE Percentile rank <code>0.0</code> to <code>1.0</code> post_rank_by_brand BIGINT Rank within brand By engagement DESC Temporal Data published_at TIMESTAMP Publication time When content was published source VARCHAR Source platform <code>reddit</code> or <code>news</code> ingested_at TIMESTAMP Ingestion time When data was loaded Trend Analysis moving_avg_sentiment_7d DOUBLE 7-day MA Rolling average sentiment sentiment_change DOUBLE Change from previous Sentiment delta Metadata _dbt_load_date TIMESTAMP dbt load timestamp When model ran _dbt_run_timestamp VARCHAR dbt run identifier Run batch ID <p>Row Count: ~800 records (Reddit + News combined)</p> <p>Business Logic Implementation:</p>"},{"location":"labs/lab2-data-models/#1-sentiment-categorization","title":"1. Sentiment Categorization","text":"<p><pre><code>CASE \n    WHEN sentiment_score &gt;= {{ var('sentiment_threshold_positive') }} THEN 'positive'\n    WHEN sentiment_score &lt;= {{ var('sentiment_threshold_negative') }} THEN 'negative'\n    ELSE 'neutral'\nEND as sentiment_category\n</code></pre> Thresholds: <code>positive &gt;= 0.3</code>, <code>negative &lt;= -0.3</code></p>"},{"location":"labs/lab2-data-models/#2-engagement-percentile","title":"2. Engagement Percentile","text":"<pre><code>ROUND(\n    (engagement_count) / \n    NULLIF(MAX(engagement_count) OVER (), 0),\n    3\n) as engagement_percentile\n</code></pre>"},{"location":"labs/lab2-data-models/#3-brand-ranking","title":"3. Brand Ranking","text":"<pre><code>ROW_NUMBER() OVER (\n    PARTITION BY brand \n    ORDER BY engagement_count DESC\n) as post_rank_by_brand\n</code></pre>"},{"location":"labs/lab2-data-models/#4-moving-average-7-day","title":"4. Moving Average (7-day)","text":"<pre><code>AVG(sentiment_score) OVER (\n    PARTITION BY brand \n    ORDER BY published_at \n    ROWS BETWEEN 6 PRECEDING AND CURRENT ROW\n) as moving_avg_sentiment_7d\n</code></pre>"},{"location":"labs/lab2-data-models/#5-sentiment-change","title":"5. Sentiment Change","text":"<pre><code>sentiment_score - LAG(sentiment_score) OVER (\n    PARTITION BY brand \n    ORDER BY published_at\n) as sentiment_change\n</code></pre> <p>Data Flow Diagram:</p> <pre><code>sequenceDiagram\n    participant S as Staging\n    participant U as Union\n    participant K as Keys\n    participant B as Business Logic\n    participant Q as Quality\n    participant I as int_sentiment_unified\n\n    S-&gt;&gt;U: Reddit + News data\n    U-&gt;&gt;K: Generate surrogate keys\n    K-&gt;&gt;B: Add sentiment categories\n    B-&gt;&gt;B: Calculate engagement metrics\n    B-&gt;&gt;B: Compute moving averages\n    B-&gt;&gt;B: Calculate trends\n    B-&gt;&gt;Q: Validate quality\n    Q-&gt;&gt;I: Store validated data</code></pre> <p>Quality Filters: - \u2705 Sentiment score between -1 and 1 - \u2705 No future dates - \u2705 No NULL required fields - \u2705 Valid brand names</p> <p>Sample Queries:</p> <pre><code>-- Top performing content by brand\nSELECT \n    brand,\n    headline,\n    sentiment_score,\n    engagement_count,\n    engagement_percentile,\n    post_rank_by_brand\nFROM {{ ref('int_sentiment_unified') }}\nWHERE post_rank_by_brand &lt;= 10\nORDER BY brand, post_rank_by_brand;\n\n-- Sentiment trends over time\nSELECT \n    brand,\n    DATE_TRUNC('day', published_at) as date,\n    AVG(sentiment_score) as daily_sentiment,\n    AVG(moving_avg_sentiment_7d) as trend_7d,\n    COUNT(*) as content_count\nFROM {{ ref('int_sentiment_unified') }}\nGROUP BY brand, DATE_TRUNC('day', published_at)\nORDER BY brand, date;\n\n-- Sentiment distribution\nSELECT \n    sentiment_category,\n    COUNT(*) as count,\n    ROUND(AVG(sentiment_score), 3) as avg_score,\n    ROUND(AVG(engagement_count), 0) as avg_engagement\nFROM {{ ref('int_sentiment_unified') }}\nGROUP BY sentiment_category\nORDER BY avg_score DESC;\n</code></pre>"},{"location":"labs/lab2-data-models/#mart-layer","title":"Mart Layer","text":""},{"location":"labs/lab2-data-models/#fct_sentiment_events","title":"fct_sentiment_events","text":"<p>Purpose: Fact table for all sentiment events (granular, event-level data).</p> <p>Materialization: Incremental (delete+insert strategy)</p> <p>Grain: One row per sentiment event (Reddit post or news article)</p> <p>Unique Key: <code>sentiment_event_id</code></p> <p>Source: {{ ref('int_sentiment_unified') }}</p> <pre><code>graph TB\n    subgraph \"Incremental Logic\"\n        A[int_sentiment_unified] --&gt; B{is_incremental?}\n        B --&gt;|Yes| C[Filter: published_at &gt; MAX]\n        B --&gt;|No| D[Load all data]\n        C --&gt; E[Filter: quality_flag = VALID]\n        D --&gt; E\n        E --&gt; F[fct_sentiment_events]\n    end\n\n    style F fill:#dcfce7,stroke:#16a34a</code></pre> <p>Schema:</p> Column Type Contract Description Primary Key sentiment_event_id VARCHAR \u2705 NOT NULL, UNIQUE Unique event identifier Foreign Keys brand_key VARCHAR \u2705 NOT NULL Brand dimension key source_key INTEGER Source identifier (1=reddit, 2=news) Descriptive Attributes content_id VARCHAR Source-specific content ID creator VARCHAR Author or publication name brand VARCHAR Brand name headline VARCHAR Post title or article headline body_text VARCHAR Full content text source VARCHAR Platform (reddit/news) Metrics engagement_count BIGINT Upvotes, shares, likes sentiment_score DOUBLE \u2705 NOT NULL, [-1,1] Sentiment score sentiment_category VARCHAR positive/negative/neutral Temporal Dimensions published_at TIMESTAMP Original publish time published_date DATE \u2705 NOT NULL Publish date (for partitioning) published_year BIGINT Year (for aggregation) published_month BIGINT Month 1-12 published_day_of_week BIGINT Day 0-6 (0=Monday) published_hour BIGINT Hour 0-23 ingested_at TIMESTAMP Data ingestion time Quality &amp; Metadata quality_flag VARCHAR \u2705 Accepted values VALID/INVALID_SENTIMENT/NULL_HEADLINE _dbt_updated_at TIMESTAMP Last update time _dbt_run_timestamp VARCHAR dbt run identifier <p>Row Count: ~800 records (only VALID quality_flag)</p> <p>Incremental Logic:</p> <pre><code>WITH base_data AS (\n    SELECT *,\n        CASE \n            WHEN NOT {{ validate_sentiment('sentiment_score') }} THEN 'INVALID_SENTIMENT'\n            WHEN headline IS NULL THEN 'NULL_HEADLINE'\n            ELSE 'VALID'\n        END as quality_flag\n    FROM {{ ref('int_sentiment_unified') }}\n\n    {% if is_incremental() %}\n    WHERE published_at &gt; (SELECT MAX(published_at) FROM {{ this }})\n    {% endif %}\n)\n\nSELECT * FROM base_data\nWHERE quality_flag = 'VALID'\n</code></pre> <p>Data Quality Tests:</p> <pre><code>graph LR\n    A[fct_sentiment_events] --&gt; B[Uniqueness Test]\n    A --&gt; C[Not Null Tests]\n    A --&gt; D[Range Tests]\n    A --&gt; E[Accepted Values]\n    A --&gt; F[Custom Logic]\n\n    B --&gt; B1[sentiment_event_id UNIQUE]\n    C --&gt; C1[sentiment_event_id NOT NULL]\n    C --&gt; C2[brand_key NOT NULL]\n    C --&gt; C3[sentiment_score NOT NULL]\n    C --&gt; C4[published_date NOT NULL]\n    D --&gt; D1[sentiment_score BETWEEN -1 AND 1]\n    D --&gt; D2[No future dates]\n    E --&gt; E1[quality_flag values]\n    F --&gt; F1[Business validations]\n\n    style A fill:#dcfce7</code></pre> <p>Sample Queries:</p> <pre><code>-- Daily sentiment by brand\nSELECT \n    published_date,\n    brand,\n    COUNT(*) as event_count,\n    ROUND(AVG(sentiment_score), 3) as avg_sentiment,\n    SUM(engagement_count) as total_engagement\nFROM {{ ref('fct_sentiment_events') }}\nGROUP BY published_date, brand\nORDER BY published_date DESC, brand;\n\n-- Sentiment distribution\nSELECT \n    sentiment_category,\n    COUNT(*) as count,\n    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as pct,\n    ROUND(AVG(sentiment_score), 3) as avg_score,\n    ROUND(AVG(engagement_count), 0) as avg_engagement\nFROM {{ ref('fct_sentiment_events') }}\nGROUP BY sentiment_category\nORDER BY count DESC;\n\n-- Top brands by positive sentiment\nSELECT \n    brand,\n    COUNT(*) as total_mentions,\n    SUM(CASE WHEN sentiment_category = 'positive' THEN 1 ELSE 0 END) as positive_mentions,\n    ROUND(AVG(sentiment_score), 3) as avg_sentiment,\n    ROUND(AVG(engagement_count), 0) as avg_engagement\nFROM {{ ref('fct_sentiment_events') }}\nGROUP BY brand\nORDER BY avg_sentiment DESC;\n\n-- Temporal patterns (hourly)\nSELECT \n    published_hour,\n    COUNT(*) as event_count,\n    ROUND(AVG(sentiment_score), 3) as avg_sentiment\nFROM {{ ref('fct_sentiment_events') }}\nGROUP BY published_hour\nORDER BY published_hour;\n</code></pre>"},{"location":"labs/lab2-data-models/#mart_daily_sentiment","title":"mart_daily_sentiment","text":"<p>Purpose: Daily aggregated sentiment metrics by brand with anomaly detection.</p> <p>Materialization: Incremental (delete+insert strategy)</p> <p>Grain: One row per (date, brand) combination</p> <p>Unique Key: ['sentiment_date', 'brand']</p> <p>Source: {{ ref('fct_sentiment_events') }}</p> <pre><code>graph TB\n    A[fct_sentiment_events] --&gt; B[Group by date, brand]\n    B --&gt; C[Calculate Aggregates]\n    C --&gt; D[Compute Z-Scores]\n    D --&gt; E[Flag Anomalies]\n    E --&gt; F[mart_daily_sentiment]\n\n    subgraph \"Aggregations\"\n        C --&gt; C1[Avg/Min/Max Sentiment]\n        C --&gt; C2[Content Counts]\n        C --&gt; C3[Engagement Metrics]\n        C --&gt; C4[Source Diversity]\n    end\n\n    subgraph \"Anomaly Detection\"\n        D --&gt; D1[Z-Score &gt; 2]\n        D1 --&gt; E\n    end\n\n    style F fill:#dcfce7,stroke:#16a34a</code></pre> <p>Schema:</p> Column Type Contract Description Calculation Dimensions sentiment_date DATE \u2705 NOT NULL Aggregation date <code>CAST(published_date AS DATE)</code> brand VARCHAR Brand name Dimension Sentiment Metrics avg_sentiment DOUBLE \u2705 [-1,1] Average sentiment <code>AVG(sentiment_score)</code> min_sentiment DOUBLE Minimum sentiment <code>MIN(sentiment_score)</code> max_sentiment DOUBLE Maximum sentiment <code>MAX(sentiment_score)</code> stddev_sentiment DOUBLE Standard deviation <code>STDDEV(sentiment_score)</code> Content Counts content_count BIGINT Total pieces of content <code>COUNT(*)</code> positive_count BIGINT Positive content count <code>SUM(CASE category = 'positive')</code> negative_count BIGINT Negative content count <code>SUM(CASE category = 'negative')</code> neutral_count BIGINT Neutral content count <code>SUM(CASE category = 'neutral')</code> positive_ratio DOUBLE Ratio of positive <code>positive_count / content_count</code> Engagement total_engagement HUGEINT Sum of all engagement <code>SUM(engagement_count)</code> avg_engagement DOUBLE Average engagement <code>AVG(engagement_count)</code> Diversity source_count BIGINT Unique sources <code>COUNT(DISTINCT source)</code> Anomaly Detection z_score_sentiment DOUBLE Statistical z-score <code>(avg - brand_avg) / brand_stddev</code> anomaly_flag VARCHAR \u2705 Accepted values NORMAL/ANOMALY <code>ABS(z_score) &gt; 2</code> Metadata mart_load_date TIMESTAMP Mart load time <code>get_current_timestamp()</code> <p>Aggregation Logic:</p> <pre><code>SELECT\n    CAST(published_date AS DATE) as sentiment_date,\n    brand,\n\n    -- Sentiment metrics\n    AVG(sentiment_score) as avg_sentiment,\n    MIN(sentiment_score) as min_sentiment,\n    MAX(sentiment_score) as max_sentiment,\n    STDDEV(sentiment_score) as stddev_sentiment,\n\n    -- Content counts\n    COUNT(*) as content_count,\n    SUM(CASE WHEN sentiment_category = 'positive' THEN 1 ELSE 0 END) as positive_count,\n    SUM(CASE WHEN sentiment_category = 'negative' THEN 1 ELSE 0 END) as negative_count,\n    SUM(CASE WHEN sentiment_category = 'neutral' THEN 1 ELSE 0 END) as neutral_count,\n    positive_count::DOUBLE / NULLIF(content_count, 0) as positive_ratio,\n\n    -- Engagement\n    SUM(engagement_count) as total_engagement,\n    AVG(engagement_count) as avg_engagement,\n\n    -- Diversity\n    COUNT(DISTINCT source) as source_count,\n\n    -- Anomaly detection\n    (avg_sentiment - AVG(avg_sentiment) OVER (PARTITION BY brand)) /\n    NULLIF(STDDEV(avg_sentiment) OVER (PARTITION BY brand), 0) as z_score_sentiment,\n\n    CASE\n        WHEN ABS(z_score_sentiment) &gt; 2 THEN 'ANOMALY'\n        ELSE 'NORMAL'\n    END as anomaly_flag,\n\n    get_current_timestamp() as mart_load_date\n\nFROM {{ ref('fct_sentiment_events') }}\nGROUP BY sentiment_date, brand\n</code></pre> <p>Anomaly Detection Visualization:</p> <pre><code>graph LR\n    A[Daily Sentiment] --&gt; B[Calculate Brand Mean]\n    A --&gt; C[Calculate Brand StdDev]\n    B --&gt; D[Compute Z-Score]\n    C --&gt; D\n    D --&gt; E{|Z-Score| &gt; 2?}\n    E --&gt;|Yes| F[ANOMALY]\n    E --&gt;|No| G[NORMAL]\n\n    style F fill:#fee2e2,stroke:#dc2626\n    style G fill:#dcfce7,stroke:#16a34a</code></pre> <p>Incremental Logic:</p> <pre><code>{% if is_incremental() %}\n    WHERE sentiment_date &gt; (SELECT MAX(sentiment_date) FROM {{ this }})\n{% endif %}\n</code></pre> <p>Sample Queries:</p> <pre><code>-- Recent anomalies\nSELECT \n    sentiment_date,\n    brand,\n    ROUND(avg_sentiment, 3) as avg_sentiment,\n    ROUND(z_score_sentiment, 2) as z_score,\n    anomaly_flag,\n    content_count\nFROM {{ ref('mart_daily_sentiment') }}\nWHERE anomaly_flag = 'ANOMALY'\n    AND sentiment_date &gt;= CURRENT_DATE - INTERVAL '7 days'\nORDER BY sentiment_date DESC, ABS(z_score_sentiment) DESC;\n\n-- Brand comparison\nSELECT \n    brand,\n    ROUND(AVG(avg_sentiment), 3) as overall_sentiment,\n    ROUND(AVG(positive_ratio), 3) as positive_ratio,\n    SUM(content_count) as total_mentions,\n    ROUND(AVG(avg_engagement), 0) as avg_engagement,\n    SUM(CASE WHEN anomaly_flag = 'ANOMALY' THEN 1 ELSE 0 END) as anomaly_days\nFROM {{ ref('mart_daily_sentiment') }}\nGROUP BY brand\nORDER BY overall_sentiment DESC;\n\n-- Trend analysis (7-day)\nSELECT \n    sentiment_date,\n    brand,\n    ROUND(avg_sentiment, 3) as sentiment,\n    ROUND(LAG(avg_sentiment, 7) OVER (PARTITION BY brand ORDER BY sentiment_date), 3) as sentiment_7d_ago,\n    ROUND(avg_sentiment - LAG(avg_sentiment, 7) OVER (PARTITION BY brand ORDER BY sentiment_date), 3) as change_7d\nFROM {{ ref('mart_daily_sentiment') }}\nWHERE sentiment_date &gt;= CURRENT_DATE - INTERVAL '30 days'\nORDER BY brand, sentiment_date;\n\n-- Weekly summary\nSELECT \n    DATE_TRUNC('week', sentiment_date) as week_start,\n    brand,\n    ROUND(AVG(avg_sentiment), 3) as weekly_sentiment,\n    SUM(content_count) as weekly_mentions,\n    ROUND(AVG(positive_ratio), 3) as avg_positive_ratio\nFROM {{ ref('mart_daily_sentiment') }}\nGROUP BY DATE_TRUNC('week', sentiment_date), brand\nORDER BY week_start DESC, brand;\n</code></pre>"},{"location":"labs/lab2-data-models/#macros","title":"Macros","text":""},{"location":"labs/lab2-data-models/#generate_surrogate_key","title":"generate_surrogate_key","text":"<p>Purpose: Generate MD5 hash-based surrogate keys for unique identification.</p> <p>Location: <code>dbt/macros/generate_surrogate_key.sql</code></p> <p>Usage: <pre><code>{{ generate_surrogate_key(['column1', 'column2', 'column3']) }}\n</code></pre></p> <p>Implementation: <pre><code>{% macro generate_surrogate_key(columns) %}\n    md5(concat(\n        {% for col in columns %}\n            coalesce(cast({{ col }} as varchar), '')\n            {%- if not loop.last %},{% endif %}\n        {% endfor %}\n    ))\n{% endmacro %}\n</code></pre></p> <p>Example: <pre><code>-- Generate unique event ID\n{{ generate_surrogate_key(['content_id', 'published_at', 'source']) }} as sentiment_event_id\n</code></pre></p>"},{"location":"labs/lab2-data-models/#validate_sentiment","title":"validate_sentiment","text":"<p>Purpose: Validate sentiment scores are within acceptable range.</p> <p>Location: <code>dbt/macros/validate_sentiment.sql</code></p> <p>Usage: <pre><code>WHERE {{ validate_sentiment('sentiment_score') }}\n</code></pre></p> <p>Implementation: <pre><code>{% macro validate_sentiment(column_name) %}\n    ({{ column_name }} &gt;= -1 AND {{ column_name }} &lt;= 1)\n{% endmacro %}\n</code></pre></p>"},{"location":"labs/lab2-data-models/#get_current_timestamp","title":"get_current_timestamp","text":"<p>Purpose: Get current timestamp consistently across models.</p> <p>Location: <code>dbt/macros/get_current_timestamp.sql</code></p> <p>Usage: <pre><code>get_current_timestamp() as load_timestamp\n</code></pre></p>"},{"location":"labs/lab2-data-models/#data-lineage-complete-view","title":"Data Lineage Complete View","text":"<pre><code>graph TB\n    subgraph \"Raw Data\"\n        R[reddit_real.csv&lt;br/&gt;500 rows]\n        N[news_real.csv&lt;br/&gt;300 rows]\n    end\n\n    subgraph \"Bronze - Staging\"\n        SR[stg_reddit__posts&lt;br/&gt;VIEW&lt;br/&gt;500 rows]\n        SN[stg_news__articles&lt;br/&gt;VIEW&lt;br/&gt;300 rows]\n    end\n\n    subgraph \"Silver - Intermediate\"\n        IU[int_sentiment_unified&lt;br/&gt;TABLE&lt;br/&gt;800 rows&lt;br/&gt;+ Surrogate Keys&lt;br/&gt;+ Business Logic&lt;br/&gt;+ Quality Flags]\n    end\n\n    subgraph \"Gold - Marts\"\n        FCT[fct_sentiment_events&lt;br/&gt;INCREMENTAL&lt;br/&gt;~800 rows&lt;br/&gt;Grain: One per event&lt;br/&gt;Key: sentiment_event_id]\n        MART[mart_daily_sentiment&lt;br/&gt;INCREMENTAL&lt;br/&gt;~varies&lt;br/&gt;Grain: One per date-brand&lt;br/&gt;Key: [date, brand]]\n    end\n\n    subgraph \"Tests\"\n        T1[14 Data Quality Tests&lt;br/&gt;\u2705 All Passing]\n    end\n\n    R --&gt;|Clean &amp; Type| SR\n    N --&gt;|Clean &amp; Type| SN\n    SR --&gt;|UNION ALL| IU\n    SN --&gt;|UNION ALL| IU\n    IU --&gt;|Filter VALID| FCT\n    FCT --&gt;|Aggregate Daily| MART\n    FCT -.-&gt;|Validate| T1\n    MART -.-&gt;|Validate| T1\n\n    style R fill:#fee2e2\n    style N fill:#fee2e2\n    style SR fill:#dbeafe\n    style SN fill:#dbeafe\n    style IU fill:#fef3c7\n    style FCT fill:#dcfce7\n    style MART fill:#dcfce7\n    style T1 fill:#e0e7ff</code></pre>"},{"location":"labs/lab2-data-models/#model-execution-order","title":"Model Execution Order","text":"<p>dbt automatically determines execution order based on dependencies:</p> <pre><code>graph LR\n    A[1. Staging Models&lt;br/&gt;Parallel] --&gt; B[2. Intermediate&lt;br/&gt;Serial]\n    B --&gt; C[3. Mart Models&lt;br/&gt;Parallel]\n    C --&gt; D[4. Tests&lt;br/&gt;Parallel]\n\n    style A fill:#dbeafe\n    style B fill:#fef3c7\n    style C fill:#dcfce7\n    style D fill:#e0e7ff</code></pre> <p>Execution Time: ~2-3 seconds for full build with sample data</p>"},{"location":"labs/lab2-data-models/#model-statistics","title":"Model Statistics","text":"Model Type Rows Columns Tests Build Time stg_reddit__posts View 500 11 0 &lt;0.1s stg_news__articles View 300 10 0 &lt;0.1s int_sentiment_unified Table 800 23 0 0.4s fct_sentiment_events Incremental ~800 20 8 0.3s mart_daily_sentiment Incremental varies 14 6 0.2s"},{"location":"labs/lab2-data-models/#query-performance-tips","title":"Query Performance Tips","text":""},{"location":"labs/lab2-data-models/#optimize-staging-queries","title":"Optimize Staging Queries","text":"<pre><code>-- Use WHERE clauses early\nSELECT *\nFROM {{ ref('stg_reddit__posts') }}\nWHERE published_at &gt;= CURRENT_DATE - INTERVAL '30 days'  -- Filter early\n    AND brand IN ('Coca-Cola', 'PepsiCo');  -- Reduce data volume\n</code></pre>"},{"location":"labs/lab2-data-models/#leverage-incremental-models","title":"Leverage Incremental Models","text":"<pre><code>-- Query only recent data\nSELECT *\nFROM {{ ref('fct_sentiment_events') }}\nWHERE published_date &gt;= CURRENT_DATE - INTERVAL '7 days';\n</code></pre>"},{"location":"labs/lab2-data-models/#use-aggregates-for-dashboards","title":"Use Aggregates for Dashboards","text":"<pre><code>-- Use pre-aggregated mart instead of raw events\nSELECT *\nFROM {{ ref('mart_daily_sentiment') }}\nWHERE sentiment_date &gt;= CURRENT_DATE - INTERVAL '30 days';\n-- Much faster than aggregating fct_sentiment_events\n</code></pre>"},{"location":"labs/lab2-data-models/#data-quality-summary","title":"Data Quality Summary","text":"<pre><code>pie title Test Coverage by Category\n    \"Uniqueness\" : 2\n    \"Not Null\" : 5\n    \"Range Validation\" : 2\n    \"Accepted Values\" : 2\n    \"Custom Logic\" : 3</code></pre> <p>Test Results: \u2705 14/14 Passing</p>"},{"location":"labs/lab2-data-models/#model-relationships","title":"Model Relationships","text":"<pre><code>erDiagram\n    FCT_SENTIMENT_EVENTS ||--o{ MART_DAILY_SENTIMENT : aggregates\n    FCT_SENTIMENT_EVENTS {\n        varchar sentiment_event_id PK\n        varchar brand_key FK\n        integer source_key FK\n        double sentiment_score\n        date published_date\n    }\n    MART_DAILY_SENTIMENT {\n        date sentiment_date PK\n        varchar brand PK\n        double avg_sentiment\n        bigint content_count\n        varchar anomaly_flag\n    }</code></pre>"},{"location":"labs/lab2-data-models/#sentiment-distribution","title":"Sentiment Distribution","text":"<pre><code>graph TD\n    A[All Events&lt;br/&gt;800 total] --&gt; B{Sentiment Score}\n    B --&gt;|&gt;= 0.3| C[Positive&lt;br/&gt;~33%]\n    B --&gt;|Between| D[Neutral&lt;br/&gt;~33%]\n    B --&gt;|&lt;= -0.3| E[Negative&lt;br/&gt;~33%]\n\n    style C fill:#dcfce7,stroke:#16a34a\n    style D fill:#fef3c7,stroke:#f59e0b\n    style E fill:#fee2e2,stroke:#dc2626</code></pre>"},{"location":"labs/lab2-data-models/#related-documentation","title":"Related Documentation","text":"<ul> <li>Lab 2 Overview - Architecture and features</li> <li>Lab 2 Setup - Installation guide</li> <li>Lab 2 Troubleshooting - Common issues</li> <li>Lab 2 Quick Reference - Common commands</li> <li>dbt Documentation - Official dbt docs</li> </ul> <p>Last Updated: November 2025 Maintained By: narensham Total Models: 5 Total Tests: 14 Status: \u2705 Production Ready</p>"},{"location":"labs/lab2-overview/","title":"Lab 2: Market Sentiment Analysis","text":""},{"location":"labs/lab2-overview/#overview","title":"Overview","text":"<p>Lab 2 implements a real-time market sentiment analysis pipeline for Consumer Packaged Goods (CPG) brands. It ingests data from Reddit and news sources, analyzes sentiment, and provides daily aggregated metrics for brand reputation monitoring.</p>"},{"location":"labs/lab2-overview/#purpose","title":"Purpose","text":"<p>Monitor and analyze public sentiment towards CPG brands to: - Track brand reputation trends over time - Identify sentiment anomalies requiring attention - Compare sentiment across multiple brands - Understand engagement patterns and content performance</p>"},{"location":"labs/lab2-overview/#architecture","title":"Architecture","text":""},{"location":"labs/lab2-overview/#data-flow","title":"Data Flow","text":"<pre><code>Raw Data Sources\n    \u2193\nReddit API + News API\n    \u2193\nPython Ingestion (pipelines/ingest_sentiment.py)\n    \u2193\nCSV Files (data/raw/)\n    \u2193\ndbt Staging Layer (stg_reddit__posts, stg_news__articles)\n    \u2193\ndbt Intermediate Layer (int_sentiment_unified)\n    \u2193\ndbt Marts Layer (fct_sentiment_events, mart_daily_sentiment)\n    \u2193\nAnalytics &amp; Dashboards\n</code></pre>"},{"location":"labs/lab2-overview/#technology-stack","title":"Technology Stack","text":"Component Technology Database DuckDB 0.9.1 Transformation dbt 1.7.0 Data Quality dbt tests + dbt_expectations Ingestion Python 3.11+ Sentiment Analysis VADER / TextBlob (planned) Orchestration Manual (Airflow planned)"},{"location":"labs/lab2-overview/#data-models","title":"Data Models","text":""},{"location":"labs/lab2-overview/#staging-layer","title":"Staging Layer","text":"<p><code>stg_reddit__posts</code> - Raw Reddit post data - Cleaned column names - Basic type casting - Source: <code>data/raw/reddit_real.csv</code></p> <p><code>stg_news__articles</code> - Raw news article data - Cleaned column names - Basic type casting - Source: <code>data/raw/news_real.csv</code></p>"},{"location":"labs/lab2-overview/#intermediate-layer","title":"Intermediate Layer","text":"<p><code>int_sentiment_unified</code> - Unified Reddit + News data - Surrogate keys generated - Sentiment categorization (positive/negative/neutral) - Engagement metrics - 7-day moving averages - Quality flags</p>"},{"location":"labs/lab2-overview/#marts-layer","title":"Marts Layer","text":"<p><code>fct_sentiment_events</code> (Fact Table) - Grain: One row per sentiment event - Unique key: <code>sentiment_event_id</code> - Incremental materialization - Contract enforcement for data quality - Columns: 20+ including sentiment scores, engagement, temporal fields</p> <p><code>mart_daily_sentiment</code> (Aggregate Table) - Grain: One row per (date, brand) - Daily sentiment aggregates - Anomaly detection (z-score based) - Content counts by sentiment category - Engagement metrics - Incremental materialization</p>"},{"location":"labs/lab2-overview/#key-features","title":"Key Features","text":""},{"location":"labs/lab2-overview/#1-surrogate-key-generation","title":"1. Surrogate Key Generation","text":"<p>Ensures unique event identification: <pre><code>generate_surrogate_key(['content_id', 'published_at', 'source'])\n</code></pre></p>"},{"location":"labs/lab2-overview/#2-sentiment-categorization","title":"2. Sentiment Categorization","text":"<p>Classifies sentiment into actionable categories: - Positive: Score \u2265 0.3 - Negative: Score \u2264 -0.3 - Neutral: -0.3 &lt; Score &lt; 0.3</p>"},{"location":"labs/lab2-overview/#3-quality-flags","title":"3. Quality Flags","text":"<p>Tracks data quality issues: - <code>VALID</code> - Clean, usable data - <code>INVALID_SENTIMENT</code> - Sentiment score out of range - <code>NULL_HEADLINE</code> - Missing headline text</p>"},{"location":"labs/lab2-overview/#4-anomaly-detection","title":"4. Anomaly Detection","text":"<p>Statistical anomaly flagging using z-scores: - <code>NORMAL</code> - Within expected range - <code>ANOMALY</code> - 2+ standard deviations from mean</p>"},{"location":"labs/lab2-overview/#5-incremental-processing","title":"5. Incremental Processing","text":"<p>Only processes new data on subsequent runs: <pre><code>WHERE published_at &gt; (SELECT MAX(published_at) FROM {{ this }})\n</code></pre></p>"},{"location":"labs/lab2-overview/#data-quality","title":"Data Quality","text":""},{"location":"labs/lab2-overview/#dbt-tests-implemented","title":"dbt Tests Implemented","text":"<p>Uniqueness Tests: - <code>sentiment_event_id</code> must be unique - <code>(sentiment_date, brand)</code> composite key must be unique</p> <p>Not Null Tests: - <code>sentiment_event_id</code> - <code>brand_key</code> - <code>sentiment_score</code> - <code>published_date</code> - <code>sentiment_date</code></p> <p>Range Tests: - Sentiment scores between -1 and 1 - No future dates allowed</p> <p>Accepted Values Tests: - <code>quality_flag</code> in ['VALID', 'INVALID_SENTIMENT', 'NULL_HEADLINE'] - <code>anomaly_flag</code> in ['NORMAL', 'ANOMALY']</p> <p>Custom Tests: - All brands present in daily aggregates - Sentiment score validation</p>"},{"location":"labs/lab2-overview/#test-results","title":"Test Results","text":"<p>\u2705 14/14 tests passing (as of latest run)</p>"},{"location":"labs/lab2-overview/#metrics-available","title":"Metrics Available","text":""},{"location":"labs/lab2-overview/#event-level-metrics","title":"Event-Level Metrics","text":"<ul> <li>Sentiment score (-1 to 1)</li> <li>Engagement count (upvotes, shares)</li> <li>Content quality flag</li> <li>Temporal dimensions (year, month, day, hour)</li> </ul>"},{"location":"labs/lab2-overview/#daily-aggregate-metrics","title":"Daily Aggregate Metrics","text":"<ul> <li>Average sentiment</li> <li>Min/Max sentiment</li> <li>Standard deviation</li> <li>Content counts (total, positive, negative, neutral)</li> <li>Positive ratio</li> <li>Total/Average engagement</li> <li>Source diversity</li> <li>Z-score for anomaly detection</li> </ul>"},{"location":"labs/lab2-overview/#common-issues-solutions","title":"Common Issues &amp; Solutions","text":""},{"location":"labs/lab2-overview/#issue-800-duplicate-sentiment_event_id","title":"Issue: 800 Duplicate sentiment_event_id","text":"<p>Root Cause: - Wrong config parameter (<code>unique_id</code> vs <code>unique_key</code>) - Double WHERE clause in SQL - Insufficient uniqueness in surrogate key</p> <p>Solution Applied: 1. Changed <code>unique_id</code> \u2192 <code>unique_key</code> in model config 2. Fixed double WHERE clause with proper CTE structure 3. Enhanced surrogate key to include <code>source</code> field 4. Fixed data type mismatches (INTEGER \u2192 BIGINT)</p> <p>Result: \u2705 All duplicates resolved, uniqueness test passing</p>"},{"location":"labs/lab2-overview/#issue-contract-enforcement-errors","title":"Issue: Contract Enforcement Errors","text":"<p>Root Cause: - Schema mismatches between contract and actual SQL output - Incremental model schema changes - Missing columns in contract definition</p> <p>Solution Applied: 1. Updated data types in SQL to match contract (BIGINT vs INTEGER) 2. Added missing columns to contract in schema.yml 3. Removed contracts where not needed (marts without strict requirements)</p>"},{"location":"labs/lab2-overview/#current-status","title":"Current Status","text":""},{"location":"labs/lab2-overview/#completed","title":"\u2705 Completed","text":"<ul> <li>Synthetic data generation</li> <li>Staging models (Reddit + News)</li> <li>Intermediate unified model</li> <li>Fact table (fct_sentiment_events)</li> <li>Daily aggregate mart</li> <li>14 data quality tests</li> <li>Full documentation</li> </ul>"},{"location":"labs/lab2-overview/#in-progress","title":"\ud83d\udd04 In Progress","text":"<ul> <li>Real API integration (Reddit PRAW, NewsAPI)</li> <li>Advanced sentiment analysis (Hugging Face transformers)</li> <li>Streamlit dashboard</li> </ul>"},{"location":"labs/lab2-overview/#planned","title":"\ud83d\udccb Planned","text":"<ul> <li>Airflow orchestration</li> <li>Brand comparison analytics</li> <li>Temporal pattern analysis</li> <li>ML-based sentiment prediction</li> <li>Email alerting for anomalies</li> </ul>"},{"location":"labs/lab2-overview/#getting-started","title":"Getting Started","text":"<p>See Lab 2 Setup Guide for installation and configuration instructions.</p>"},{"location":"labs/lab2-overview/#related-documentation","title":"Related Documentation","text":"<ul> <li>Lab 2 Setup Guide</li> <li>Lab 2 Data Models</li> <li>Lab 2 Troubleshooting</li> <li>dbt Best Practices</li> <li>Architecture Overview</li> </ul> <p>Lab Owner: narensham Created: November 2025 Last Updated: November 2025 Status: \u2705 Active Development</p>"},{"location":"labs/lab2-quick-reference/","title":"Lab 2: Quick Reference Card","text":"<p>Essential commands and information for Market Sentiment Analysis lab.</p>"},{"location":"labs/lab2-quick-reference/#common-commands","title":"Common Commands","text":"<pre><code># Setup\ncd lab2_market_sentiment\nsource lab2_env/bin/activate\ndbt deps\n\n# Generate Data\npython pipelines/ingest_sentiment.py\n\n# Run Pipeline\ncd dbt\ndbt run                    # Incremental\ndbt run --full-refresh     # Full rebuild\ndbt run --select fct_sentiment_events  # Single model\n\n# Test\ndbt test                   # All tests\ndbt test --select fct_sentiment_events  # Model tests\n\n# Documentation\ndbt docs generate\ndbt docs serve\n\n# Cleanup\ndbt clean\n</code></pre>"},{"location":"labs/lab2-quick-reference/#key-metrics","title":"Key Metrics","text":"Metric Location Description Event Count fct_sentiment_events Total sentiment events Avg Sentiment mart_daily_sentiment Daily average by brand Anomaly Flag mart_daily_sentiment Statistical outliers Engagement Both tables Upvotes/shares metric Quality Flag fct_sentiment_events Data quality status"},{"location":"labs/lab2-quick-reference/#model-materialization","title":"Model Materialization","text":"Model Type Refresh stg_reddit__posts View Always stg_news__articles View Always int_sentiment_unified Table Full fct_sentiment_events Incremental New data only mart_daily_sentiment Incremental New dates only"},{"location":"labs/lab2-quick-reference/#data-quality-checks","title":"Data Quality Checks","text":"<ul> <li>\u2705 14 tests total</li> <li>Uniqueness: sentiment_event_id, (date, brand)</li> <li>Ranges: sentiment [-1, 1]</li> <li>Not null: 4 critical fields</li> <li>Accepted values: 2 categorical fields</li> </ul>"},{"location":"labs/lab2-quick-reference/#key-thresholds","title":"Key Thresholds","text":"<pre><code>sentiment_threshold_positive = 0.3\nsentiment_threshold_negative = -0.3\nanomaly_z_score_threshold = 2.0\n</code></pre>"},{"location":"labs/lab2-quick-reference/#common-queries","title":"Common Queries","text":""},{"location":"labs/lab2-quick-reference/#brand-sentiment-overview","title":"Brand Sentiment Overview","text":"<pre><code>SELECT \n    brand,\n    AVG(avg_sentiment) as sentiment,\n    SUM(content_count) as mentions\nFROM mart_daily_sentiment\nGROUP BY brand\nORDER BY sentiment DESC;\n</code></pre>"},{"location":"labs/lab2-quick-reference/#recent-anomalies","title":"Recent Anomalies","text":"<pre><code>SELECT * \nFROM mart_daily_sentiment\nWHERE anomaly_flag = 'ANOMALY'\nAND sentiment_date &gt;= CURRENT_DATE - INTERVAL '7 days'\nORDER BY sentiment_date DESC;\n</code></pre>"},{"location":"labs/lab2-quick-reference/#top-performing-content","title":"Top Performing Content","text":"<pre><code>SELECT \n    headline,\n    brand,\n    sentiment_score,\n    engagement_count\nFROM fct_sentiment_events\nORDER BY engagement_count DESC\nLIMIT 10;\n</code></pre>"},{"location":"labs/lab2-quick-reference/#file-locations","title":"File Locations","text":"<pre><code>lab2_market_sentiment/\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 raw/*.csv           # Source data\n\u2502   \u2514\u2500\u2500 *.duckdb            # Database\n\u251c\u2500\u2500 dbt/\n\u2502   \u251c\u2500\u2500 models/             # SQL transformations\n\u2502   \u251c\u2500\u2500 macros/             # Reusable SQL\n\u2502   \u251c\u2500\u2500 tests/              # Custom tests\n\u2502   \u2514\u2500\u2500 schema.yml          # Model contracts\n\u2514\u2500\u2500 pipelines/\n    \u2514\u2500\u2500 ingest_sentiment.py # Data generator\n</code></pre>"},{"location":"labs/lab2-quick-reference/#troubleshooting-quick-fixes","title":"Troubleshooting Quick Fixes","text":"Issue Quick Fix Duplicates <code>dbt run --full-refresh</code> Contract error Check data types match schema.yml Test fails <code>dbt test --select [test_name]</code> Slow run Use <code>--select</code> for specific models Database locked Close all connections, retry"},{"location":"labs/lab2-quick-reference/#important-dbt-config","title":"Important dbt Config","text":"<pre><code># dbt_project.yml\nvars:\n  sentiment_threshold_positive: 0.3\n  sentiment_threshold_negative: -0.3\n\n# fct_sentiment_events\nmaterialized: incremental\nunique_key: sentiment_event_id\nincremental_strategy: delete+insert\n\n# mart_daily_sentiment\nmaterialized: incremental\nunique_key: ['sentiment_date', 'brand']\n</code></pre>"},{"location":"labs/lab2-quick-reference/#database-connection","title":"Database Connection","text":"<pre><code># dbt/profiles.yml\nlab2_market_sentiment:\n  outputs:\n    dev:\n      type: duckdb\n      path: ../data/lab2_market_sentiment.duckdb\n      threads: 4\n  target: dev\n</code></pre>"},{"location":"labs/lab2-quick-reference/#python-dependencies","title":"Python Dependencies","text":"<pre><code>dbt-duckdb==1.7.0\nduckdb==0.9.1\npandas&gt;=2.0.0\nnumpy&gt;=1.24.0\npyyaml&gt;=6.0\n</code></pre>"},{"location":"labs/lab2-quick-reference/#useful-sql-snippets","title":"Useful SQL Snippets","text":""},{"location":"labs/lab2-quick-reference/#check-row-counts","title":"Check Row Counts","text":"<pre><code>SELECT \n    'reddit' as source,\n    COUNT(*) as count \nFROM stg_reddit__posts\nUNION ALL\nSELECT \n    'news' as source,\n    COUNT(*) as count \nFROM stg_news__articles;\n</code></pre>"},{"location":"labs/lab2-quick-reference/#find-data-quality-issues","title":"Find Data Quality Issues","text":"<pre><code>SELECT \n    quality_flag,\n    COUNT(*) as count\nFROM fct_sentiment_events\nGROUP BY quality_flag;\n</code></pre>"},{"location":"labs/lab2-quick-reference/#check-incremental-status","title":"Check Incremental Status","text":"<pre><code>SELECT \n    MAX(published_at) as latest_event,\n    MAX(_dbt_updated_at) as latest_refresh\nFROM fct_sentiment_events;\n</code></pre>"},{"location":"labs/lab2-quick-reference/#dbt-packages","title":"dbt Packages","text":"<pre><code># packages.yml\npackages:\n  - package: calogica/dbt_expectations\n    version: 0.10.4\n  - package: calogica/dbt_date\n    version: 0.10.1\n</code></pre>"},{"location":"labs/lab2-quick-reference/#metrics-dashboard-future","title":"Metrics Dashboard (Future)","text":"<p>Planned visualizations: - Sentiment trend by brand (line chart) - Daily mention volume (bar chart) - Anomaly alerts (table) - Engagement distribution (histogram) - Brand comparison (radar chart)</p>"},{"location":"labs/lab2-quick-reference/#next-steps-after-setup","title":"Next Steps After Setup","text":"<ol> <li>Review Data Models</li> <li>Understand Troubleshooting</li> <li>Explore dbt docs: <code>dbt docs serve</code></li> <li>Plan real API integration</li> <li>Design Streamlit dashboard</li> </ol> <p>Quick Help: See Lab 2 Overview for full details</p> <p>Last Updated: November 2025</p>"},{"location":"labs/lab2-setup/","title":"Lab 2: Market Sentiment Analysis - Setup Guide","text":"<p>Complete setup instructions for the Market Sentiment Analysis lab with step-by-step commands and configurations.</p>"},{"location":"labs/lab2-setup/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have:</p> <ul> <li>Python 3.11+ installed</li> <li>Git for version control</li> <li>2GB disk space available</li> <li>BuildCPG Labs root repository cloned</li> <li>Terminal access (Mac/Linux) or WSL2 (Windows)</li> </ul> <p>Check prerequisites: <pre><code>python3 --version  # Should show 3.11.x or higher\ngit --version      # Should show 2.x or higher\ndf -h             # Check disk space\n</code></pre></p>"},{"location":"labs/lab2-setup/#directory-structure","title":"Directory Structure","text":"<p>Understanding the lab structure before setup:</p> <pre><code>lab2_market_sentiment/\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 raw/                        # Source CSV files\n\u2502   \u2502   \u251c\u2500\u2500 reddit_real.csv         # Generated by ingestion\n\u2502   \u2502   \u2514\u2500\u2500 news_real.csv           # Generated by ingestion\n\u2502   \u251c\u2500\u2500 bronze/                     # dbt staging output\n\u2502   \u251c\u2500\u2500 silver/                     # dbt intermediate output\n\u2502   \u251c\u2500\u2500 gold/                       # dbt marts output\n\u2502   \u2514\u2500\u2500 lab2_market_sentiment.duckdb  # Main database file\n\u2502\n\u251c\u2500\u2500 dbt/\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u2502   \u251c\u2500\u2500 staging/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 stg_reddit__posts.sql\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 stg_news__articles.sql\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 _sources.yml\n\u2502   \u2502   \u251c\u2500\u2500 intermediate/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 int_sentiment_unified.sql\n\u2502   \u2502   \u2514\u2500\u2500 mart/\n\u2502   \u2502       \u251c\u2500\u2500 fct_sentiment_events.sql\n\u2502   \u2502       \u2514\u2500\u2500 mart_daily_sentiment.sql\n\u2502   \u251c\u2500\u2500 macros/\n\u2502   \u2502   \u251c\u2500\u2500 generate_surrogate_key.sql\n\u2502   \u2502   \u251c\u2500\u2500 validate_sentiment.sql\n\u2502   \u2502   \u2514\u2500\u2500 get_current_timestamp.sql\n\u2502   \u251c\u2500\u2500 tests/\n\u2502   \u2502   \u251c\u2500\u2500 test_no_future_dates.sql\n\u2502   \u2502   \u251c\u2500\u2500 test_sentiment_range.sql\n\u2502   \u2502   \u2514\u2500\u2500 test_mart_daily_has_all_brands.sql\n\u2502   \u251c\u2500\u2500 dbt_project.yml              # Project configuration\n\u2502   \u251c\u2500\u2500 profiles.yml                 # Database connection\n\u2502   \u251c\u2500\u2500 packages.yml                 # dbt packages\n\u2502   \u2514\u2500\u2500 schema.yml                   # Model contracts &amp; tests\n\u2502\n\u251c\u2500\u2500 pipelines/\n\u2502   \u2514\u2500\u2500 ingest_sentiment.py          # Data generation script\n\u2502\n\u251c\u2500\u2500 lab2_env/                        # Virtual environment (created during setup)\n\u251c\u2500\u2500 requirements.txt                 # Python dependencies\n\u2514\u2500\u2500 README.md                        # Lab documentation\n</code></pre>"},{"location":"labs/lab2-setup/#step-1-navigate-to-lab-2","title":"Step 1: Navigate to Lab 2","text":"<p>From the BuildCPG Labs root:</p> <pre><code>cd buildcpg-labs/lab2_market_sentiment\n\n# Verify you're in the right place\npwd\n# Should show: /path/to/buildcpg-labs/lab2_market_sentiment\n\nls\n# Should show: data/ dbt/ pipelines/ requirements.txt README.md\n</code></pre>"},{"location":"labs/lab2-setup/#step-2-create-virtual-environment","title":"Step 2: Create Virtual Environment","text":"<p>Create an isolated Python environment for Lab 2:</p> <pre><code># Create virtual environment\npython3 -m venv lab2_env\n\n# Activate it (Mac/Linux)\nsource lab2_env/bin/activate\n\n# Activate it (Windows WSL2)\nsource lab2_env/bin/activate\n\n# Verify activation (prompt should show lab2_env)\nwhich python\n# Should show: /path/to/lab2_market_sentiment/lab2_env/bin/python\n</code></pre> <p>Troubleshooting: - If activation fails on Mac: Try <code>source ./lab2_env/bin/activate</code> - If Python not found: Use <code>python3 -m venv lab2_env</code> - If permission denied: Check folder permissions with <code>ls -la</code></p>"},{"location":"labs/lab2-setup/#step-3-install-python-dependencies","title":"\ud83d\udce6 Step 3: Install Python Dependencies","text":"<p>Install all required packages:</p> <pre><code># Upgrade pip first\npip install --upgrade pip\n\n# Install requirements\npip install -r requirements.txt\n\n# Verify installations\npip list | grep -E \"dbt|duckdb|pandas\"\n</code></pre> <p>Expected output: <pre><code>dbt-core                  1.7.0\ndbt-duckdb                1.7.0\nduckdb                    0.9.1\npandas                    2.1.4\nnumpy                     1.26.2\npyyaml                    6.0.1\n</code></pre></p> <p>Dependencies include: - <code>dbt-duckdb==1.7.0</code> - dbt adapter for DuckDB - <code>duckdb==0.9.1</code> - Embedded database - <code>pandas&gt;=2.0.0</code> - Data manipulation - <code>numpy&gt;=1.24.0</code> - Numerical computing - <code>pyyaml&gt;=6.0</code> - Configuration parsing</p> <p>Troubleshooting: - If installation fails: Try <code>pip install --no-cache-dir -r requirements.txt</code> - If dbt-duckdb fails: Install separately with <code>pip install dbt-duckdb==1.7.0</code> - If memory errors: Install packages one at a time</p>"},{"location":"labs/lab2-setup/#step-4-install-dbt-packages","title":"Step 4: Install dbt Packages","text":"<p>Install external dbt packages for data quality testing:</p> <pre><code>cd dbt\n\n# Install packages\ndbt deps\n\n# Expected output:\n# Installing calogica/dbt_expectations\n# Installed from version 0.10.4\n# Installing calogica/dbt_date\n# Installed from version 0.10.1\n</code></pre> <p>Packages installed: - <code>calogica/dbt_expectations</code> - Advanced data quality tests - <code>calogica/dbt_date</code> - Date utility functions</p> <p>Package location: <code>dbt/dbt_packages/</code></p> <p>Troubleshooting: - If deps fails: Delete <code>dbt_packages/</code> folder and retry - If network error: Check internet connection - If version conflicts: Update <code>packages.yml</code> versions</p>"},{"location":"labs/lab2-setup/#step-5-configure-dbt-profile","title":"Step 5: Configure dbt Profile","text":"<p>Verify database connection settings:</p> <pre><code># Check profiles.yml exists\nls -la profiles.yml\n\n# View configuration\ncat profiles.yml\n</code></pre> <p>Expected <code>profiles.yml</code> content:</p> <pre><code>lab2_market_sentiment:\n  outputs:\n    dev:\n      type: duckdb\n      path: ../data/lab2_market_sentiment.duckdb\n      threads: 4\n\n  target: dev\n</code></pre> <p>Key settings: - <code>type: duckdb</code> - Use DuckDB adapter - <code>path: ../data/lab2_market_sentiment.duckdb</code> - Database location (relative to dbt/) - <code>threads: 4</code> - Parallel execution threads - <code>target: dev</code> - Default environment</p> <p>If file doesn't exist, create it:</p> <pre><code>cat &gt; profiles.yml &lt;&lt; 'EOF'\nlab2_market_sentiment:\n  outputs:\n    dev:\n      type: duckdb\n      path: ../data/lab2_market_sentiment.duckdb\n      threads: 4\n  target: dev\nEOF\n</code></pre>"},{"location":"labs/lab2-setup/#step-6-verify-dbt-setup","title":"Step 6: Verify dbt Setup","text":"<p>Test dbt configuration:</p> <pre><code># Must be in dbt/ directory\npwd  # Should show: /path/to/lab2_market_sentiment/dbt\n\n# Run dbt debug\ndbt debug\n</code></pre> <p>Expected output:</p> <pre><code>Running with dbt=1.7.0\n\u2705 Connection test: OK\n\u2705 All checks passed!\n</code></pre> <p>What dbt debug checks: 1. Python version compatibility 2. dbt installation 3. Profile configuration 4. Database connection 5. Project configuration</p> <p>Troubleshooting: - <code>Profile not found</code>: Check profiles.yml name matches dbt_project.yml - <code>Connection failed</code>: Verify path in profiles.yml is correct - <code>Python version</code>: Ensure Python 3.11+</p>"},{"location":"labs/lab2-setup/#step-7-generate-sample-data","title":"Step 7: Generate Sample Data","text":"<p>Generate synthetic sentiment data:</p> <pre><code># Go back to lab root\ncd ..\n\n# Run data generation script\npython pipelines/ingest_sentiment.py\n</code></pre> <p>Expected output: <pre><code>\ud83d\udd04 Fetching Reddit data...\nReddit data ingested: 500 records \u2192 data/raw/reddit_real.csv\n\ud83d\udd04 Fetching news data...\nNews data ingested: 300 records \u2192 data/raw/news_real.csv\nIngestion complete.\n</code></pre></p> <p>Files created: - <code>data/raw/reddit_real.csv</code> (500 records) - <code>data/raw/news_real.csv</code> (300 records)</p> <p>Data includes: - 5 CPG brands: Coca-Cola, PepsiCo, Unilever, Procter &amp; Gamble, Nestl\u00e9 - Sentiment scores: -1 (negative) to 1 (positive) - Timestamps: Last 90 days - Engagement metrics: Upvotes, comments, shares</p> <p>Verify data: <pre><code># Check files exist\nls -lh data/raw/\n\n# View first few lines\nhead -5 data/raw/reddit_real.csv\nhead -5 data/raw/news_real.csv\n\n# Count rows\nwc -l data/raw/*.csv\n</code></pre></p>"},{"location":"labs/lab2-setup/#step-8-run-dbt-pipeline","title":"Step 8: Run dbt Pipeline","text":"<p>Execute the complete data transformation pipeline:</p> <pre><code>cd dbt\n\n# Run with full refresh (first time)\ndbt build --full-refresh\n</code></pre> <p>What happens:</p> <ol> <li>Staging models (2 models)</li> <li><code>stg_reddit__posts</code> - Clean Reddit data</li> <li> <p><code>stg_news__articles</code> - Clean news data</p> </li> <li> <p>Intermediate model (1 model)</p> </li> <li> <p><code>int_sentiment_unified</code> - Unified data with business logic</p> </li> <li> <p>Mart models (2 models)</p> </li> <li><code>fct_sentiment_events</code> - Fact table</li> <li> <p><code>mart_daily_sentiment</code> - Daily aggregates</p> </li> <li> <p>Data quality tests (14 tests)</p> </li> <li>Uniqueness checks</li> <li>Not null validations</li> <li>Range validations</li> <li>Accepted values</li> <li>Custom business logic</li> </ol> <p>Expected output: <pre><code>Running with dbt=1.7.0\nFound 5 models, 14 tests\n\n1 of 17 START sql table model main.sentiment_unified .............. [RUN]\n1 of 17 OK created sql table model main.sentiment_unified ......... [OK in 0.42s]\n\n2 of 17 START sql incremental model main.sentiment_events ......... [RUN]\n2 of 17 OK created sql incremental model main.sentiment_events .... [OK in 0.22s]\n\n[... tests running ...]\n\nCompleted successfully\n\u2705 PASS=14 WARN=0 ERROR=0 SKIP=0 TOTAL=17\n</code></pre></p> <p>Build time: ~2-3 seconds with sample data</p> <p>Troubleshooting: - If duplicates error: See Troubleshooting Guide - If contract error: Check data types match schema.yml - If test fails: Run <code>dbt test</code> alone to isolate issue</p>"},{"location":"labs/lab2-setup/#step-9-verify-data-creation","title":"Step 9: Verify Data Creation","text":"<p>Check that all tables were created:</p> <pre><code># List all models\ndbt ls --resource-type model\n</code></pre> <p>Expected output: <pre><code>stg_reddit__posts\nstg_news__articles\nint_sentiment_unified\nfct_sentiment_events\nmart_daily_sentiment\n</code></pre></p> <p>Check row counts:</p> <pre><code># From lab2_market_sentiment directory\npython -c \"\nimport duckdb\nconn = duckdb.connect('data/lab2_market_sentiment.duckdb')\n\n# Check staging\nprint('Staging:')\nprint(f\\\"Reddit: {conn.execute('SELECT COUNT(*) FROM main.stg_reddit__posts').fetchone()[0]} rows\\\")\nprint(f\\\"News: {conn.execute('SELECT COUNT(*) FROM main.stg_news__articles').fetchone()[0]} rows\\\")\n\n# Check intermediate\nprint(f\\\"\\nIntermediate:\\\")\nprint(f\\\"Unified: {conn.execute('SELECT COUNT(*) FROM main.sentiment_unified').fetchone()[0]} rows\\\")\n\n# Check marts\nprint(f\\\"\\nMarts:\\\")\nprint(f\\\"Events: {conn.execute('SELECT COUNT(*) FROM main.sentiment_events').fetchone()[0]} rows\\\")\nprint(f\\\"Daily: {conn.execute('SELECT COUNT(*) FROM main.mart_daily_sentiment').fetchone()[0]} rows\\\")\n\nconn.close()\n\"\n</code></pre> <p>Expected output: <pre><code>Staging:\nReddit: 500 rows\nNews: 300 rows\n\nIntermediate:\nUnified: 800 rows\n\nMarts:\nEvents: 800 rows\nDaily: [varies by date range]\n</code></pre></p>"},{"location":"labs/lab2-setup/#step-10-run-data-quality-tests","title":"Step 10: Run Data Quality Tests","text":"<p>Execute all data quality tests:</p> <pre><code>cd dbt\n\n# Run all tests\ndbt test\n</code></pre> <p>Tests executed (14 total):</p> <ol> <li>Uniqueness Tests (2)</li> <li><code>unique_fct_sentiment_events_sentiment_event_id</code></li> <li> <p><code>expect_compound_columns_to_be_unique_mart_daily_sentiment</code></p> </li> <li> <p>Not Null Tests (4)</p> </li> <li><code>not_null_fct_sentiment_events_sentiment_event_id</code></li> <li><code>not_null_fct_sentiment_events_brand_key</code></li> <li><code>not_null_fct_sentiment_events_sentiment_score</code></li> <li> <p><code>not_null_fct_sentiment_events_published_date</code></p> </li> <li> <p>Range Tests (2)</p> </li> <li><code>expect_column_values_to_be_between_sentiment_score</code> (-1 to 1)</li> <li> <p><code>test_sentiment_range</code></p> </li> <li> <p>Accepted Values Tests (2)</p> </li> <li><code>accepted_values_quality_flag</code></li> <li> <p><code>accepted_values_anomaly_flag</code></p> </li> <li> <p>Custom Business Logic Tests (4)</p> </li> <li><code>test_no_future_dates</code></li> <li><code>test_mart_daily_has_all_brands</code></li> <li>Additional custom validations</li> </ol> <p>Expected output: <pre><code>\u2705 PASS=14 WARN=0 ERROR=0 SKIP=0 TOTAL=14\n</code></pre></p> <p>If tests fail: See Troubleshooting Guide</p>"},{"location":"labs/lab2-setup/#step-11-generate-documentation","title":"Step 11: Generate Documentation","text":"<p>Create and view dbt documentation:</p> <pre><code># Generate docs\ndbt docs generate\n\n# Serve docs locally\ndbt docs serve\n</code></pre> <p>What's included: - Model lineage diagram - Column descriptions - Test results - SQL code - Relationships</p> <p>Access documentation: http://localhost:8080</p> <p>View in browser: - Navigate model DAG (Directed Acyclic Graph) - Click models to see details - View SQL compilation - See test results</p> <p>Stop server: Press <code>Ctrl+C</code></p>"},{"location":"labs/lab2-setup/#step-12-test-incremental-runs","title":"Step 12: Test Incremental Runs","text":"<p>Test that incremental models work correctly:</p> <pre><code># Generate more data\ncd ..\npython pipelines/ingest_sentiment.py\n\n# Run incrementally (only new data)\ncd dbt\ndbt run\n\n# Should process only new records\n# Expected output: \"Inserted X rows\" or \"No new data\"\n</code></pre> <p>Incremental behavior: - <code>fct_sentiment_events</code>: Only inserts records with <code>published_at &gt; MAX(published_at)</code> - <code>mart_daily_sentiment</code>: Only processes new dates</p>"},{"location":"labs/lab2-setup/#configuration-files-reference","title":"Configuration Files Reference","text":""},{"location":"labs/lab2-setup/#dbt_projectyml","title":"dbt_project.yml","text":"<pre><code>name: 'lab2_market_sentiment'\nversion: '1.0.0'\nprofile: 'lab2_market_sentiment'\n\nconfig-version: 2\n\nmodel-paths: [\"models\"]\ntest-paths: [\"tests\"]\nmacro-paths: [\"macros\"]\ntarget-path: \"target\"\nclean-targets: [\"target\", \"dbt_packages\", \"logs\"]\n\nmodels:\n  lab2_market_sentiment:\n    staging:\n      +materialized: view\n      +tags: ['staging']\n    intermediate:\n      +materialized: table\n      +tags: ['intermediate']\n    mart:\n      +materialized: incremental\n      +tags: ['mart']\n\nvars:\n  sentiment_threshold_positive: 0.3\n  sentiment_threshold_negative: -0.3\n</code></pre>"},{"location":"labs/lab2-setup/#packagesyml","title":"packages.yml","text":"<pre><code>packages:\n  - package: calogica/dbt_expectations\n    version: 0.10.4\n  - package: calogica/dbt_date\n    version: 0.10.1\n</code></pre> <p>Note: Consider upgrading to <code>metaplane/dbt_expectations</code> (newer version)</p>"},{"location":"labs/lab2-setup/#requirementstxt","title":"requirements.txt","text":"<pre><code>dbt-duckdb==1.7.0\nduckdb==0.9.1\npandas&gt;=2.0.0\nnumpy&gt;=1.24.0\npyyaml&gt;=6.0\n</code></pre>"},{"location":"labs/lab2-setup/#common-commands-reference","title":"Common Commands Reference","text":""},{"location":"labs/lab2-setup/#data-generation","title":"Data Generation","text":"<pre><code>python pipelines/ingest_sentiment.py  # Generate new data\n</code></pre>"},{"location":"labs/lab2-setup/#dbt-operations","title":"dbt Operations","text":"<pre><code>dbt deps                    # Install packages\ndbt debug                   # Verify setup\ndbt compile                 # Compile SQL\ndbt run                     # Run models\ndbt run --full-refresh      # Full rebuild\ndbt run --select model_name # Run specific model\ndbt test                    # Run tests\ndbt test --select model_name  # Test specific model\ndbt docs generate           # Generate docs\ndbt docs serve              # Serve docs\ndbt clean                   # Clean artifacts\ndbt ls                      # List resources\n</code></pre>"},{"location":"labs/lab2-setup/#database-operations","title":"Database Operations","text":"<pre><code># Connect to database\npython -c \"import duckdb; conn = duckdb.connect('data/lab2_market_sentiment.duckdb'); print(conn.execute('SHOW TABLES').fetchall())\"\n\n# Vacuum database\npython -c \"import duckdb; conn = duckdb.connect('data/lab2_market_sentiment.duckdb'); conn.execute('VACUUM'); print('Done')\"\n</code></pre>"},{"location":"labs/lab2-setup/#troubleshooting-quick-reference","title":"Troubleshooting Quick Reference","text":"Issue Solution dbt deps fails <code>rm -rf dbt_packages/ &amp;&amp; dbt deps</code> Database locked Close all connections, retry Tests fail <code>dbt test --select [test_name]</code> Duplicates See full guide Contract error Match data types in schema.yml Profile not found Check profiles.yml name <p>Full troubleshooting: See Lab 2 Troubleshooting</p>"},{"location":"labs/lab2-setup/#setup-verification-checklist","title":"Setup Verification Checklist","text":"<p>Before proceeding, verify:</p> <ul> <li> Virtual environment activated (<code>lab2_env</code>)</li> <li> All dependencies installed (<code>pip list</code>)</li> <li> dbt packages installed (<code>dbt_packages/</code>)</li> <li> dbt debug passes (<code>\u2705 All checks passed</code>)</li> <li> Sample data generated (800 total rows)</li> <li> All 5 models created (check with <code>dbt ls</code>)</li> <li> All 14 tests passing (<code>dbt test</code>)</li> <li> Database file exists (<code>data/lab2_market_sentiment.duckdb</code>)</li> <li> Documentation generated (<code>dbt docs generate</code>)</li> </ul>"},{"location":"labs/lab2-setup/#next-steps","title":"Next Steps","text":"<p>After successful setup:</p> <ol> <li>Explore Data Models - See Data Models Reference</li> <li>Run Sample Queries - See Quick Reference</li> <li>Understand Architecture - See Lab 2 Overview</li> <li>Plan Real API Integration - Upgrade from synthetic to real data</li> <li>Build Dashboard - Create Streamlit visualization</li> </ol>"},{"location":"labs/lab2-setup/#incremental-updates","title":"Incremental Updates","text":"<p>For subsequent runs:</p> <pre><code># Activate environment\nsource lab2_env/bin/activate\n\n# Generate new data\npython pipelines/ingest_sentiment.py\n\n# Run incrementally\ncd dbt\ndbt run  # Only processes new data\n\n# Run tests\ndbt test\n</code></pre>"},{"location":"labs/lab2-setup/#cleanup-reset","title":"Cleanup &amp; Reset","text":"<p>To start fresh:</p> <pre><code># Clean dbt artifacts\ncd dbt\ndbt clean\n\n# Delete database\nrm ../data/lab2_market_sentiment.duckdb\n\n# Rebuild from scratch\ndbt build --full-refresh\n</code></pre>"},{"location":"labs/lab2-setup/#getting-help","title":"Getting Help","text":"<ul> <li>Setup Issues: See Troubleshooting</li> <li>Model Questions: See Data Models</li> <li>Quick Reference: See Quick Reference</li> <li>General Help: See Main Documentation</li> </ul> <p>Setup Time: ~10-15 minutes Difficulty: Intermediate Prerequisites: Python, Git, Terminal basics Last Updated: November 2025 Maintainer: narensham</p>"},{"location":"labs/lab2-troubleshooting/","title":"Lab 2: Troubleshooting Guide","text":"<p>Comprehensive solutions to all issues encountered in the Market Sentiment Analysis lab, with diagrams and step-by-step fixes.</p>"},{"location":"labs/lab2-troubleshooting/#critical-issues","title":"Critical Issues","text":""},{"location":"labs/lab2-troubleshooting/#issue-1-duplicate-sentiment_event_id-800-duplicates","title":"Issue #1: Duplicate sentiment_event_id (800 duplicates)","text":"<p>Severity: \ud83d\udd34 Critical Impact: Data quality test failure, invalid fact table Status: \u2705 Resolved</p> <pre><code>graph TB\n    A[Problem: 800 Duplicates] --&gt; B[Root Cause 1:&lt;br/&gt;Wrong Config Param]\n    A --&gt; C[Root Cause 2:&lt;br/&gt;Double WHERE Clause]\n    A --&gt; D[Root Cause 3:&lt;br/&gt;Weak Surrogate Key]\n    A --&gt; E[Root Cause 4:&lt;br/&gt;Data Type Mismatch]\n\n    B --&gt; F[Solution 1:&lt;br/&gt;unique_key not unique_id]\n    C --&gt; G[Solution 2:&lt;br/&gt;Fix CTE Structure]\n    D --&gt; H[Solution 3:&lt;br/&gt;Add source to key]\n    E --&gt; I[Solution 4:&lt;br/&gt;Use BIGINT not INTEGER]\n\n    F --&gt; J[\u2705 All Tests Pass]\n    G --&gt; J\n    H --&gt; J\n    I --&gt; J\n\n    style A fill:#fee2e2,stroke:#dc2626\n    style J fill:#dcfce7,stroke:#16a34a</code></pre>"},{"location":"labs/lab2-troubleshooting/#root-cause-analysis","title":"Root Cause Analysis","text":"<p>1. Wrong Config Parameter <pre><code>-- \u274c WRONG (in fct_sentiment_events.sql)\n{{ config(\n  materialized='incremental',\n  unique_id='sentiment_event_id',  -- Wrong parameter name\n  ...\n) }}\n</code></pre></p> <p>Impact: dbt doesn't use this key for deduplication, allowing duplicates to accumulate.</p> <p>2. Double WHERE Clause <pre><code>-- \u274c WRONG SQL Structure\nFROM {{ ref('int_sentiment_unified') }}\n\n{% if is_incremental() %}\n    WHERE published_at &gt; (SELECT MAX(published_at) FROM {{ this }})\n{% endif %}\n\nWHERE quality_flag = 'VALID'  -- \u274c Second WHERE clause = SQL error\n</code></pre></p> <p>Impact: SQL compilation error or incorrect filtering logic.</p> <p>3. Insufficient Surrogate Key <pre><code>-- \u274c WEAK: Only uses 2 fields\n{{ generate_surrogate_key(['content_id', 'published_at']) }}\n</code></pre></p> <p>Impact: Same content_id + timestamp from different sources creates collisions.</p> <p>4. Data Type Mismatch <pre><code>-- \u274c SQL produces INTEGER\nCAST(EXTRACT(YEAR FROM published_at) AS INTEGER) as published_year\n\n-- \u2705 Contract expects BIGINT\ndata_type: bigint\n</code></pre></p> <p>Impact: Contract enforcement fails, model doesn't build.</p>"},{"location":"labs/lab2-troubleshooting/#complete-solution","title":"Complete Solution","text":"<p>Step 1: Fix Config Parameter</p> <pre><code>{{ config(\n  materialized='incremental',\n  unique_key='sentiment_event_id',  -- \u2705 CORRECT\n  tags=['mart', 'facts'],\n  description='Fact table: One row per sentiment event',\n  incremental_strategy='delete+insert'\n) }}\n</code></pre> <p>Step 2: Fix WHERE Clause with CTE</p> <pre><code>-- \u2705 CORRECT SQL Structure\nWITH base_data AS (\n    SELECT *,\n        -- Calculate quality_flag in CTE\n        CASE \n            WHEN NOT {{ validate_sentiment('sentiment_score') }} THEN 'INVALID_SENTIMENT'\n            WHEN headline IS NULL THEN 'NULL_HEADLINE'\n            ELSE 'VALID'\n        END as quality_flag\n    FROM {{ ref('int_sentiment_unified') }}\n\n    {% if is_incremental() %}\n    WHERE published_at &gt; (SELECT MAX(published_at) FROM {{ this }})\n    {% endif %}\n)\n\nSELECT * FROM base_data\nWHERE quality_flag = 'VALID'  -- \u2705 Single WHERE clause\n</code></pre> <p>Step 3: Enhance Surrogate Key</p> <pre><code>-- \u2705 STRONG: Uses 3 fields including source\n{{ generate_surrogate_key(['content_id', 'published_at', 'source']) }}\n</code></pre> <p>Step 4: Fix Data Types</p> <pre><code>-- \u2705 CORRECT: Match contract expectations\nCAST(engagement_count AS BIGINT) as engagement_count,\nCAST(EXTRACT(YEAR FROM published_at) AS BIGINT) as published_year,\nCAST(EXTRACT(MONTH FROM published_at) AS BIGINT) as published_month,\nCAST(EXTRACT(DAYOFWEEK FROM published_at) AS BIGINT) as published_day_of_week,\nCAST(EXTRACT(HOUR FROM published_at) AS BIGINT) as published_hour,\n</code></pre>"},{"location":"labs/lab2-troubleshooting/#verification","title":"Verification","text":"<pre><code># Run dbt with full refresh\ncd dbt\ndbt clean\ndbt build --full-refresh\n\n# Check uniqueness test\ndbt test --select unique_fct_sentiment_events_sentiment_event_id\n\n# Expected output:\n# \u2705 PASS unique_fct_sentiment_events_sentiment_event_id\n</code></pre>"},{"location":"labs/lab2-troubleshooting/#prevention","title":"Prevention","text":"<pre><code>graph LR\n    A[Before Deploy] --&gt; B[Run dbt debug]\n    B --&gt; C[Run dbt compile]\n    C --&gt; D[Check compiled SQL]\n    D --&gt; E[Run dbt test]\n    E --&gt; F{All Pass?}\n    F --&gt;|Yes| G[Deploy]\n    F --&gt;|No| H[Fix Issues]\n    H --&gt; B\n\n    style G fill:#dcfce7\n    style H fill:#fee2e2</code></pre>"},{"location":"labs/lab2-troubleshooting/#issue-2-contract-enforcement-failures","title":"Issue #2: Contract Enforcement Failures","text":"<p>Severity: \ud83d\udfe1 High Impact: Model won't build with enforced contracts Status: \u2705 Resolved</p> <pre><code>graph TB\n    A[Contract Enforcement Error] --&gt; B{Error Type}\n    B --&gt;|Data Type| C[Type Mismatch]\n    B --&gt;|Missing Column| D[Schema Incomplete]\n    B --&gt;|Schema Change| E[Incremental Conflict]\n\n    C --&gt; F[Solution: Match Types]\n    D --&gt; G[Solution: Add Columns]\n    E --&gt; H[Solution: on_schema_change]\n\n    F --&gt; I[\u2705 Build Success]\n    G --&gt; I\n    H --&gt; I\n\n    style A fill:#fef3c7,stroke:#f59e0b\n    style I fill:#dcfce7,stroke:#16a34a</code></pre>"},{"location":"labs/lab2-troubleshooting/#symptoms","title":"Symptoms","text":"<pre><code>Compilation Error: This model has an enforced contract that failed.\n\n| column_name       | definition_type | contract_type | mismatch_reason    |\n| engagement_count  | INTEGER         | BIGINT        | data type mismatch |\n| published_year    | INTEGER         | BIGINT        | data type mismatch |\n</code></pre>"},{"location":"labs/lab2-troubleshooting/#root-cause","title":"Root Cause","text":"<p>SQL produces <code>INTEGER</code> but contract expects <code>BIGINT</code> in DuckDB.</p>"},{"location":"labs/lab2-troubleshooting/#solution-approaches","title":"Solution Approaches","text":"<p>Approach 1: Update SQL to Match Contract (Recommended)</p> <pre><code>-- Change all INTEGER casts to BIGINT\nCAST(engagement_count AS BIGINT) as engagement_count,\nCAST(source_key AS INTEGER) as source_key,  -- This one stays INTEGER\n</code></pre> <p>Approach 2: Update Contract to Match SQL</p> <pre><code># In schema.yml\ncolumns:\n  - name: engagement_count\n    data_type: bigint  # Match what SQL actually produces\n</code></pre> <p>Approach 3: Remove Contract (If too strict)</p> <pre><code># In schema.yml - comment out contract\n# contract:\n#   enforced: true\n</code></pre>"},{"location":"labs/lab2-troubleshooting/#for-incremental-models-with-contracts","title":"For Incremental Models with Contracts","text":"<pre><code>{{ config(\n  materialized='incremental',\n  unique_key='sentiment_event_id',\n  on_schema_change='append_new_columns',  -- \u2705 REQUIRED with contracts\n  contract={\n    'enforced': true\n  }\n) }}\n</code></pre> <p>Valid <code>on_schema_change</code> values with contracts: - <code>append_new_columns</code> \u2705 - <code>fail</code> \u2705 - <code>sync_all_columns</code> \u274c (not allowed)</p>"},{"location":"labs/lab2-troubleshooting/#contract-validation-checklist","title":"Contract Validation Checklist","text":"<pre><code>graph LR\n    A[Define Contract] --&gt; B[Write SQL]\n    B --&gt; C[dbt compile]\n    C --&gt; D{Types Match?}\n    D --&gt;|Yes| E[dbt run]\n    D --&gt;|No| F[Fix Mismatches]\n    F --&gt; B\n    E --&gt; G{Build Success?}\n    G --&gt;|Yes| H[\u2705 Done]\n    G --&gt;|No| I[Check Error]\n    I --&gt; F\n\n    style H fill:#dcfce7</code></pre>"},{"location":"labs/lab2-troubleshooting/#issue-3-cte-reference-errors","title":"Issue #3: CTE Reference Errors","text":"<p>Severity: \ud83d\udd34 Critical Impact: Model compilation failure Status: \u2705 Resolved</p> <pre><code>graph TB\n    A[Catalog Error:&lt;br/&gt;Table combined does not exist] --&gt; B[Root Cause:&lt;br/&gt;Broken CTE Chain]\n    B --&gt; C[Missing Comma]\n    B --&gt; D[Extra Comma]\n    B --&gt; E[Wrong Nesting]\n\n    C --&gt; F[Solution:&lt;br/&gt;Fix CTE Structure]\n    D --&gt; F\n    E --&gt; F\n\n    F --&gt; G[\u2705 Compilation Success]\n\n    style A fill:#fee2e2,stroke:#dc2626\n    style G fill:#dcfce7,stroke:#16a34a</code></pre>"},{"location":"labs/lab2-troubleshooting/#error-message","title":"Error Message","text":"<pre><code>Runtime Error: Catalog Error: Table with name combined does not exist!\nDid you mean \"temp.information_schema.columns\"?\n</code></pre>"},{"location":"labs/lab2-troubleshooting/#root-cause_1","title":"Root Cause","text":"<p>Incorrect CTE chain structure - comma placement breaks the flow.</p>"},{"location":"labs/lab2-troubleshooting/#wrong-cte-structure","title":"Wrong CTE Structure","text":"<pre><code>-- \u274c WRONG: Comma after combined closes the chain\ncombined AS (\n    SELECT * FROM reddit_refined\n    UNION ALL\n    SELECT * FROM news_refined\n),  -- \u274c This comma closes the CTE chain\n\n-- Now \"combined\" is out of scope\nwith_surrogate_keys AS (\n    SELECT ... FROM combined  -- \u274c Error: combined doesn't exist\n)\n</code></pre>"},{"location":"labs/lab2-troubleshooting/#correct-cte-structure","title":"Correct CTE Structure","text":"<pre><code>-- \u2705 CORRECT: Continuous chain with proper commas\nWITH reddit_refined AS (\n    SELECT ...\n),\n\nnews_refined AS (\n    SELECT ...\n),\n\ncombined AS (\n    SELECT * FROM reddit_refined\n    UNION ALL\n    SELECT * FROM news_refined\n),  -- \u2705 Comma continues chain\n\nwith_surrogate_keys AS (\n    SELECT ... FROM combined  -- \u2705 combined is in scope\n),\n\nfinal AS (\n    SELECT ... FROM with_surrogate_keys\n)  -- \u2705 No comma on last CTE\n\nSELECT * FROM final  -- Main query\n</code></pre>"},{"location":"labs/lab2-troubleshooting/#cte-structure-rules","title":"CTE Structure Rules","text":"<pre><code>graph LR\n    A[WITH] --&gt; B[CTE1]\n    B --&gt;|,| C[CTE2]\n    C --&gt;|,| D[CTE3]\n    D --&gt;|NO COMMA| E[Main SELECT]\n\n    style A fill:#dbeafe\n    style E fill:#dcfce7</code></pre> <p>Rules: 1. Start with <code>WITH</code> 2. Separate CTEs with commas 3. NO comma after last CTE 4. End with main <code>SELECT</code></p>"},{"location":"labs/lab2-troubleshooting/#dbt-issues","title":"dbt Issues","text":""},{"location":"labs/lab2-troubleshooting/#issue-4-dbt-deps-package-warning","title":"Issue #4: dbt deps Package Warning","text":"<p>Severity: \ud83d\udfe1 Low Impact: Deprecation warning, works but should upgrade Status: \u26a0\ufe0f Advisory</p> <pre><code>[WARNING]: Deprecated functionality\nThe `calogica/dbt_expectations` package is deprecated in favor of\n`metaplane/dbt_expectations`. Please update your `packages.yml`\n</code></pre>"},{"location":"labs/lab2-troubleshooting/#solution","title":"Solution","text":"<p>Update <code>packages.yml</code>:</p> <pre><code># \u274c OLD (deprecated)\npackages:\n  - package: calogica/dbt_expectations\n    version: 0.10.4\n\n# \u2705 NEW (recommended)\npackages:\n  - package: metaplane/dbt_expectations\n    version: 0.10.4\n</code></pre> <p>Then: <pre><code>dbt clean\nrm -rf dbt_packages/\ndbt deps\n</code></pre></p>"},{"location":"labs/lab2-troubleshooting/#issue-5-incremental-schema-out-of-sync","title":"Issue #5: Incremental Schema Out of Sync","text":"<p>Severity: \ud83d\udfe1 High Impact: Incremental model won't run Status: \u2705 Resolved</p> <pre><code>graph TB\n    A[Schema Out of Sync Error] --&gt; B{Resolution Strategy}\n    B --&gt;|Quick Fix| C[Drop &amp; Rebuild]\n    B --&gt;|Preserve Data| D[Add on_schema_change]\n    B --&gt;|Manual| E[ALTER TABLE]\n\n    C --&gt; F[dbt run --full-refresh]\n    D --&gt; G[on_schema_change config]\n    E --&gt; H[Manual SQL ALTER]\n\n    F --&gt; I[\u2705 Resolved]\n    G --&gt; I\n    H --&gt; I\n\n    style A fill:#fef3c7\n    style I fill:#dcfce7</code></pre>"},{"location":"labs/lab2-troubleshooting/#error-message_1","title":"Error Message","text":"<pre><code>The source and target schemas on this incremental model are out of sync!\nSource columns not in target: [stddev_sentiment, z_score_sentiment, anomaly_flag]\n</code></pre>"},{"location":"labs/lab2-troubleshooting/#quick-fix-drop-and-rebuild","title":"Quick Fix: Drop and Rebuild","text":"<pre><code># Option 1: Python script\npython -c \"\nimport duckdb\nconn = duckdb.connect('data/lab2_market_sentiment.duckdb')\nconn.execute('DROP TABLE IF EXISTS main.mart_daily_sentiment')\nconn.close()\nprint('\u2705 Table dropped')\n\"\n\n# Option 2: dbt full-refresh\ndbt run --full-refresh --select mart_daily_sentiment\n</code></pre>"},{"location":"labs/lab2-troubleshooting/#permanent-fix-add-config","title":"Permanent Fix: Add Config","text":"<pre><code>{{ config(\n  materialized='incremental',\n  on_schema_change='append_new_columns',  -- \u2705 Add this\n  ...\n) }}\n</code></pre> <p>Valid options: - <code>fail</code> - Fail on schema change (safe) - <code>append_new_columns</code> - Add new columns automatically - <code>ignore</code> - Ignore schema changes (dangerous)</p>"},{"location":"labs/lab2-troubleshooting/#issue-6-profile-not-found","title":"Issue #6: Profile Not Found","text":"<p>Severity: \ud83d\udd34 Critical Impact: Can't connect to database Status: \u2705 Resolved</p> <p>```mermaid graph LR     A[Profile Not Found] \u2192 B{Check Location}     B \u2192|Wrong Name| C[Fix</p>"},{"location":"phases/phase1-foundation/","title":"Phase 1: Foundation","text":"<p>Establish shared utilities and central configuration that all labs use.</p>"},{"location":"phases/phase1-foundation/#what-phase-1-delivers","title":"What Phase 1 Delivers","text":"<p>\u2705 Shared Utilities - DataInspector, CSVMonitor, Config Loaders \u2705 Central Configuration - Lab registry, path management \u2705 Lab1 Scripts - Data inspection and monitoring \u2705 Clean Git History - No build artifacts or venv  </p>"},{"location":"phases/phase1-foundation/#what-gets-created","title":"What Gets Created","text":"<pre><code>buildcpg-labs/\n\u251c\u2500\u2500 shared/                          \u2190 NEW\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 utils/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 data_inspector.py       # Database inspection\n\u2502   \u2502   \u251c\u2500\u2500 csv_monitor.py          # Detect new data\n\u2502   \u2502   \u2514\u2500\u2500 config_loader.py        # Load configurations\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 data_quality/\n\u2502   \u2502   \u251c\u2500\u2500 validators.py           # Validation rules\n\u2502   \u2502   \u2514\u2500\u2500 expectations.py         # Data expectations\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 templates/                  \u2190 For Phase 2\n\u2502       \u2514\u2500\u2500 (templates will go here)\n\u2502\n\u251c\u2500\u2500 config/                          \u2190 NEW\n\u2502   \u251c\u2500\u2500 labs_config.yaml            # Lab registry\n\u2502   \u2514\u2500\u2500 paths.py                    # Path helpers\n\u2502\n\u2514\u2500\u2500 lab1_sales_performance/         \u2190 UPDATED\n    \u251c\u2500\u2500 scripts/\n    \u2502   \u251c\u2500\u2500 inspect_data.py         # NEW - uses shared utilities\n    \u2502   \u2514\u2500\u2500 check_for_new_data.py   # NEW - uses shared utilities\n    \u2514\u2500\u2500 .gitignore                  # NEW - ignore venv, artifacts\n</code></pre>"},{"location":"phases/phase1-foundation/#step-by-step-setup","title":"Step-by-Step Setup","text":""},{"location":"phases/phase1-foundation/#step-1-create-directory-structure","title":"Step 1: Create Directory Structure","text":"<pre><code>cd buildcpg-labs\nmkdir -p shared/utils\nmkdir -p shared/data_quality\nmkdir -p shared/templates\nmkdir -p config\n\n# Create __init__.py files\ntouch shared/__init__.py\ntouch shared/utils/__init__.py\ntouch shared/data_quality/__init__.py\n</code></pre>"},{"location":"phases/phase1-foundation/#step-2-create-shared-utilities","title":"Step 2: Create Shared Utilities","text":"<p>Create these files in <code>shared/utils/</code>:</p> <ul> <li><code>data_inspector.py</code> - Inspect databases</li> <li><code>csv_monitor.py</code> - Monitor CSV changes</li> <li><code>config_loader.py</code> - Load lab configurations</li> </ul> <p>See documentation links below.</p>"},{"location":"phases/phase1-foundation/#step-3-create-configuration","title":"Step 3: Create Configuration","text":"<p>Create these files in <code>config/</code>:</p> <ul> <li><code>labs_config.yaml</code> - Central lab registry</li> <li><code>paths.py</code> - Path helpers for accessing labs</li> </ul>"},{"location":"phases/phase1-foundation/#step-4-create-lab1-scripts","title":"Step 4: Create Lab1 Scripts","text":"<p>Create these files in <code>lab1_sales_performance/scripts/</code>:</p> <ul> <li><code>inspect_data.py</code> - Inspect lab1 database using shared DataInspector</li> <li><code>check_for_new_data.py</code> - Monitor lab1 CSV using shared CSVMonitor</li> </ul>"},{"location":"phases/phase1-foundation/#step-5-add-git-ignore","title":"Step 5: Add Git Ignore","text":"<p>Create <code>.gitignore</code> at root and <code>lab1_sales_performance/.gitignore</code></p>"},{"location":"phases/phase1-foundation/#step-6-test-everything-works","title":"Step 6: Test Everything Works","text":"<pre><code># Test shared utilities\npython shared/utils/data_inspector.py\npython shared/utils/csv_monitor.py\n\n# Test config\npython config/paths.py\n\n# Test lab1 scripts\ncd lab1_sales_performance\npython scripts/inspect_data.py\npython scripts/check_for_new_data.py\n</code></pre>"},{"location":"phases/phase1-foundation/#step-7-commit-to-git","title":"Step 7: Commit to Git","text":"<pre><code>git add shared/\ngit add config/\ngit add lab1_sales_performance/scripts/\ngit add .gitignore\ngit add lab1_sales_performance/.gitignore\n\ngit commit -m \"Phase 1: Create shared utilities and foundational structure\"\ngit push origin main\n</code></pre>"},{"location":"phases/phase1-foundation/#how-it-works","title":"How It Works","text":""},{"location":"phases/phase1-foundation/#shared-utilities","title":"Shared Utilities","text":"<p>Written once in <code>shared/</code>, used by all labs:</p> <pre><code># In lab1_sales_performance/scripts/inspect_data.py\nimport sys\nsys.path.insert(0, '../..')  # Go to root\nfrom shared.utils.data_inspector import DataInspector\n\ninspector = DataInspector('data/lab1_sales_performance.duckdb')\nquality = inspector.get_quality_score('gold', 'gold_orders_summary')\n</code></pre> <pre><code># In lab2_forecast_model/scripts/inspect_data.py (FUTURE)\nimport sys\nsys.path.insert(0, '../..')  # Go to root\nfrom shared.utils.data_inspector import DataInspector\n\ninspector = DataInspector('data/lab2_forecast_model.duckdb')\nquality = inspector.get_quality_score('gold', 'forecast_summary')\n# Same code, different database\n</code></pre>"},{"location":"phases/phase1-foundation/#central-configuration","title":"Central Configuration","text":"<p>All labs registered in one place:</p> <pre><code># config/labs_config.yaml\nlabs:\n  lab1_sales_performance:\n    path: lab1_sales_performance\n    db_path: lab1_sales_performance/data/lab1_sales_performance.duckdb\n    dbt_path: lab1_sales_performance/dbt\n\n  lab2_forecast_model:  # When created\n    path: lab2_forecast_model\n    db_path: lab2_forecast_model/data/lab2_forecast_model.duckdb\n    dbt_path: lab2_forecast_model/dbt\n</code></pre>"},{"location":"phases/phase1-foundation/#verification-checklist","title":"Verification Checklist","text":"<p>After Phase 1 completion, verify:</p> <ul> <li> <code>shared/</code> directory exists with utilities</li> <li> <code>config/</code> directory exists with labs_config.yaml and paths.py</li> <li> <code>lab1_sales_performance/scripts/inspect_data.py</code> works</li> <li> <code>lab1_sales_performance/scripts/check_for_new_data.py</code> works</li> <li> <code>python config/paths.py</code> returns \u2705</li> <li> <code>git status</code> shows clean working tree</li> <li> Changes committed to git</li> </ul>"},{"location":"phases/phase1-foundation/#whats-not-included-yet","title":"What's NOT Included Yet","text":"<p>Phase 1 focuses on foundation. These come later:</p> <ul> <li>\u274c Makefile (Phase 2)</li> <li>\u274c Bootstrap script (Phase 2)</li> <li>\u274c Orchestration/Airflow (Phase 3)</li> <li>\u274c Advanced monitoring (Phase 3+)</li> </ul>"},{"location":"phases/phase1-foundation/#troubleshooting-phase-1","title":"Troubleshooting Phase 1","text":""},{"location":"phases/phase1-foundation/#import-error-no-module-named-shared","title":"Import Error: \"No module named 'shared'\"","text":"<p>Make sure you're running scripts from the right directory: <pre><code>cd lab1_sales_performance\npython scripts/inspect_data.py  # Works\npython -c \"import scripts.inspect_data\"  # Also works\n</code></pre></p>"},{"location":"phases/phase1-foundation/#file-not-found-labs_configyaml","title":"File Not Found: \"labs_config.yaml\"","text":"<p>Verify the path exists: <pre><code>ls -la config/labs_config.yaml\n</code></pre></p>"},{"location":"phases/phase1-foundation/#datainspector-returns-empty-schemas","title":"DataInspector returns empty schemas","text":"<p>Database might not have tables yet. Create some with dbt: <pre><code>cd lab1_sales_performance/dbt\ndbt run\n</code></pre></p> <p>See Troubleshooting for more help.</p>"},{"location":"phases/phase1-foundation/#next-phase","title":"Next Phase","text":"<p>Phase 2 adds automation: - Makefile for standardized commands - Bootstrap script for creating labs - Testing framework</p> <p>See Phase 2: Automation</p>"}]}